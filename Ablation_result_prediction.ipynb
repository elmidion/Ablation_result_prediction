{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ablation_result_prediction.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOMAKRcv/SJTkg/Mzzhwoqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elmidion/Ablation_result_prediction/blob/main/Ablation_result_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddgHunYS_XRk",
        "outputId": "50a0fb77-19bd-40cd-cda4-b7302b7a102b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7iTKsHb_Gpf",
        "outputId": "e1cf4ef6-46bd-42c7-93ae-033243ef51b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "print(tf.__version__)\n",
        "tf.random.set_seed(20200819)\n",
        "\n",
        "import os\n",
        "from time import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQuGljgJ_Pn4"
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive/AI/Dose prediction/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShUGEbmL_JJk",
        "outputId": "5fd89c56-39db-4a30-b9a0-88b15af0af32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "Dose = pd.read_csv('Dose prediction_20200819.csv')\n",
        "\n",
        "Dose.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>HTN</th>\n",
              "      <th>DM</th>\n",
              "      <th>TB</th>\n",
              "      <th>Hepatitis</th>\n",
              "      <th>OtherthyroidD</th>\n",
              "      <th>Op</th>\n",
              "      <th>biopsy</th>\n",
              "      <th>tumorsize</th>\n",
              "      <th>pT</th>\n",
              "      <th>pN</th>\n",
              "      <th>pM</th>\n",
              "      <th>cT</th>\n",
              "      <th>preadmissiondate</th>\n",
              "      <th>preTg</th>\n",
              "      <th>preATA</th>\n",
              "      <th>preTSH</th>\n",
              "      <th>dose</th>\n",
              "      <th>preparation</th>\n",
              "      <th>FUdate</th>\n",
              "      <th>FUscan</th>\n",
              "      <th>Furesult</th>\n",
              "      <th>Fubinary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TT</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1b</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2000-12-28</td>\n",
              "      <td>55.6</td>\n",
              "      <td>128.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>350</td>\n",
              "      <td>1</td>\n",
              "      <td>2002-01-16</td>\n",
              "      <td>1</td>\n",
              "      <td>Biochemical incomplete</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TT RND</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1b</td>\n",
              "      <td>1a</td>\n",
              "      <td>0</td>\n",
              "      <td>1b</td>\n",
              "      <td>2001-08-13</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>69.80</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2002-03-06</td>\n",
              "      <td>1</td>\n",
              "      <td>Biochemical incomplete</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TT</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1a</td>\n",
              "      <td>0</td>\n",
              "      <td>4a</td>\n",
              "      <td>2001-09-03</td>\n",
              "      <td>29.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>61.20</td>\n",
              "      <td>300</td>\n",
              "      <td>1</td>\n",
              "      <td>2002-04-03</td>\n",
              "      <td>2</td>\n",
              "      <td>Structural incomplete</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TT</td>\n",
              "      <td>1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3a</td>\n",
              "      <td>1a</td>\n",
              "      <td>0</td>\n",
              "      <td>3a</td>\n",
              "      <td>2001-10-08</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>59.89</td>\n",
              "      <td>150</td>\n",
              "      <td>1</td>\n",
              "      <td>2002-05-08</td>\n",
              "      <td>1</td>\n",
              "      <td>Biochemical incomplete</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23</td>\n",
              "      <td>62</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>TT</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1a</td>\n",
              "      <td>0</td>\n",
              "      <td>3b</td>\n",
              "      <td>2001-10-08</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>58.46</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2002-08-07</td>\n",
              "      <td>1</td>\n",
              "      <td>Biochemical incomplete</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  age  sex  HTN  ...      FUdate  FUscan                Furesult  Fubinary\n",
              "0  16   29    1    0  ...  2002-01-16       1  Biochemical incomplete         0\n",
              "1  19   26    2    0  ...  2002-03-06       1  Biochemical incomplete         0\n",
              "2  20   54    2    0  ...  2002-04-03       2   Structural incomplete         0\n",
              "3  22   41    1    0  ...  2002-05-08       1  Biochemical incomplete         0\n",
              "4  23   62    2    0  ...  2002-08-07       1  Biochemical incomplete         0\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt4YGRtoJaxG"
      },
      "source": [
        "# THW 환자만 대상\n",
        "THW = Dose['preparation']==1\n",
        "Dose_main = Dose[THW]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgN_j9PWA5Yt",
        "outputId": "68e1d571-4f0a-4eab-e799-7b57cedc0bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "Dose_main.columns\n",
        "Dose_main = Dose_main.drop(['ID', 'preadmissiondate', 'preparation', 'FUdate', 'FUscan', 'Furesult'], axis='columns')\n",
        "#Dose_main.head\n",
        "#Dose_main.describe()\n",
        "\n",
        "Dose_main.pN[Dose_main.pN==' 1b'] = '1b'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiTZn8FmJfi0",
        "outputId": "bc15a517-30be-41fe-85b9-2e5ba4fbe519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "Dose_main.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>HTN</th>\n",
              "      <th>DM</th>\n",
              "      <th>TB</th>\n",
              "      <th>Hepatitis</th>\n",
              "      <th>OtherthyroidD</th>\n",
              "      <th>Op</th>\n",
              "      <th>biopsy</th>\n",
              "      <th>tumorsize</th>\n",
              "      <th>pT</th>\n",
              "      <th>pN</th>\n",
              "      <th>pM</th>\n",
              "      <th>cT</th>\n",
              "      <th>preTg</th>\n",
              "      <th>preATA</th>\n",
              "      <th>preTSH</th>\n",
              "      <th>dose</th>\n",
              "      <th>Fubinary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TT</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1b</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>55.6</td>\n",
              "      <td>128.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>350</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TT RND</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1b</td>\n",
              "      <td>1a</td>\n",
              "      <td>0</td>\n",
              "      <td>1b</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>69.80</td>\n",
              "      <td>200</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TT</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1a</td>\n",
              "      <td>0</td>\n",
              "      <td>4a</td>\n",
              "      <td>29.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>61.20</td>\n",
              "      <td>300</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TT</td>\n",
              "      <td>1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3a</td>\n",
              "      <td>1a</td>\n",
              "      <td>0</td>\n",
              "      <td>3a</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>59.89</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>TT</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1a</td>\n",
              "      <td>0</td>\n",
              "      <td>3b</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>58.46</td>\n",
              "      <td>200</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  HTN  DM  TB  Hepatitis  ...  cT preTg  preATA  preTSH dose Fubinary\n",
              "0   29    1    0   0   0          0  ...   2  55.6   128.0    1.00  350        0\n",
              "1   26    2    0   0   0          0  ...  1b  11.0     2.0   69.80  200        0\n",
              "2   54    2    0   0   0          0  ...  4a  29.0     2.0   61.20  300        0\n",
              "3   41    1    0   0   0          0  ...  3a  16.0     2.0   59.89  150        0\n",
              "4   62    2    0   0   0          1  ...  3b  22.0     2.0   58.46  200        0\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCwpLLRRKtCw",
        "outputId": "f7c24467-0fdf-4108-a3dc-0934cd79a0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "Dose_main.Fubinary.value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    327\n",
              "1    304\n",
              "Name: Fubinary, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQZcIGfqsJkg"
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "def base_stat(*features):\n",
        "  for feature in features:\n",
        "    print(pd.DataFrame(feature))\n",
        "    print('Min : ', np.min(feature))  \n",
        "    print('Median : ', np.median(feature))\n",
        "    print('Max : ', np.max(feature))\n",
        "    \n",
        "    print('Mean : ', np.mean(feature))\n",
        "    print('std : ', np.std(feature))\n",
        "  if len(features) == 2:\n",
        "    print('Compare the mean between two groups')\n",
        "    \n",
        "    print('\\n')\n",
        "    feature1 = features[0]\n",
        "    feature2 = features[1]\n",
        "    levene, levene_p = stats.levene(feature1, feature2)\n",
        "    print('등분산 여부 : levene', levene, 'p-value :', levene_p)\n",
        "    \n",
        "    print('\\n')\n",
        "    if levene_p < 0.05:\n",
        "      t, p = stats.ttest_ind(feature1, feature2, equal_var=False) \n",
        "      print('T값 : ', t, 'p-value :',p)\n",
        "    else:\n",
        "      t, p = stats.ttest_ind(feature1, feature2, equal_var=True) \n",
        "      print('T값 : ', t, 'p-value :',p)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHZwzq--Jr2e",
        "outputId": "a80b497c-e135-4dcc-c898-90f830a85af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#(ax.set_title(), ax.set_xlabel(), ax.set_ylabel())\n",
        "\n",
        "sns.distplot(Dose_main[Dose_main['Fubinary'] == 0]['age'], hist=False, color=\"orange\", label=\"Failure\")\n",
        "sns.distplot(Dose_main[Dose_main['Fubinary'] == 1]['age'], hist=False, color=\"skyblue\", label=\"Success\")\n",
        "plt.title('Age of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.show()\n",
        "\n",
        "base_stat(Dose_main[Dose_main['Fubinary'] == 0]['age'], Dose_main[Dose_main['Fubinary'] == 1]['age'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "     age\n",
            "0     29\n",
            "1     26\n",
            "2     54\n",
            "3     41\n",
            "4     62\n",
            "..   ...\n",
            "741   28\n",
            "754   25\n",
            "769   81\n",
            "773   38\n",
            "774   43\n",
            "\n",
            "[327 rows x 1 columns]\n",
            "Min :  9\n",
            "Median :  48.0\n",
            "Max :  82\n",
            "Mean :  47.345565749235476\n",
            "std :  12.782101289105128\n",
            "     age\n",
            "12    50\n",
            "27    25\n",
            "41    61\n",
            "44    31\n",
            "47    70\n",
            "..   ...\n",
            "714   50\n",
            "729   35\n",
            "734   70\n",
            "747   43\n",
            "759   24\n",
            "\n",
            "[304 rows x 1 columns]\n",
            "Min :  23\n",
            "Median :  50.0\n",
            "Max :  76\n",
            "Mean :  49.56578947368421\n",
            "std :  11.740153238392663\n",
            "Compare the mean between two groups\n",
            "\n",
            "\n",
            "등분산 여부 : levene 1.5130601057000719 p-value : 0.21913258652366646\n",
            "\n",
            "\n",
            "T값 :  -2.2636585484167218 p-value : 0.02393497177233088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwIeObhh39Xz"
      },
      "source": [
        "def Xtab(index, columns):\n",
        "  Xtab = pd.crosstab(index=index, columns=columns, margins=True)\n",
        "  print(Xtab)\n",
        "  c, p, dof, expected = stats.chi2_contingency(Xtab)\n",
        "  print('카이제곱값: ', c, 'P-values :', p, '\\n')\n",
        "  print(pd.crosstab(index=index, columns=columns, margins=True, normalize=True))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8CEUltZBGFl",
        "outputId": "abc2f8f8-c1e1-4960-9b2e-8dcbd2426a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "x = ['Female', 'Male']\n",
        "y = Dose_main['sex'].value_counts()\n",
        "\n",
        "a = ['Female', 'Male']\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['sex'].value_counts()\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('Sex of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.show()\n",
        "\n",
        "Xtab(Dose_main['sex'], Dose_main['Fubinary'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "sex                    \n",
            "1          73   40  113\n",
            "2         254  264  518\n",
            "All       327  304  631\n",
            "카이제곱값:  9.003829073967477 P-values : 0.06100384355570909 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "sex                                   \n",
            "1         0.115689  0.063391  0.179081\n",
            "2         0.402536  0.418384  0.820919\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DgjSzU1WJ4M",
        "outputId": "9b5c0b7d-40ed-41c4-fedf-c0bc9d7975ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "x = [0, 1]\n",
        "y = Dose_main['HTN'].value_counts()\n",
        "\n",
        "a = [0, 1]\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['HTN'].value_counts()\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('HTN of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['No HTN', 'HTN'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['HTN'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "HTN                    \n",
            "0         267  242  509\n",
            "1          60   62  122\n",
            "All       327  304  631\n",
            "카이제곱값:  0.42289476312414076 P-values : 0.9805598904204162 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "HTN                                   \n",
            "0         0.423138  0.383518  0.806656\n",
            "1         0.095087  0.098257  0.193344\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX4VdSpUW1nk",
        "outputId": "59766d05-e581-4479-c49d-e49d34ae08ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "x = [0, 1]\n",
        "y = Dose_main['DM'].value_counts()\n",
        "\n",
        "a = [0, 1]\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['DM'].value_counts()\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('DM of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['No DM', 'DM'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['DM'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "DM                     \n",
            "0         304  285  589\n",
            "1          23   19   42\n",
            "All       327  304  631\n",
            "카이제곱값:  0.15571066273012116 P-values : 0.9971220803293211 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "DM                                    \n",
            "0         0.481775  0.451664  0.933439\n",
            "1         0.036450  0.030111  0.066561\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTbeh7TFH8Ej",
        "outputId": "6565f585-8c19-481f-d157-32e90b112d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "x = [0, 1]\n",
        "y = Dose_main['TB'].value_counts()\n",
        "\n",
        "a = [0, 1]\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['TB'].value_counts()\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('TB of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['No TB', 'TB'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['TB'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAac0lEQVR4nO3dfXRV1Z3G8e+PEA0UBYWASqg3rbRCExIwgkqrDKj1BQe1FZyhIyJTupbgy4zDyFhtLbWK01qrLVUzxYKKFnyrqIxKURdOwZcE4yu6RCQmESUgUIPiC/nNH3eTBkzITXJDws7zWSvrnr3PPvvsk5X13JN9zznX3B0REYlLl/YegIiIpJ/CXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp36RTM7CwzqzCzGjMb2sb7+mrYT0Zb7kdkTxTu0i5C+O38qTWzT+qVJ5rZ1Wb2eb261Wb2vVbs8lfAdHfv4e4vpus4AMxsnZmduLPs7u+G/exIQ99Pm9m/trYf6XwU7tIuQvj1cPcewLvAGfXqFoRmC+u1uRS4y8z6tXCXhwOvpWHoIvsEhbvsE9z9ceAj4OsNrTezLmZ2pZmVm9kGM7vDzHqa2f5mVgNkAC+Z2duNbO9mdrGZrTWzjWb2SzPrEtZ93cyeNLNNYd0CM+sV1t0JfBV4OPyH8Z9mlgj9dQ1teprZXDNbb2ZVZnbNzikbMzvfzP7PzH5lZpvN7B0zOzWs+wXwHeB3oe/fWdKN4Rj/ZmavmFleOn/XEgeFu3R4IdBOB/YDXm+k2fnh5x+ArwE9gN+5+6fhzB+gwN0bfHMIzgKKgGHAOOCCnUMArgMOAwYBA4CrAdz9X9j1P4//bqDfecAXwBHAUOBkoP5UywjgTaAP8N/AXDMzd/8x8Ax/n06aHrY9HvgG0BMYD2zawzFJJ6Vwl45svJltAWqAxcC17r6lkbYTgV+7+1p3rwH+Czh359lziq539w/d/V3gN8A/Abj7GndfGt4oqoFfAyek0mGYRjoNuNTdt7n7BuBG4Nx6zcrd/X/CHP184FCgsemnz4EDgCMBc/fV7r6+GcconYTCXTqyRe7ey92/QnI65jwz+1EjbQ8DyuuVy4GuNB6SDanYbfvDIBnQZvanMKXyN+AukmfZqTgcyATWm9mW8GZ1G9C3Xpv3dy64+8dhsQcNcPcngd8Bc4ANZlZsZgemOBbpRBTusk9w93XA/wJnNNLkPZJButNXSU6FfNCM3QzYbfv3wvK1gAP57n4g8AOSUzV1w9tDnxXAp0Cf8EbVy90PdPdvpTimL/Xt7je7+1HAYJLTMzNS7Es6EYW77BPMLAc4hcaveLkH+DczyzWzHiQDeaG7f9GM3cwws4PMbABwCbAw1B9Acmpoq5n158th+gHJef4vCVMmTwA3mNmB4YPfr5tZStM6u/dtZkeb2QgzywS2AduB2hT7kk5E4S4d2YSd17kDLwB/BX7WSNvbgTuB5cA7JEPvombu7yGgFCgDHgXmhvqfkfyQdWuof2C37a4DrgzTLv/RQL/n8fcPgzcD95GcV0/FTcD3w5U0NwMHAv8T+ikn+WHqL1PsSzoR05d1iCQvhQQGuvua9h6LSDrozF1EJEIKdxGRCGlaRkQkQjpzFxGJUHPu3mszffr08UQi0d7DEBHZp5SWlm509+yG1nWIcE8kEpSUlLT3MERE9ilmVt7YOk3LiIhESOEuIhIhhbuISIQ6xJy7iHQ+n3/+OZWVlWzfvr29h9LhZWVlkZOTQ2ZmZsrbKNxFpF1UVlZywAEHkEgkMLOmN+ik3J1NmzZRWVlJbm5uyttpWkZE2sX27dvp3bu3gr0JZkbv3r2b/R+Owl1E2o2CPTUt+T0p3EVEIqRwF5HoZGRkUFhYSF5eHmeccQZbtjT21bstk0gk2LhxI1u2bOH3v/99WvtOl33+A9XEzEfbewjSga2bfXp7D0HaQbdu3SgrKwNg0qRJzJkzhx//+Mdp38/OcL/wwgvT3ndr6cxdRKJ27LHHUlVVBcDbb7/NKaecwlFHHcV3vvMd3njjDQDuvfde8vLyKCgo4Pjjjwdg3rx5TJ8+va6fsWPH8vTTT+/S98yZM3n77bcpLCxkxoyO9VW2KZ25m1kv4A9AHskv7L0AeJPkd0wmgHXAeHffbMmZ/5uA04CPgfPdfVXaRy4i0oQdO3awbNkypkyZAsDUqVO59dZbGThwIM899xwXXnghTz75JLNmzeLxxx+nf//+zZrCmT17Nq+++mrdfwkdSarTMjcBj7n7981sP6A7cAWwzN1nm9lMYCZwOXAqMDD8jABuCa8iInvFJ598QmFhIVVVVQwaNIiTTjqJmpoaVqxYwTnnnFPX7tNPPwVg5MiRnH/++YwfP56zzz67vYadVk1Oy5hZT+B4wpcFu/tn7r4FGAfMD83mA2eG5XHAHZ70LNDLzFL9MmARkVbbOedeXl6OuzNnzhxqa2vp1asXZWVldT+rV68G4NZbb+Waa66hoqKCo446ik2bNtG1a1dqa2vr+tzX7qRNZc49F6gG/mhmL5rZH8zsK0A/d18f2rwP9AvL/YGKettXhrpdmNlUMysxs5Lq6uqWH4GISCO6d+/OzTffzA033ED37t3Jzc3l3nvvBZJ3fr700ktAci5+xIgRzJo1i+zsbCoqKkgkEpSVlVFbW0tFRQXPP//8l/o/4IAD+Oijj/bqMaUqlXDvCgwDbnH3ocA2klMwdTz5XX3N+r4+dy929yJ3L8rObvBZ8yIirTZ06FCGDBnCPffcw4IFC5g7dy4FBQV861vf4qGHHgJgxowZ5Ofnk5eXx3HHHUdBQQEjR44kNzeXwYMHc/HFFzNs2LAv9d27d29GjhxJXl7ePvmBaiVQ6e7PhfJ9JMP9AzM71N3Xh2mXDWF9FTCg3vY5oU5EZK+oqanZpfzwww/XLT/22GNfav/AAw802M+CBQsarF+3bl3d8t13392CEba9Js/c3f19oMLMvhmqxgCvA4uBSaFuEvBQWF4MnGdJxwBb603fiIjIXpDq1TIXAQvClTJrgckk3xgWmdkUoBwYH9ouIXkZ5BqSl0JOTuuIRUSkSSmFu7uXAUUNrBrTQFsHprVyXCIi0gq6Q1VEJEIKdxGRCCncRUQitM8/FVJE4pDuJ7ym8kTQX/ziF9x9991kZGTQpUsXbrvtNkaMiONpKQp3EemUVq5cySOPPMKqVavYf//92bhxI5999ll7DyttNC0jIp3S+vXr6dOnD/vvvz8Affr04bDDDqv7Ig6AkpISRo0aBSRvjJo8eTL5+fkMGTKE+++/H0jeFDVs2DAKCgoYMyZ5AeG2bdu44IILGD58OEOHDq27E/a1115j+PDhFBYWMmTIEN566y22bdvG6aefTkFBAXl5eSxcuDAtx6czdxHplE4++WRmzZrFN77xDU488UQmTJjACSec0Gj7n//85/Ts2ZNXXnkFgM2bN1NdXc0Pf/hDli9fTm5uLh9++CGQnO4ZPXo0t99+O1u2bGH48OGceOKJ3HrrrVxyySVMnDiRzz77jB07drBkyRIOO+wwHn00OS21devWtByfztxFpFPq0aMHpaWlFBcXk52dzYQJE5g3b16j7f/yl78wbdrfb+E56KCDePbZZzn++OPJzc0F4OCDDwbgiSeeYPbs2RQWFjJq1Ci2b9/Ou+++y7HHHsu1117L9ddfT3l5Od26dSM/P5+lS5dy+eWX88wzz9CzZ8+0HJ/O3EWk08rIyGDUqFGMGjWK/Px85s+fv8ujflv6mF935/777+eb3/zmLvWDBg1ixIgRPProo5x22mncdtttjB49mlWrVrFkyRKuvPJKxowZw09+8pNWH5vO3EWkU3rzzTd566236splZWUcfvjhJBIJSktLAerm1QFOOukk5syZU1fevHkzxxxzDMuXL+edd94BqJuW+e53v8tvf/tbkjfsw4svvgjA2rVr+drXvsbFF1/MuHHjePnll3nvvffo3r07P/jBD5gxYwarVqXni+t05i4iHcLe/jLzmpoaLrroIrZs2ULXrl054ogjKC4uZvXq1UyZMoWrrrqq7sNUgCuvvJJp06aRl5dHRkYGP/3pTzn77LMpLi7m7LPPpra2lr59+7J06VKuuuoqLr30UoYMGUJtbS25ubk88sgjLFq0iDvvvJPMzEwOOeQQrrjiCl544QVmzJhBly5dyMzM5JZbbknL8dnOd5b2VFRU5CUlJS3aNt3Xxkpc9nZgSOpWr17NoEGD2nsY+4yGfl9mVuruDT33S9MyIiIxUriLiERI4S4iEiGFu4hIhBTuIiIRUriLiERI17mLSMdwdXpuu/97f00/oyUjI4P8/Py68p///GcSiUSDbY877jhWrFjBunXrGDt2LK+++mq6RtomFO4i0ml169aNsrKylNquWLGixfvZsWMHGRkZLd6+JTQtIyIS1NTUMGbMGIYNG0Z+fn7do3oh+aCx3c2bN4/p06fXlceOHcvTTz9d1/6yyy6joKCAlStXctddd9U97vdHP/oRO3bsaNNjUbiLSKf1ySefUFhYSGFhIWeddRZZWVk8+OCDrFq1iqeeeorLLruMlt7Fv23bNkaMGMFLL71E7969WbhwIX/9618pKysjIyODBQsWpPlodqVpGRHptHaflvn888+54oorWL58OV26dKGqqooPPviAQw45pNl9Z2Rk8L3vfQ+AZcuWUVpaytFHHw0k31T69u2bnoNohMJdRCRYsGAB1dXVlJaWkpmZSSKR2ONjf+s/Hhh2fURwVlZW3Ty7uzNp0iSuu+66thv8bjQtIyISbN26lb59+5KZmclTTz1FeXn5HtsnEgnKysqora2loqKC559/vsF2Y8aM4b777mPDhg1A8tHATfXdWimduZvZOuAjYAfwhbsXmdnBwEIgAawDxrv7ZjMz4CbgNOBj4Hx3T88DikUkXilcutjWJk6cyBlnnEF+fj5FRUUceeSRe2w/cuRIcnNzGTx4MIMGDWLYsGENths8eDDXXHMNJ598MrW1tWRmZjJnzhwOP/zwtjgMoHnTMv/g7hvrlWcCy9x9tpnNDOXLgVOBgeFnBHBLeBUR6VBqamp2Kffp04eVK1fusW0ikai7xt3MGv1gdPe+J0yYwIQJE1o75JS1ZlpmHDA/LM8HzqxXf4cnPQv0MrNDW7EfERFpplTD3YEnzKzUzKaGun7uvj4svw/0C8v9gYp621aGul2Y2VQzKzGzkurq6hYMXUREGpPqtMy33b3KzPoCS83sjfor3d3NrFkXg7p7MVAMyW9ias62IhIHdyf5MZ3sSUuutU/pzN3dq8LrBuBBYDjwwc7plvC6ITSvAgbU2zwn1ImI1MnKymLTpk0tvkmos3B3Nm3aRFZWVrO2a/LM3cy+AnRx94/C8snALGAxMAmYHV533qe7GJhuZn8i+UHq1nrTNyIiAOTk5FBZWYmmZZuWlZVFTk5Os7ZJZVqmH/Bg+NepK3C3uz9mZi8Ai8xsClAOjA/tl5C8DHINyUshJzdrRCLSKWRmZpKbm9vew4hWk+Hu7muBggbqNwFjGqh3YFpaRiciIi2iO1RFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIpRzuZpZhZi+a2SOhnGtmz5nZGjNbaGb7hfr9Q3lNWJ9om6GLiEhjmnPmfgmwul75euBGdz8C2AxMCfVTgM2h/sbQTkRE9qKUwt3McoDTgT+EsgGjgftCk/nAmWF5XCgT1o8J7UVEZC9J9cz9N8B/ArWh3BvY4u5fhHIl0D8s9wcqAML6raH9LsxsqpmVmFlJdXV1C4cvIiINaTLczWwssMHdS9O5Y3cvdvcidy/Kzs5OZ9ciIp1e1xTajAT+0cxOA7KAA4GbgF5m1jWcnecAVaF9FTAAqDSzrkBPYFPaRy4iIo1q8szd3f/L3XPcPQGcCzzp7hOBp4Dvh2aTgIfC8uJQJqx/0t09raMWEZE9as117pcD/25ma0jOqc8N9XOB3qH+34GZrRuiiIg0VyrTMnXc/Wng6bC8FhjeQJvtwDlpGJuIiLSQ7lAVEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYlQ1/YeQGuty/rn9h6CdGhb23sAIu1CZ+4iIhFSuIuIREjhLiISoSbD3cyyzOx5M3vJzF4zs5+F+lwze87M1pjZQjPbL9TvH8prwvpE2x6CiIjsLpUz90+B0e5eABQCp5jZMcD1wI3ufgSwGZgS2k8BNof6G0M7ERHZi5oMd0+qCcXM8OPAaOC+UD8fODMsjwtlwvoxZmZpG7GIiDQppTl3M8swszJgA7AUeBvY4u5fhCaVQP+w3B+oAAjrtwK9G+hzqpmVmFlJdXV1645CRER2kVK4u/sOdy8EcoDhwJGt3bG7F7t7kbsXZWdnt7Y7ERGpp1lXy7j7FuAp4Figl5ntvAkqB6gKy1XAAICwviewKS2jFRGRlKRytUy2mfUKy92Ak4DVJEP++6HZJOChsLw4lAnrn3R3T+egRURkz1J5/MChwHwzyyD5ZrDI3R8xs9eBP5nZNcCLwNzQfi5wp5mtAT4Ezm2DcYuIyB40Ge7u/jIwtIH6tSTn33ev3w6ck5bRiYhIi+gOVRGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEIKdxGRCCncRUQipHAXEYmQwl1EJEJNhruZDTCzp8zsdTN7zcwuCfUHm9lSM3srvB4U6s3MbjazNWb2spkNa+uDEBGRXaVy5v4FcJm7DwaOAaaZ2WBgJrDM3QcCy0IZ4FRgYPiZCtyS9lGLiMgeNRnu7r7e3VeF5Y+A1UB/YBwwPzSbD5wZlscBd3jSs0AvMzs07SMXEZFGNWvO3cwSwFDgOaCfu68Pq94H+oXl/kBFvc0qQ52IiOwlKYe7mfUA7gcudfe/1V/n7g54c3ZsZlPNrMTMSqqrq5uzqYiINCGlcDezTJLBvsDdHwjVH+ycbgmvG0J9FTCg3uY5oW4X7l7s7kXuXpSdnd3S8YuISANSuVrGgLnAanf/db1Vi4FJYXkS8FC9+vPCVTPHAFvrTd+IiMhe0DWFNiOBfwFeMbOyUHcFMBtYZGZTgHJgfFi3BDgNWAN8DExO64hFRKRJTYa7u/8fYI2sHtNAewemtXJcIiLSCrpDVUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQgp3EZEIKdxFRCKkcBcRiZDCXUQkQk2Gu5ndbmYbzOzVenUHm9lSM3srvB4U6s3MbjazNWb2spkNa8vBi4hIw1I5c58HnLJb3UxgmbsPBJaFMsCpwMDwMxW4JT3DFBGR5mgy3N19OfDhbtXjgPlheT5wZr36OzzpWaCXmR2arsGKiEhqWjrn3s/d14fl94F+Ybk/UFGvXWWo+xIzm2pmJWZWUl1d3cJhiIhIQ1r9gaq7O+At2K7Y3YvcvSg7O7u1wxARkXpaGu4f7JxuCa8bQn0VMKBeu5xQJyIie1FLw30xMCksTwIeqld/Xrhq5hhga73pGxER2Uu6NtXAzO4BRgF9zKwS+CkwG1hkZlOAcmB8aL4EOA1YA3wMTG6DMYuISBOaDHd3/6dGVo1poK0D01o7KBERaR3doSoiEiGFu4hIhBTuIiIRUriLiERI4S4iEiGFu4hIhBTuIiIRUriLiERI4S4iEiGFu4hIhBTuIiIRUriLiERI4S4iEiGFu4hIhBTuIiIRUriLiERI4S4iEiGFu4hIhBTuIiIRUriLiERI4S4iEiGFu4hIhBTuIiIR6treAxCJXWLmo+09BOnA1s0+vU361Zm7iEiE2iTczewUM3vTzNaY2cy22IeIiDQu7dMyZpYBzAFOAiqBF8xssbu/nu59iewL1mX9c3sPQTq0rW3Sa1ucuQ8H1rj7Wnf/DPgTMK4N9iMiIo1oiw9U+wMV9cqVwIjdG5nZVGBqKNaY2ZttMJbOqA+wsb0H0WH8zNp7BPJl+hutr3V/o4c3tqLdrpZx92KguL32HyszK3H3ovYeh0hj9De6d7TFtEwVMKBeOSfUiYjIXtIW4f4CMNDMcs1sP+BcYHEb7EdERBqR9mkZd//CzKYDjwMZwO3u/lq69yON0lSXdHT6G90LzN3bewwiIpJmukNVRCRCCncRkQjpwWEdlJk58Gt3vyyU/wPo4e5Xp7DtZOCSUBwMvAnsAB4D3gB+SfIKpkxgNXCeu3+c7mOQzsvMegPLQvEQkn9/1aFcALwEWKif7u4r9vogI6c59w7KzLYD64Gj3X1jc8J9t37WAUXuvjGUzw/l6aF8N7DU3f+YxuGL1DGzq4Ead/9VKNe4e4+w/F3gCnc/oR2HGCVNy3RcX5C8quDfdl9hZgkze9LMXjazZWb21ZbswMy6Al8BNrduqCItdiD6+2sTCveObQ4w0cx67lb/W2C+uw8BFgA3N7PfCWZWRnJq5mDg4VaPVCR13cyszMzeAP4A/Ly9BxQjhXsH5u5/A+4ALt5t1bHA3WH5TuDbzex6obsXkpwLfQWY0ZpxijTTJ+5e6O5HAqcAd5iZHgKUZgr3ju83wBSS0ydp5ckPXB4Gjk933yKpcPeVJB8klt3eY4mNwr2Dc/cPgUUkA36nFSQf6wAwEXimFbv4NvB2K7YXaTEzO5Lkneyb2nsssdGlkPuGG4Dp9coXAX80sxkkLy+b3Mz+JpjZt0m+uVcC56djkCIp6hY+84Hk5ZCT3H1Hew4oRroUUkQkQpqWERGJkMJdRCRCCncRkQgp3EVEIqRwFxGJkMJdRCRCCncRkQj9PwDxzZYEr1FoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "TB                     \n",
            "0         321  295  616\n",
            "1           6    9   15\n",
            "All       327  304  631\n",
            "카이제곱값:  0.8601936352819076 P-values : 0.9302009219916003 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "TB                                    \n",
            "0         0.508716  0.467512  0.976228\n",
            "1         0.009509  0.014263  0.023772\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eux555a7ZE7y",
        "outputId": "afd137fc-8c2f-455d-e475-37435ac1b91c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "x = [0, 1]\n",
        "y = Dose_main['Hepatitis'].value_counts()\n",
        "\n",
        "a = [0, 1]\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['Hepatitis'].value_counts()\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('Hepatitis of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['No Hepatitis', 'Hepatitis'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['Hepatitis'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary     0    1  All\n",
            "Hepatitis               \n",
            "0          311  295  606\n",
            "1           16    9   25\n",
            "All        327  304  631\n",
            "카이제곱값:  1.5461446422889518 P-values : 0.8184372869308019 \n",
            "\n",
            "Fubinary          0         1      All\n",
            "Hepatitis                             \n",
            "0          0.492868  0.467512  0.96038\n",
            "1          0.025357  0.014263  0.03962\n",
            "All        0.518225  0.481775  1.00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzOs_QxHZlc7",
        "outputId": "de081b3b-238d-49cb-f019-2b03582c9eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "x = [0, 1]\n",
        "y = Dose_main['OtherthyroidD'].value_counts()\n",
        "\n",
        "a = [0, 1]\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['OtherthyroidD'].value_counts()\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('Other thyroid disease of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['No Other thyroid disease', 'Other thyroid disease'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['OtherthyroidD'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary         0    1  All\n",
            "OtherthyroidD               \n",
            "0              193  155  348\n",
            "1              134  149  283\n",
            "All            327  304  631\n",
            "카이제곱값:  4.111589159773389 P-values : 0.391114497467873 \n",
            "\n",
            "Fubinary              0         1       All\n",
            "OtherthyroidD                              \n",
            "0              0.305864  0.245642  0.551506\n",
            "1              0.212361  0.236133  0.448494\n",
            "All            0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVb_-lMdZRih",
        "outputId": "f8a0aec8-12e1-4040-db7e-46d5c037c65c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        }
      },
      "source": [
        "x = [0, 1, 2]\n",
        "y = Dose_main['Op'].value_counts()\n",
        "\n",
        "a = [0, 1, 2]\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['Op'].value_counts()\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('Operation types of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1,2],['TT', 'TT with CND', 'TT with CND and LND'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['Op'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "Op                     \n",
            "TT         61   46  107\n",
            "TT CND    204  218  422\n",
            "TT RND     62   40  102\n",
            "All       327  304  631\n",
            "카이제곱값:  6.482617790983625 P-values : 0.37134937413335894 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "Op                                    \n",
            "TT        0.096672  0.072900  0.169572\n",
            "TT CND    0.323296  0.345483  0.668780\n",
            "TT RND    0.098257  0.063391  0.161648\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUL6-I8eazSG",
        "outputId": "522c112f-ecea-4122-92a5-dbf6d1d8605f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# x = [0, 1, 2]\n",
        "# y = Dose_main['Op'].value_counts()\n",
        "\n",
        "# a = [0, 1, 2]\n",
        "# b = Dose_main[Dose_main['Fubinary'] == 0]['Op'].value_counts()\n",
        "sns.boxplot(x=(1-Dose_main['Fubinary']), y=Dose_main['tumorsize'])\n",
        "\n",
        "# plt.boxplot(x, y, label='Success')\n",
        "# plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('Tumor size of patients')\n",
        "# plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['Success', 'Failure'])\n",
        "plt.xlabel('result')\n",
        "plt.show()\n",
        "\n",
        "base_stat(Dose_main[Dose_main['Fubinary'] == 0]['tumorsize'], Dose_main[Dose_main['Fubinary'] == 1]['tumorsize'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "     tumorsize\n",
            "0          4.0\n",
            "1          2.0\n",
            "2          2.0\n",
            "3          1.5\n",
            "4          3.0\n",
            "..         ...\n",
            "741        0.5\n",
            "754        1.3\n",
            "769        2.7\n",
            "773        2.0\n",
            "774        1.2\n",
            "\n",
            "[327 rows x 1 columns]\n",
            "Min :  0.1\n",
            "Median :  1.1\n",
            "Max :  5.5\n",
            "Mean :  1.3142507645259947\n",
            "std :  0.818629031145349\n",
            "     tumorsize\n",
            "12         1.2\n",
            "27         1.0\n",
            "41         2.0\n",
            "44         1.5\n",
            "47         1.5\n",
            "..         ...\n",
            "714        0.9\n",
            "729        2.7\n",
            "734        1.2\n",
            "747        0.7\n",
            "759        0.7\n",
            "\n",
            "[304 rows x 1 columns]\n",
            "Min :  0.2\n",
            "Median :  0.9\n",
            "Max :  5.0\n",
            "Mean :  1.115953947368422\n",
            "std :  0.7020297718122318\n",
            "Compare the mean between two groups\n",
            "\n",
            "\n",
            "등분산 여부 : levene 10.080950543456124 p-value : 0.001571292509017688\n",
            "\n",
            "\n",
            "T값 :  3.267832110954589 p-value : 0.0011431341525635083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM2Nc04Xf5Yk",
        "outputId": "4fde1964-59de-484c-d88e-b10f49eb7157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "Dose_main['cT'].value_counts().sort_index(ascending=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1a    283\n",
              "1b    136\n",
              "2      46\n",
              "3a      8\n",
              "3b    102\n",
              "4a     56\n",
              "Name: cT, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2nFNF8NcmAb",
        "outputId": "a6e417d1-5d47-4e2b-e55f-2a33b687595a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "x = ['1a', '1b', '2', '3a', '3b', '4a']\n",
        "y = Dose_main['cT'].value_counts().sort_index(ascending=True)\n",
        "\n",
        "a = ['1a', '1b', '2', '3a', '3b', '4a']\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['cT'].value_counts().sort_index(ascending=True)\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('cT stages of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "#plt.xticks([0,1,2],['TT', 'TT with CND', 'TT with CND and LND'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['cT'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "cT                     \n",
            "1a        131  152  283\n",
            "1b         74   62  136\n",
            "2          28   18   46\n",
            "3a          5    3    8\n",
            "3b         55   47  102\n",
            "4a         34   22   56\n",
            "All       327  304  631\n",
            "카이제곱값:  7.661747650473859 P-values : 0.8109737492208257 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "cT                                    \n",
            "1a        0.207607  0.240887  0.448494\n",
            "1b        0.117274  0.098257  0.215531\n",
            "2         0.044374  0.028526  0.072900\n",
            "3a        0.007924  0.004754  0.012678\n",
            "3b        0.087163  0.074485  0.161648\n",
            "4a        0.053883  0.034865  0.088748\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELXT8DTmL_31",
        "outputId": "1c52ce59-729a-4dad-8af0-427fc9855778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "Dose_main['pT'].value_counts()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1a    341\n",
              "1b    209\n",
              "2      67\n",
              "3a     11\n",
              "4a      3\n",
              "Name: pT, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWOX2UpOcGOX",
        "outputId": "aa381407-31fa-49dc-c28c-986cf1c724db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "x = ['1a', '1b', '2', '3a', '4a']\n",
        "y = Dose_main['pT'].value_counts()\n",
        "\n",
        "a = ['1a', '1b', '2', '3a', '4a']\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['pT'].value_counts()\n",
        "\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('pT stages of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "#plt.xticks([0,1,2],['TT', 'TT with CND', 'TT with CND and LND'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['pT'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeYUlEQVR4nO3df5RVdb3/8eeLYa4DaaIw/gCMmcoKZGDEEVT6wRU1U7uolVhWZBZ2hdTvdXEj0zTTonWvda9FKl29kKGJv9KUb0UIXyp/IOCgKLpEhRhEQRQUE38w7+8few8ex4E5M3POHGbP67HWWbPP3p+9z/tz1szr7PnsH0cRgZmZZUuPUhdgZmaF53A3M8sgh7uZWQY53M3MMsjhbmaWQQ53M7MMcribdYCkj0qql/SqpHM74fUekzSm2K9jXZ98nrt1hKQzgGvTp2XAHsA/mpZHxJ5t2NZMoCEiLipkjcUk6TrglYj4P0XY9kyK9H5IuhT4cER8udDbtt2D99ytQyJidkTsmYb4Z4Dnmp63Jdi7sEHAY6Uuwuw9IsIPP1p9AKuB7wKPAy8D/wtUNGszhmRPc1fbEfAzYAPwCvAoMBSYCLwFvAlsBX6ftp8KPA28mr72KTnbKgOuBF4EngUmAwH0TJfvDVwHrAfWAZcDZemyDwP/D9iSrn/zLmr+F5IA3wwsBAan8+8FtgPb0po/0sK6C4EfA4vT/t4J7Juz/Bbg+bSORcAh6fydvR+rgWPS6R45788mYE7TtoGq9L2YAPw97eP30mXHp9t9K9328nT+14Bn0vf6WeCMUv/e+dGBv9lSF+BH13ikobICOAjYF/gbcHmzNvmE+6eBpUCfNOgHAwemy2a2sM0vAP3TIBsPvJbT/ltp4A8E9gH+3Czc7yAZMnofsF8asGeny24CvpdutwL4+E7q/Uj6mscC5cC/A6uAf0qXLwS+sYv+LiT5YBma1nEb8Juc5V8H9iIZzvovoD5nWUvvR264nwc8kPZ/j7SvN6XLmsL9V0AvYDjwBu98MF3arI73kXz4fDR9fiDpB40fXfPhYRlri19ExNqIeAm4AvhiO7bxFkmYfYzkmM/KiFi/s8YRcUtEPBcRjRFxM/AUMDJdfBrw3xHREBEvA9Oa1pO0P3ACcH5EvBYRG0j+Yzg9p45BQP+I2BYRf91JCeOBeyJiXkS8BfwnSVge1YY+3xARKyLiNeBi4DRJZWn/ro+IVyPiDZLAHS5p7zy3+y2SvfGGnPU/L6lnTpsfRMTrEbEcWE4S8jvTCAyV1Csi1keEh5u6MIe7tcXanOk1JHvUbRIR9wK/AKYDGyTNkPT+nbWX9NX0bJTNkjaT7AH3Sxf3b1ZT7vQgkj3t9TnrXkuyBw/JHriAxekZKF/fSQn9SfraVH9j+joD8uvxe+pak9bVT1KZpGmSnpb0CsleOTn9a80g4I6c/q0kGSbaP6fN8znT/wBaPA6SfvCMJ/nAWC/pHkkfy7MO2w053K0tDsqZ/gDwXHs2EhFXRcRhwBCSYY8pTYty20kaRDKsMBnoGxF9SIaGlDZZTzIk0VJ9a0mGIfpFRJ/08f6IOCSt4fmI+GZE9AfOBn4p6cMtlPscSYg21aT0dda1ocvN37e3SMbAvwSMA44hOT5Q1fQy6c/WTmVbC3wmp399IqIiIvKp7T3bjog/RsSxJEMyT5C899ZFOdytLSZJGihpX5Lx6pvbugFJh0saJamcZCx7G8lwAMALwAdzmr+PJIQ2puueSbLn3mQOcJ6kAZL6AN9pWpAO9fwJuFLS+yX1kPQhSZ9Kt/UFSU0fDC+nr9PIe80BTpQ0Nq35ApIPjfva0O0vSxoiqTdwGXBrRGwnGZ56g+RgaG/gR83Wa/5+NHcNcEX6IYikSknj8qzpBaBKUo903f0ljZP0vrSmrbT8flgX4XC3triRJDCfITlD4/J2bOP9JHuEL5MMUWwC/iNddh0wJB1m+F1EPE5yNsz9JGFUQ3Igt8mv0noeAR4G5gJvkwxNAHwV+CfeOcPnVpK9UoDDgQclbQXuAs6LiGeaFxsRTwJfBn5Osrf9WeCzEfFmG/p8A8nB0edJDt42Xez06/Q9WJfW+ECz9d71frSw3f9Oa/+TpFfT9UflWdMt6c9NkpaRZMG/kfyn8hLwKeBf89yW7YZ8EZPlRdJqkrNC/lzqWnZG0meAayJiUKuNO4mkhSRnpfxPqWux7sV77tZlSeol6QRJPSUNAC4hOf3RrNtzuFtXJuAHJEMuD5OcLfL9klZktpvwsIyZWQZ5z93MLIN6tt6k+Pr16xdVVVWlLsPMrEtZunTpixFR2dKy3SLcq6qqWLJkSanLMDPrUiSt2dmyVodlJFVIWixpeXqZ9g/S+TMlPZteGl4vqTadL0lXSVol6RFJIwrXFTMzy0c+e+5vAEdHxNb0Cr2/Svq/6bIpEXFrs/afAQ5OH6OAq8n/wgozMyuAVvfcI7E1fVqePnZ1is044Nfpeg8AfSQduIv2ZmZWYHmNuae3J11K8gUH0yPiQUn/SnJfi+8D84Gp6W1HB/Duu+A1pPPWN9vmRJIvJOADH/hAR/thZl3MW2+9RUNDA9u2bSt1Kbu9iooKBg4cSHl5ed7r5BXu6U2OatObM90haSjJt/I8T3LvjhkkN226LN8XjogZ6XrU1dX5ZHuzbqahoYG99tqLqqoqkpttWksigk2bNtHQ0EB1dXXe67XpPPeI2AwsAI5Pb+Yf6d76//LOFyis4923OB1I226PambdwLZt2+jbt6+DvRWS6Nu3b5v/w8nnbJnKdI8dSb1Ivm7siaZx9PT+1ieT3GcbkrvUfTU9a+YIYMuuvmnHzLovB3t+2vM+5TMscyAwKx137wHMiYi7Jd0rqZLk/h71JN/gAsltV08g+Z7JfwBntrkqMzPrkFbDPSIeAQ5tYf7RO2kfwKSOl2Zm1j5lZWXU1NTw9ttvU11dzQ033ECfPn0Ktv2mCy979uzJjTfeyDnnnFOwbRfKbnGFakdUTb2n1CUUzOppJ5a6BLNM6NWrF/X19QBMmDCB6dOn873vfa/gr7N582Z++ctf7pbh7huHmVmmHXnkkaxbl5zT8fTTT3P88cdz2GGH8YlPfIInnngCgFtuuYWhQ4cyfPhwPvnJTwIwc+ZMJk+evGM7J510EgsXLnzXtqdOncrTTz9NbW0tU6ZMYXfS5ffczcx2Zvv27cyfP5+zzjoLgIkTJ3LNNddw8MEH8+CDD3LOOedw7733ctlll/HHP/6RAQMGsHnz5ry3P23aNFasWLHjv4TdicPdzDLn9ddfp7a2lnXr1jF48GCOPfZYtm7dyn333ccXvvCFHe3eeOMNAEaPHs3XvvY1TjvtNE499dRSlV1QHpYxs8xpGnNfs2YNEcH06dNpbGykT58+1NfX73isXLkSgGuuuYbLL7+ctWvXcthhh7Fp0yZ69uxJY2Pjjm12tStpHe5mllm9e/fmqquu4sorr6R3795UV1dzyy23AMmVn8uXLweSsfhRo0Zx2WWXUVlZydq1a6mqqqK+vp7GxkbWrl3L4sWL37P9vfbai1dffbVT+5Qvh7uZZdqhhx7KsGHDuOmmm5g9ezbXXXcdw4cP55BDDuHOO+8EYMqUKdTU1DB06FCOOuoohg8fzujRo6murmbIkCGce+65jBjx3ruX9+3bl9GjRzN06FAfUDUzK7atW7e+6/nvf//7HdN/+MMf3tP+9ttvb3E7s2fPbnH+6tWrd0zfeOON7aiw+LznbmaWQQ53M7MMcribmWWQw93MLIMc7mZmGeRwNzPLIJ8KaWa7hULf4TWfu6xeccUV3HjjjZSVldGjRw+uvfZaRo0aVdA6SsXhbmbd0v3338/dd9/NsmXL2GOPPXjxxRd58803S11WwXhYxsy6pfXr19OvXz/22GMPAPr160f//v2pqqrixRdfBGDJkiWMGTMGSC6MOvPMM6mpqWHYsGHcdtttQHJR1IgRIxg+fDhjx44F4LXXXuPrX/86I0eO5NBDD91xJexjjz3GyJEjqa2tZdiwYTz11FO89tprnHjiiQwfPpyhQ4dy8803F6R/3nM3s27puOOO47LLLuMjH/kIxxxzDOPHj+dTn/rUTtv/8Ic/ZO+99+bRRx8F4OWXX2bjxo1885vfZNGiRVRXV/PSSy8ByXDP0UcfzfXXX8/mzZsZOXIkxxxzDNdccw3nnXceZ5xxBm+++Sbbt29n7ty59O/fn3vuSYaltmzZUpD+ec/dzLqlPffck6VLlzJjxgwqKysZP348M2fO3Gn7P//5z0ya9M43iO6zzz488MADfPKTn6S6uhqAfffdF4A//elPTJs2jdraWsaMGcO2bdv4+9//zpFHHsmPfvQjfvKTn7BmzRp69epFTU0N8+bN4zvf+Q5/+ctf2HvvvQvSP++5m1m3VVZWxpgxYxgzZgw1NTXMmjXrXbf6be9tfiOC2267jY9+9KPvmj948GBGjRrFPffcwwknnMC1117L0UcfzbJly5g7dy4XXXQRY8eO5fvf/36H++Y9dzPrlp588kmeeuqpHc/r6+sZNGgQVVVVLF26FGDHuDrAsccey/Tp03c8f/nllzniiCNYtGgRzz77LMCOYZlPf/rT/PznPyciAHj44YcBeOaZZ/jgBz/Iueeey7hx43jkkUd47rnn6N27N1/+8peZMmUKy5YtK0j/Wt1zl1QBLAL2SNvfGhGXSKoGfgv0BZYCX4mINyXtAfwaOAzYBIyPiNUFqdbMMquzvyB+69atfPvb32bz5s307NmTD3/4w8yYMYOVK1dy1llncfHFF+84mApw0UUXMWnSJIYOHUpZWRmXXHIJp556KjNmzODUU0+lsbGR/fbbj3nz5nHxxRdz/vnnM2zYMBobG6murubuu+9mzpw53HDDDZSXl3PAAQdw4YUX8tBDDzFlyhR69OhBeXk5V199dUH6p6ZPlp02kAS8LyK2SioH/gqcB/wbcHtE/FbSNcDyiLha0jnAsIj4lqTTgVMiYvyuXqOuri6WLFnSrg4U+tzYUursX26zUlq5ciWDBw8udRldRkvvl6SlEVHXUvtWh2Ui0XRz5PL0EcDRwK3p/FnAyen0uPQ56fKx6QeEmZl1krzG3CWVSaoHNgDzgKeBzRHxdtqkARiQTg8A1gKky7eQDN2YmVknySvcI2J7RNQCA4GRwMc6+sKSJkpaImnJxo0bO7o5MzPL0aazZSJiM7AAOBLoI6npgOxAYF06vQ44CCBdvjfJgdXm25oREXURUVdZWdnO8s3MrCWthrukSkl90ulewLHASpKQ/3zabAJwZzp9V/qcdPm90dpRWzMzK6h8LmI6EJglqYzkw2BORNwt6XHgt5IuBx4GrkvbXwfcIGkV8BJwehHqNjOzXWg13CPiEeDQFuY/QzL+3nz+NuALBanOzLqPSwtz2f0722v9Hi1lZWXU1NTseP673/2OqqqqFtseddRR3HfffaxevZqTTjqJFStWFKrSovDtB8ys2+rVqxf19fV5tb3vvvva/Trbt2+nrKys3eu3h28/YGaW2rp1K2PHjmXEiBHU1NTsuFUvJDcaa27mzJlMnjx5x/OTTjqJhQsX7mh/wQUXMHz4cO6//35+85vf7Ljd79lnn8327duL2heHu5l1W6+//jq1tbXU1tZyyimnUFFRwR133MGyZctYsGABF1xwAe09H+S1115j1KhRLF++nL59+3LzzTfzt7/9jfr6esrKypg9e3aBe/NuHpYxs26r+bDMW2+9xYUXXsiiRYvo0aMH69at44UXXuCAAw5o87bLysr43Oc+B8D8+fNZunQphx9+OJB8qOy3336F6cROONzNzFKzZ89m48aNLF26lPLycqqqqnZ529/c2wPDu28RXFFRsWOcPSKYMGECP/7xj4tXfDMeljEzS23ZsoX99tuP8vJyFixYwJo1a3bZvqqqivr6ehobG1m7di2LFy9usd3YsWO59dZb2bBhA5DcGri1bXeU99zNbPeQx6mLxXbGGWfw2c9+lpqaGurq6vjYx3Z9p5XRo0dTXV3NkCFDGDx4MCNGjGix3ZAhQ7j88ss57rjjaGxspLy8nOnTpzNo0KBidANwuJtZN7Z169Z3Pe/Xrx/333//LttWVVXtOMdd0k4PjDbf9vjx4xk/fpd3Py8oD8uYmWWQw93MLIMc7mZWMr6nYH7a8z453M2sJCoqKti0aZMDvhURwaZNm6ioqGjTej6gamYlMXDgQBoaGvCX9bSuoqKCgQMHtmkdh7uZlUR5eTnV1dWlLiOzPCxjZpZBDnczswxyuJuZZZDD3cwsgxzuZmYZ5HA3M8sgh7uZWQY53M3MMqjVcJd0kKQFkh6X9Jik89L5l0paJ6k+fZyQs853Ja2S9KSkTxezA2Zm9l75XKH6NnBBRCyTtBewVNK8dNnPIuI/cxtLGgKcDhwC9Af+LOkjEVHcr/o2M7MdWt1zj4j1EbEsnX4VWAkM2MUq44DfRsQbEfEssAoYWYhizcwsP20ac5dUBRwKPJjOmizpEUnXS9onnTcAWJuzWgMtfBhImihpiaQlvnGQmVlh5R3ukvYEbgPOj4hXgKuBDwG1wHrgyra8cETMiIi6iKirrKxsy6pmZtaKvMJdUjlJsM+OiNsBIuKFiNgeEY3Ar3hn6GUdcFDO6gPTeWZm1knyOVtGwHXAyoj4ac78A3OanQKsSKfvAk6XtIekauBgYHHhSjYzs9bkc7bMaOArwKOS6tN5FwJflFQLBLAaOBsgIh6TNAd4nORMm0k+U8bMrHO1Gu4R8VdALSyau4t1rgCu6EBdZmbWAb5C1cwsgxzuZmYZ5HA3M8sgh7uZWQY53M3MMsjhbmaWQQ53M7MMcribmWWQw93MLIMc7mZmGeRwNzPLIIe7mVkGOdzNzDLI4W5mlkEOdzOzDHK4m5llkMPdzCyDHO5mZhnkcDczy6B8viDbdmNVU+8pdQkFs3raiaUuwSwzvOduZpZBrYa7pIMkLZD0uKTHJJ2Xzt9X0jxJT6U/90nnS9JVklZJekTSiGJ3wszM3i2fPfe3gQsiYghwBDBJ0hBgKjA/Ig4G5qfPAT4DHJw+JgJXF7xqMzPbpVbDPSLWR8SydPpVYCUwABgHzEqbzQJOTqfHAb+OxANAH0kHFrxyMzPbqTaNuUuqAg4FHgT2j4j16aLngf3T6QHA2pzVGtJ5zbc1UdISSUs2btzYxrLNzGxX8g53SXsCtwHnR8QrucsiIoBoywtHxIyIqIuIusrKyrasamZmrcgr3CWVkwT77Ii4PZ39QtNwS/pzQzp/HXBQzuoD03lmZtZJ8jlbRsB1wMqI+GnOoruACen0BODOnPlfTc+aOQLYkjN8Y2ZmnSCfi5hGA18BHpVUn867EJgGzJF0FrAGOC1dNhc4AVgF/AM4s6AVm5lZq1oN94j4K6CdLB7bQvsAJnWwLjMz6wBfoWpmlkEOdzOzDHK4m5llkMPdzCyDHO5mZhnkcDczyyCHu5lZBjnczcwyyOFuZpZBDnczswxyuJuZZZDD3cwsgxzuZmYZ5HA3M8sgh7uZWQY53M3MMsjhbmaWQQ53M7MMcribmWWQw93MLIMc7mZmGdSz1AV01OqKL5W6hALaUuoCzCwjWt1zl3S9pA2SVuTMu1TSOkn16eOEnGXflbRK0pOSPl2sws3MbOfyGZaZCRzfwvyfRURt+pgLIGkIcDpwSLrOLyWVFapYMzPLT6vhHhGLgJfy3N444LcR8UZEPAusAkZ2oD4zM2uHjhxQnSzpkXTYZp903gBgbU6bhnTee0iaKGmJpCUbN27sQBlmZtZce8P9auBDQC2wHriyrRuIiBkRURcRdZWVle0sw8zMWtKucI+IFyJie0Q0Ar/inaGXdcBBOU0HpvPMzKwTtSvcJR2Y8/QUoOlMmruA0yXtIakaOBhY3LESzcysrVo9z13STcAYoJ+kBuASYIykWiCA1cDZABHxmKQ5wOPA28CkiNhenNLNzGxnWg33iPhiC7Ov20X7K4ArOlKUmZl1jG8/YGaWQQ53M7MMcribmWWQw93MLIMc7mZmGeRwNzPLIIe7mVkGOdzNzDLI4W5mlkEOdzOzDHK4m5llkMPdzCyDHO5mZhnU6l0hbfe2uuJLpS6hgLaUugCzzPCeu5lZBjnczcwyyOFuZpZBDnczswxyuJuZZZDD3cwsgxzuZmYZ1Gq4S7pe0gZJK3Lm7StpnqSn0p/7pPMl6SpJqyQ9ImlEMYs3M7OW5bPnPhM4vtm8qcD8iDgYmJ8+B/gMcHD6mAhcXZgyzcysLVoN94hYBLzUbPY4YFY6PQs4OWf+ryPxANBH0oGFKtbMzPLT3jH3/SNifTr9PLB/Oj0AWJvTriGd9x6SJkpaImnJxo0b21mGmZm1pMMHVCMigGjHejMioi4i6iorKztahpmZ5WhvuL/QNNyS/tyQzl8HHJTTbmA6z8zMOlF7w/0uYEI6PQG4M2f+V9OzZo4AtuQM35iZWSdp9Za/km4CxgD9JDUAlwDTgDmSzgLWAKelzecCJwCrgH8AZxahZjMza0Wr4R4RX9zJorEttA1gUkeLMjOzjvEVqmZmGeRwNzPLIIe7mVkGOdzNzDLI4W5mlkEOdzOzDHK4m5llkMPdzCyDHO5mZhnkcDczyyCHu5lZBjnczcwyyOFuZpZBDnczswxq9Za/Zrurqqn3lLqEglk97cRSl2AZ4z13M7MMcribmWWQw93MLIMc7mZmGeRwNzPLIIe7mVkGdehUSEmrgVeB7cDbEVEnaV/gZqAKWA2cFhEvd6xMMzNri0Lsuf9zRNRGRF36fCowPyIOBuanz83MrBMVY1hmHDArnZ4FnFyE1zAzs13oaLgH8CdJSyVNTOftHxHr0+nngf1bWlHSRElLJC3ZuHFjB8swM7NcHb39wMcjYp2k/YB5kp7IXRgRISlaWjEiZgAzAOrq6lpsY2Zm7dOhPfeIWJf+3ADcAYwEXpB0IED6c0NHizQzs7Zpd7hLep+kvZqmgeOAFcBdwIS02QTgzo4WaWZmbdORYZn9gTskNW3nxoj4g6SHgDmSzgLWAKd1vEwzM2uLdod7RDwDDG9h/iZgbEeKMsvH6oovlbqEAtpS6gIsY3yFqplZBjnczcwyyOFuZpZBDnczswxyuJuZZZDD3cwsgxzuZmYZ5HA3M8sgh7uZWQY53M3MMsjhbmaWQQ53M7MMcribmWWQw93MLIMc7mZmGeRwNzPLIIe7mVkGOdzNzDKoI9+hamYlVDX1nlKXUBCrp51Y6hIyyXvuZmYZ5D13sy4qO18Q7i8HLwbvuZuZZVDRwl3S8ZKelLRK0tRivY6Zmb1XUYZlJJUB04FjgQbgIUl3RcTjxXg9M+tesnIwGYp3QLlYY+4jgVUR8QyApN8C4wCHu5l1WHaON0CxjjkoIgq/UenzwPER8Y30+VeAURExOafNRGBi+vSjwJMFL6Sw+gEvlrqIEunOfYfu3X/3ffc2KCIqW1pQsrNlImIGMKNUr99WkpZERF2p6yiF7tx36N79d9+7bt+LdUB1HXBQzvOB6TwzM+sExQr3h4CDJVVL+ifgdOCuIr2WmZk1U5RhmYh4W9Jk4I9AGXB9RDxWjNfqRF1mCKkIunPfoXv3333voopyQNXMzErLV6iamWWQw93MLIMc7i2QdL2kDZJWlLqWztZS3yUtlNRlTwlrD0kHSVog6XFJj0k6r9Q1FZukCkmLJS1P+/yDUtfU2SSVSXpY0t2lrqWjHO4tmwkcX+oiSmQm3bfvud4GLoiIIcARwCRJQ0pcU7G9ARwdEcOBWuB4SUeUuKbOdh6wstRFFILDvQURsQh4KXeepG9Keijdq7lNUu8SlVdULfU99RVJ9ZJWSBrZ2XV1tohYHxHL0ulXSf7gB5S2quKKxNb0aXn6CEnfT3/3V0iaIUklLLNoJA0ETgT+J2del+27wz1/t0fE4elezUrgrFIX1Ml6R0QtcA5wfamL6UySqoBDgQdLW0nxpcMS9cAGYF5EPAj8Iv3dHwr0Ak4qaZHF81/AvwONOfO6bN8d7vkbKukvkh4FzgAOKXVBnewm2LFn/35JfUpcT6eQtCdwG3B+RLxS6nqKLSK2px/iA4GRkoYC/yzpwfR3/2gy+Lsv6SRgQ0Qsbbaoy/bd38SUv5nAyRGxXNLXgDElrabzNb8gIvMXSEgqJwn22RFxe6nr6UwRsVnSAuBk4NtAXUSslXQpUFHS4opjNPAvkk4g6d/7Jd0KfIIu2nfvuedvL2B9+gd/RqmLKYHxAJI+DmyJiEx/N1o6tnodsDIiflrqejqDpMqm/8gk9SL5PobV6eIX0/9iPl+i8ooqIr4bEQMjoorkdin3At9IF3fJvnvPvQWSbiLZM+8nqQG4BLiYZMx1Y/pzr5IVWEQ76TvANkkPkxxk+3qJyutMo4GvAI+mY9AAF0bE3BLWVGwHArPSL9vpAcyJiN9I+hiwAnie5L5R3UL638uv6KJ99+0HzMwyyMMyZmYZ5HA3M8sgh7uZWQY53M3MMsjhbmaWQQ53M7MMcribmWXQ/wdL4PwZ/UkV9QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "pT                     \n",
            "1a        156  185  341\n",
            "1b        119   90  209\n",
            "2          43   24   67\n",
            "3a          8    3   11\n",
            "4a          1    2    3\n",
            "All       327  304  631\n",
            "카이제곱값:  13.664121855237983 P-values : 0.18887057547647118 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "pT                                    \n",
            "1a        0.247227  0.293185  0.540412\n",
            "1b        0.188590  0.142631  0.331220\n",
            "2         0.068146  0.038035  0.106181\n",
            "3a        0.012678  0.004754  0.017433\n",
            "4a        0.001585  0.003170  0.004754\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbja9kL7c71x",
        "outputId": "022368e8-99f8-48b7-db19-14287b9b2ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        }
      },
      "source": [
        "x = ['0', '1a', '1b']\n",
        "y = Dose_main['pN'].value_counts().sort_index(ascending=True)\n",
        "\n",
        "a = ['0', '1a', '1b']\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['pN'].value_counts().sort_index(ascending=True)\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('pN stages of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "#plt.xticks([0,1,2],['TT', 'TT with CND', 'TT with CND and LND'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['pN'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "pN                     \n",
            "0          72   76  148\n",
            "1a        187  185  372\n",
            "1b         68   43  111\n",
            "All       327  304  631\n",
            "카이제곱값:  4.9176732658091495 P-values : 0.5544147786267184 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "pN                                    \n",
            "0         0.114105  0.120444  0.234548\n",
            "1a        0.296355  0.293185  0.589540\n",
            "1b        0.107765  0.068146  0.175911\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggcWADoecI3l",
        "outputId": "cdb71a60-77fd-4112-b802-050a13ec868a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "Dose_main.pM.value_counts()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    623\n",
              "1      8\n",
              "Name: pM, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09dXsBgAdYoP",
        "outputId": "c16e548a-aadf-48de-bb67-9d07f1f0072b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "x = [0, 1]\n",
        "y = Dose_main['pM'].value_counts()\n",
        "\n",
        "a = [0, 1]\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['pM'].value_counts()\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('pM stages of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "#plt.xticks([0,1,2],['TT', 'TT with CND', 'TT with CND and LND'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['pM'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "pM                     \n",
            "0         322  301  623\n",
            "1           5    3    8\n",
            "All       327  304  631\n",
            "카이제곱값:  0.37000493765790105 P-values : 0.984858196769656 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "pM                                    \n",
            "0         0.510301  0.477021  0.987322\n",
            "1         0.007924  0.004754  0.012678\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDLded7idu2V",
        "outputId": "e576af86-f62f-4a6d-a247-345735f60330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# x = [0, 1, 2]\n",
        "# y = Dose_main['Op'].value_counts()\n",
        "\n",
        "# a = [0, 1, 2]\n",
        "# b = Dose_main[Dose_main['Fubinary'] == 0]['Op'].value_counts()\n",
        "sns.boxplot(x=(1-Dose_main['Fubinary']), y=Dose_main['preTg'])\n",
        "\n",
        "# plt.boxplot(x, y, label='Success')\n",
        "# plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('stimulated Tg of patients')\n",
        "# plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['Success', 'Failure'])\n",
        "plt.xlabel('result')\n",
        "plt.ylim([0,50])\n",
        "plt.show()\n",
        "\n",
        "base_stat(Dose_main[Dose_main['Fubinary'] == 0]['preTg'], Dose_main[Dose_main['Fubinary'] == 1]['preTg'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "      preTg\n",
            "0     55.60\n",
            "1     11.00\n",
            "2     29.00\n",
            "3     16.00\n",
            "4     22.00\n",
            "..      ...\n",
            "741    4.53\n",
            "754  202.10\n",
            "769   22.80\n",
            "773   39.90\n",
            "774    1.57\n",
            "\n",
            "[327 rows x 1 columns]\n",
            "Min :  0.06\n",
            "Median :  4.0\n",
            "Max :  375.0\n",
            "Mean :  17.11874617737003\n",
            "std :  38.55639706581829\n",
            "     preTg\n",
            "12    1.00\n",
            "27    1.00\n",
            "41    1.00\n",
            "44    1.00\n",
            "47    1.00\n",
            "..     ...\n",
            "714   0.02\n",
            "729   0.18\n",
            "734   0.18\n",
            "747   0.09\n",
            "759   0.10\n",
            "\n",
            "[304 rows x 1 columns]\n",
            "Min :  0.01\n",
            "Median :  1.0\n",
            "Max :  1.0\n",
            "Mean :  0.7732861842105264\n",
            "std :  0.35254328822469716\n",
            "Compare the mean between two groups\n",
            "\n",
            "\n",
            "등분산 여부 : levene 47.429002212618414 p-value : 1.385409410761309e-11\n",
            "\n",
            "\n",
            "T값 :  7.654026669167461 p-value : 2.224492669516725e-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIstf8yGeVGi",
        "outputId": "180762cd-3f9f-4413-af42-8815a0ce4f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# x = [0, 1, 2]\n",
        "# y = Dose_main['Op'].value_counts()\n",
        "\n",
        "# a = [0, 1, 2]\n",
        "# b = Dose_main[Dose_main['Fubinary'] == 0]['Op'].value_counts()\n",
        "sns.boxplot(x=(1-Dose_main['Fubinary']), y=Dose_main['preATA'])\n",
        "\n",
        "# plt.boxplot(x, y, label='Success')\n",
        "# plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('stimulated ATA of patients')\n",
        "# plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['Success', 'Failure'])\n",
        "plt.xlabel('result')\n",
        "plt.ylim([0,150])\n",
        "plt.show()\n",
        "\n",
        "base_stat(Dose_main[Dose_main['Fubinary'] == 0]['preATA'], Dose_main[Dose_main['Fubinary'] == 1]['preATA'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hdZZ328e/dtEA5Q6kVSiFIOjAIqBCBUZAqBSvn8QCor0bFq44ooKOviuJAlRd1HEUojlIFDCoCol4WgQqtdAAFNCBSjkOEFlqhDYFyKoc2/b1/rCeLnZBT0+y1drPvz3Xlyl7nX3ZX972etdZ+liICMzMzgDFlF2BmZrXDoWBmZjmHgpmZ5RwKZmaWcyiYmVnOoWBmZjmHgo0YSddKaqnSukNSUzXW3ce2Fkr6eBHbGi5Jn5S0XNJzkiZUeVsHSXqgmtuw2uFQsGGRdKakn1WOi4h3RURrWTUBSGpMATK2ytv5SNrO8Wn4oPQB/Zyk59O05yp+dkrznZmm7b8e2x4HfBc4LCI2j4jOkfmr8vX3COCIuCkidhuhdS+WNH0k1mXV4VAwG54W4Engw5B/cG4eEZsDr0/zbN09LiIekaQ0f77cME0CNgHuWY91mPXJoWADkvRFScskPSvpAUmHSJoBfBk4Ph0F/y3Nm592SUfSf5R0jqSVkh6S9JY0/lFJKypPNfU+ZZPmu7mfmo6Q9FdJz6R1nVkx+cb0e2Wq7V/SMh+TdJ+kpyT9XtLOFes7VNL9kp6WdD6gQd6TnYGDgZnAOyW9dohv50HA9sApwAmSNhpgGxtL+p6kf6Sf76Vx/wR0n8pZKekPfSzb3VqamZZ9TNLnK6bvJ+mW9O/ymKTzu2uR1P3+/S29f8dLmiZpacXyO0j6laQOSQ9LOqVi2pmSrpB0Sdpn7pHUnKb9FNgJuCqt+wuSNpH0M0mdqZ6/SJo0xPfTqiEi/OOfPn+A3YBHgR3ScCOwa3p9JvCzXvMvBD6eXn8EWAN8FGgAzgIeAb4PbAwcBjwLbN572Yrlb64YDqApvZ4G7EV2ULM3sBw4tqLGAMZWLHsM0A78MzAWOB34U5q2XarjvcA44LOp7o8P8L58Ffhzer0I+Fyv6a+qIY2/ELgibacTeM8A2/gacCvwGmAi8Cfg6wOtv4/t/wLYLL1XHcD0NH1f4ID0XjQC9wGf6eu9rni/l6bXY4Dbgf8ANgJeBzwEvLNiv3gRODz9u38DuLViXYu760jDnwCuAjZN8+8LbFn2vl/PP24p2EC6yD7A95A0LiIWR8Tf12H5hyPi4ojoAi4HpgBfi4iXIuI64GVgnS8eR8TCiFgUEWsj4i6yD7+DB1jk34BvRMR9EbEGOBt4YzriPxy4JyKujIjVwPeAxwcp4cPApen1pQzhVJCkTYH3AZem7Vw5yHIfJHuvVkREBzAL+NBg2+llVkQ8HxGLgIuB9wNExO0RcWtErImIxcAFDPz+VXozMDEivhYRL0fEQ8CPgBMq5rk5Iq5J/+4/Bd4wwPpWAxPIQqgr1fbMOv2VNqIcCtaviGgHPkN29LdC0mWSdliHVSyveP1CWmfvcZuva12S9pd0Qzp98TTZh/52AyyyM3BuOj2xkuycvoDJwA5krSFSfVE53Me23wrsAlyWRl0K7CXpjYOU/a9kLZBr0vDPgXdJmtjP/DsASyqGl6Rx66Ly78iXl/RPkn4n6XFJz5CF5EDvX6WdgR2638v0fn6Z7DpHt8pQXQVsov4v/P8U+D1wWTrV9Z/pQrqVxKFgA4qISyPiQLIPgwC+1T1phDf1PNkphG4Dnae/FJgLTImIrYAf8sp1gL7qehT4RERsXfEzPiL+BDxG1oIBIF0MntLHOrq1pG3dKelx4LaK8QNpIQvAR9JyvyQ7jfSBfub/B9l73m2nNG5dVP4dlcv/ALgfmBoRW5J9qA94HaXCo2QtwMr3couIOHyIy/f494mI1RExKyL2AN4CHMn6XYS39eRQsH5J2k3SOyRtTHae+AVgbZq8HGiUNFL70J3AuyVtmm6HPHGAebcAnoyIFyXtR88P1o5U4+sqxv0QOE3S6wEkbSXpfWna1cDrJb07Hc2eQj+BJGkT4DiyC8xvrPg5GfhAf0fDkiYDh5B94HUv8waygO3vA/AXwOmSJkrajuwc/s/6mbc/X03v5+vJru1cnsZvATwDPCdpd+CTvZZbTs/3r9KfgWeV3YAwXlKDpD0lvXmINfVYt6S3S9pLUkOqaTWv7GNWAoeCDWRj4JvAE2SnBF4DnJam/TL97pR0xwhs6xyyawzLgVay0yv9OQn4mqRnyT4sr+ieEBGrgP8H/DGd3jggIn5D9gF8WTpdcjfwrjT/E2Tn+r9JdvF3KvDHfrZ7LFkwXhIRj3f/ABeRXbSd0c9yHwLujIjrei13HrC3pD37WOYsoA24i+xi9h1p3Lr4H7IL7AuA/0rXcQA+Txakz5JdD7i813JnAq3p/TuuckK6TtAdbg+T7Rs/BrYaYk3fIAu7lemOqNeSXV95huyC9/+QnVKykig7hWpmo4WkRrIP7HHpwrrZkLmlYGZmOYeCmZnlfPrIzMxybimYmVmuqj1JVtt2220XjY2NZZcxKixfvpyVK1ey9dZbM2mSu56x2vDggw+ydu0rd6iOGTOGqVOnlljR6HD77bc/ERF9fnFygw6FxsZG2trayi5jVOjs7GTWrFmcccYZTJhQ1e75zYbsu9/9LldffTVdXV00NDRw5JFH8tnPfrbssjZ4kpb0N82njwyACRMmcN555zkQrKa0tLR0d5xHRPDhD/vLztXmUDAzs5xDwcxqVmtrK2PGZB9TY8aM4ZJLLim5otHPoWBmNWv+/PmsWZN9KXvNmjVcf/31JVc0+jkUzKxmTZ8+nbFjs/thxo4dy6GHHlpyRaOfQ8GA7O6jU045hc7OEX0GvNl6aWlpyU8fNTQ0+EJzARwKBmTnbhctWuRztlZTJkyYwNvf/nYApk2b5rvjCuBQMDo7O5k3bx4Rwbx589xasJry4osvAvDSSy+VXEl9cCgYra2t+bdGu7q63FqwmtHZ2clNN90EwI033ugDlgI4FMx3eFjNuuCCC/IDlrVr1zJnzpySKxr9qhYKki6StELS3X1M+5ykSI8ZRJnzJLVLukvSPtWqy17Nd3hYrVqwYEGP4fnz55dUSf2oZkvhJ/TxeEJJU4DDgEcqRr+L7DGIU8mef/uDKtZlvbS0tPQ4GvMdHlYrJA04bCOvaqEQETcCT/Yx6RzgC0DlgxyOIXvubUTErcDWkravVm1mtmE45JBDBhy2kVfoNQVJxwDLIuJvvSZNBh6tGF6axvW1jpmS2iS1dXR0VKnS+tLa2pofgUnyhWarGTNnzuyxb86cObPkika/wkJB0qbAl4H/WJ/1RMSciGiOiOaJE/vsDtzW0fz58+nq6gKyu498odlqkU8dFaPIlsKuwC7A3yQtBnYE7pD0WmAZMKVi3h3TOCuALzRbrarsEM+t2GIUFgoRsSgiXhMRjRHRSHaKaJ+IeByYC3w43YV0APB0RDxWVG31zl0JWK1yK7Z41bwl9RfALcBukpZKOnGA2a8BHgLagR8BJ1WrLnu1CRMmMGPGDCQxY8YMdyVgNWP69Ok9rim4FVt9VXscZ0S8f5DpjRWvA/hUtWqxwbW0tLB48WK3EqymHH300cydOxfInrx21FFHlVzR6OdvNBvgx3FabZo7d26PlsJVV11VckWjn0PBzGrW/Pnzezyj2dcUqs+hYGY1y3fGFc+hYAC0t7dzxBFH0N7eXnYpZjnfGVc8h4IBcNZZZ/H8889z1llnlV2KWc53xhXPoWC0t7ezePFiABYvXuzWgtWUlpYW9tprL7cSCuJQsFe1DtxasFriO+OK5VCwvJXQ37BZmTo7OznllFP81LWCOBSMxsbGAYfNytTa2sqiRYvc71FBHArG6aefPuCwWVk6OzuZN28eEcG8efPcWiiAQ8FoamrKWweNjY00NTWVW5BZ0tramneIt2bNGrcWCuBQMCBrHWy22WZuJVhNcS+pxXMoGJC1Fq6++mq3EqymHHjggT2GDzrooJIqqR8OBTOrWX7aWvEcCgb4tj+rTTfddNOAwzbyHAoG+LY/q03uEK94DgXzbX9Ws9whXvEcCkZraytr164Fsjs83FqwWuEO8YrnUDDmz5/PmjVrgOxecN/2Z7XEHeIVy6FgPm9rNe2pp57i73//O0899VTZpdSFqoWCpIskrZB0d8W4b0u6X9Jdkn4jaeuKaadJapf0gKR3VqsuezWft7Va5md9FKuaLYWfADN6jbse2DMi9gb+FzgNQNIewAnA69My/y2poYq1WQWft7Va5Wd9FK9qoRARNwJP9hp3XUSsSYO3Ajum18cAl0XESxHxMNAO7Fet2uzVfN7WapGf9VG8Mq8pfAy4Nr2eDDxaMW1pGvcqkmZKapPU1tHRUeUS64cfZGK1yM/6KF4poSDpK8Aa4OfrumxEzImI5ohonjhx4sgXZ2Y1w8/6KF7hoSDpI8CRwAcjItLoZcCUitl2TOPMrI75WR/FKzQUJM0AvgAcHRGrKibNBU6QtLGkXYCpwJ+LrM3Mao+f9VG8at6S+gvgFmA3SUslnQicD2wBXC/pTkk/BIiIe4ArgHuBecCnIqKrWrWZ2YbDz/ooll45g7PhaW5ujra2trLLGBU6OzuZNWsWZ5xxhi82m41ykm6PiOa+pvkbzQbABRdcwF133cWcOXPKLsWsB3frXiyHgtHZ2cl1110HwHXXXef/fFZT3K17sRwKxgUXXJC/jgi3FqxmuFv34jkUjPnz5/cYdi+pVivcrXvxHAqW/6frb9isLO7WvXgOBTOrWdOnT0cSAJLcrXsBHArGZpttNuCwWVmOPvpoum+bjwiOOuqokisa/RwKRldX14DDZmWZO3duj5bCVVddVXJFo59DwTj44IMHHDYry/z583u0FHxNofocCsaG/K12G938qNjiORSMm2++ucfwTTfdVFIlZj35UbHFcygY06dPp6Ehe/ppQ0ODj8asZvhRscVzKBgtLS15KIwdO9ZHY1ZT/KjYYjkUjAkTJjBt2jQApk2b5qMxqyl+VGyxHAoGkN/2Z1Zr3EtqsRwKRmdnJzfccAMACxcu9H8+qynuJbVYDgVzp2NWszo7O7n22muJCK699lofsBTAoWDudMxqVmtra75vrl692gcsBXAoGNOnT+8x7FtSrVZcf/31Pb7R3P0wKKseh4Lxtre9bcBhs7JMmjRpwGEbeVULBUkXSVoh6e6KcdtKul7Sg+n3Nmm8JJ0nqV3SXZL2qVZd9mrnn39+j+HZs2eXVIlZT8uXLx9w2EZeNVsKPwFm9Br3JWBBREwFFqRhgHcBU9PPTOAHVazLelm8ePGAw2ZlOfTQQ3v0knrYYYeVXNHoN7ZaK46IGyU19hp9DDAtvW4FFgJfTOMviezk4a2Stpa0fUQ8Vq367BWNjY09gqCxsbG0Wqy2zJ49m/b29tK2v3r16h4dNj744IOceuqppdXT1NTEySefXNr2i1D0NYVJFR/0jwPdJwgnA49WzLc0jXsVSTMltUlq6+joqF6ldeT0008fcNisLOPGjct7Sd12220ZN25cyRWNflVrKQwmIkLSOvfZHBFzgDkAzc3NG3yfz2UfiXUbM2YMa9euZeONNy71mkI9HIltSGrh3+Kkk05iyZIlzJkzx11dFKDolsJySdsDpN8r0vhlwJSK+XZM46wgG220EQA777xzyZWY9TRu3DiampocCAUpuqUwF2gBvpl+/7Zi/KclXQbsDzxdL9cTauFIDMjP05577rklV2JmZapaKEj6BdlF5e0kLQXOIAuDKySdCCwBjkuzXwMcDrQDq4CPVqsuMzPrXzXvPnp/P5MO6WPeAD5VrVrMzGxo/I1mMzPLORTMzCznUDAzs5xDwczMcg4FMzPLORTMzCznUDAzs5xDwczMcg4FMzPLORTMzCznUDAzs5xDwczMcg4FMzPLORTMzCznUDAzs9ywQ0HSm0eyEDMzK986PWRH0h7A+9PPSqC5GkWZmVk5Bg0FSY28EgSrgZ2B5ohYXM3CzMyseAOePpJ0C3A1WXi8JyL2BZ51IJiZjU6DXVNYDmwBTAImpnFR1YrMzKw0A4ZCRBwL7AXcDpwp6WFgG0n7rc9GJX1W0j2S7pb0C0mbSNpF0m2S2iVdLmmj9dmGmZmtu8FOH707Ip6OiIsj4jBgf+CrwDmSHh3OBiVNBk4huy6xJ9AAnAB8CzgnIpqAp4ATh7N+MzMbvsFOH51eORARKyLi/Ih4K3Dgemx3LDBe0lhgU+Ax4B3AlWl6K3DseqzfzMyGYdjfU4iIJcNcbhnwX8AjZGHwNNnpqZURsSbNthSY3NfykmZKapPU1tHRMZwSzMysH4Pdkrq7pLv6GC8gImLvdd2gpG2AY4BdyL7r8EtgxlCXj4g5wByA5uZmX/Q2MxtBg4XCw8BRI7zN6cDDEdEBIOnXwFuBrSWNTa2FHYFlI7xdMzMbxGCh8PJwTxMN4BHgAEmbAi8AhwBtwA3Ae4HLgBbgtyO8XTMzG8Rg1xT+2HuEpF0lfVXSPcPZYETcRnZB+Q5gUaphDvBF4N8ltQMTgAuHs34zMxu+AVsKEfFpAEk7AMcDHyD73sI3yG4jHZaIOAM4o9foh4D1+v6DmZmtn8G+pzBT0g3AQrKj9xOBxyJiVkQsKqA+MzMr0GDXFM4HbgE+EBFtAJJ8x4+Z2Sg1WChsD7wP+I6k1wJXAOOqXpWZmZVisL6POiPihxFxMNldQiuB5ZLuk3R2IRWamVlhhvyN5ohYGhHfiYhm4GjgxeqVZWZmZRhSKEjaNN2GOqdi9B1VqsnMzEoy1JbCxcBLwFvS8DLgrKpUZGZmpRlqKOwaEf9J9jhOImIVWf9HZmY2igw1FF6WNJ701DVJu5K1HMzMbBQZ7JbUbmcA84Apkn5O1oHdR6pVlJmZlWPQUJA0BtgGeDdwANlpo1Mj4okq12ZmZgUbNBQiYq2kL0TEFcDVBdRkZmYlGeo1hfmSPi9piqRtu3+qWpmZmRVuqNcUjie7yHxSr/GvG9lyzMysTEMNhT3IAuFAsnC4CfhhtYoyM7NyDDUUWoFngPPS8AfSuOOqUZSZmZVjqKGwZ0TsUTF8g6R7q1GQmZmVZ6gXmu+QdED3gKT9yZ6rbGZmo8hQWwr7An+S9Ega3gl4QNIiICJi76pUZ2ZmhRpqKMyoahVmZlYThhQKEbFkJDcqaWvgx8CeZHczfQx4ALgcaAQWA8dFxFMjuV0zMxvYkB+yM8LOBeZFxO7AG4D7gC8BCyJiKrAgDZuZWYEKDwVJWwFvAy4EiIiXI2IlcAzZba6k38cWXZuZWb0ro6WwC9ABXCzpr5J+LGkzYFJEPJbmeRyY1NfCkmZKapPU1tHRUVDJZmb1oYxQGAvsA/wgIt4EPE+vU0UREaRnN/QWEXMiojkimidOnFj1Ys3M6kkZobAUWBoRt6XhK8lCYrmk7QHS7xUl1GZmVtcKD4WIeBx4VNJuadQhwL3AXKAljWsBflt0bWZm9W6o31MYaScDP5e0EfAQ8FGygLpC0onAEtyvkplZ4UoJhYi4E2juY9IhRddiZmavKOt7CmZmVoMcCmZmlnMomJlZzqFgZmY5h4KZmeUcCmZmlnMomJlZzqFgZmY5h4KZmeUcCmZmllPWS/WGqbm5Odra2oa9/OzZs2lvbx/BijZc3e9DU1NTyZXUhqamJk4++eTStu998xXeN3saiX1T0u0R0VdXQ6V1iFcT2tvbufPu++jadNuySyndmJezg4PbH1peciXla1j1ZNkl0N7ezoP3/JWdNu8qu5TSbbQ6O6Hx0pLhHwCOFo8811D1bdR1KAB0bbotL+x+eNllWA0Zf/81ZZcAwE6bd/HlfZ4puwyrIWffsWXVt+FrCmZmlnMomJlZzqFgZmY5h4KZmeUcCmZmlnMomJlZzqFgZma50kJBUoOkv0r6XRreRdJtktolXS5po7JqMzOrV2W2FE4F7qsY/hZwTkQ0AU8BJ5ZSlZlZHSslFCTtCBwB/DgNC3gHcGWapRU4tozazMzqWVkthe8BXwDWpuEJwMqIWJOGlwKT+1pQ0kxJbZLaOjo6ql+pmVkdKTwUJB0JrIiI24ezfETMiYjmiGieOHHiCFdnZlbfyugQ763A0ZIOBzYBtgTOBbaWNDa1FnYElpVQm5lZXSu8pRARp0XEjhHRCJwA/CEiPgjcALw3zdYC/Lbo2szM6l0tfU/hi8C/S2onu8ZwYcn1mJnVnVKfpxARC4GF6fVDwH5l1mNmVu9qqaVgZmYlcyiYmVnOoWBmZjmHgpmZ5RwKZmaWcyiYmVnOoWBmZjmHgpmZ5RwKZmaWcyiYmVnOoWBmZjmHgpmZ5RwKZmaWcyiYmVnOoWBmZjmHgpmZ5Up9yE7Zli1bRsOqpxl//zVll2I1pGFVJ8uWrSm7DLNS1HUomNWqZcuW8fyzDZx9x5Zll2I1ZMmzDWy2bFlVt1HXoTB58mQef2ksL+x+eNmlWA0Zf/81TJ48qewyzEpR16FgVqsmT57MS2se48v7PFN2KVZDzr5jSzaePLmq2yj8QrOkKZJukHSvpHsknZrGbyvpekkPpt/bFF2bmVm9K+PuozXA5yJiD+AA4FOS9gC+BCyIiKnAgjRsZmYFKjwUIuKxiLgjvX4WuA+YDBwDtKbZWoFji67NzKzelfo9BUmNwJuA24BJEfFYmvQ40OeVPkkzJbVJauvo6CikTjOzelFaKEjaHPgV8JmI6HE1LSICiL6Wi4g5EdEcEc0TJ04soFIzs/pRSihIGkcWCD+PiF+n0cslbZ+mbw+sKKM2M7N6VsbdRwIuBO6LiO9WTJoLtKTXLcBvi67NzKzelfE9hbcCHwIWSbozjfsy8E3gCkknAkuA40qozcysrhUeChFxM6B+Jh9SZC1mZtaTe0k1M7Nc3Xdz0bDqSfeSCox5MbsBbO0m7oCtYdWT9HNHtNmoV9eh0NTUVHYJNaO9/VkAml7nD0OY5H3D6lZdh8LJJ59cdgk149RTTwXg3HPPLbkSMytTXYeCWS175Dk/TwFg+ars0uekTdeWXEn5HnmugalV3oZDwawG+fTVK15ubwdg4539nkyl+vuGQ8GsBvnU5it8arNYviXVzMxyDgUzM8s5FMzMLOdQMDOznEPBzMxyDgUzM8s5FMzMLOdQMDOznEPBzMxyDgUzM8s5FMzMLOdQMDOznEPBzMxyioiya+hB0gzgXKAB+HFEfLO/eZubm6Otra2w2qph9uzZtKeugcvUXUPZXTY3NTW5h9AaUgv7Z63sm901jIb9U9LtEdHc17Sa6jpbUgPwfeBQYCnwF0lzI+Lecisb/caPH192CWZ98r5ZrJpqKUj6F+DMiHhnGj4NICK+0df8o6GlYGZWtA2mpQBMBh6tGF4K7F85g6SZwMw0+JykBwqqrR5sBzxRdhFmffC+ObJ27m9CrYXCoCJiDjCn7DpGI0lt/R09mJXJ+2Zxau3uo2XAlIrhHdM4MzMrQK2Fwl+AqZJ2kbQRcAIwt+SazMzqRk2dPoqINZI+Dfye7JbUiyLinpLLqic+LWe1yvtmQWrq7iMzMytXrZ0+MjOzEjkUzMws51DYwEn6iqR7JN0l6U5J+w++lFnxJHWlfbT7p3GAef+UfjdKuruoGq3GLjTbuknfAD8S2CciXpK0HbBRyWWZ9eeFiHjjUGaMiLcMdyOSGiKia7jL1zu3FDZs2wNPRMRLABHxRET8Q9LiFBBIapa0ML3eXNLFkhallsV70vgZku6Q9DdJC9K4zSRdJOnPkv4q6Zg0/vVp3J1pHVPTvFen5e+WdHwZb4ZtWNL+uCDte4u697E07bk+5v+IpPMrhn8naVr3/JK+I+lvwL9I+j8V++kFqV81GwKHwobtOmCKpP+V9N+SDh5k/q8CT0fEXhGxN/AHSROBHwHviYg3AO9L834F+ENE7Ae8Hfi2pM2AfwPOTUd8zWRdkcwA/hERb4iIPYF5I/2H2qgwvuLU0W+AF4F/jYh9yPax70jSMNe9GXBb2oc7geOBt6b9tAv44AjUXxd8+mgDFhHPSdoXOIjsP9Xlkr40wCLTyb4Q2L38U5KOAm6MiIfTuCfT5MOAoyV9Pg1vAuwE3AJ8RdKOwK8j4kFJi8j+Q38L+F1E3DSCf6aNHj1OH0kaB5wt6W3AWrK+zyYBjw9j3V3Ar9LrQ4B9yXpZBhgPrFiPuuuKQ2EDl86dLgQWpg/nFmANr7QCNxnmqkXWeujd4eB9km4DjgCukfSJiPiDpH2Aw4GzJC2IiK8Nc7tWPz4ITAT2jYjVkhYz8P5auV/Ta94XK64jCGiNiNNGsth64dNHGzBJu0maWjHqjcASYDHZkRLAeyqmXw98qmL5bYBbgbdJ2iWN2zZN/j1wcndzXtKb0u/XAQ9FxHnAb4G9Je0ArIqInwHfBvYZyb/TRq2tgBUpEN7OAD13JouBN0oaI2kKsF8/8y0A3ivpNZDt05IGW7clbils2DYHZkvamuwoqp2sW/F/Bi6U9HWyVkS3s4Dvp1v8uoBZEfHr1B35ryWNIWtmHwp8HfgecFca/zDZnU7HAR+StJqsmX828Gayaw5rgdXAJ6v7Z9so8XPgqtTCbQPuH2T+P5Lth/cC9wF39DVTRNwr6XTgurTvriY7GFoyUoWPZu7mwszMcj59ZGZmOYeCmZnlHApmZpZzKJiZWc6hYGZmOYeCWQkq+/GRdKykPcquyQwcCmbrRJmR/n9zLOBQsJrgUDAbROrT/wFJlwB3A1+V9JfUS+ysNE+fPcX212NtxbrfAhxN9uW/OyXtWugfZ9aLv9FsNjRTyfqV2hJ4L1kXCwLmpg7dJpL1FHsEgKSthrLSiPiTpLlkHQleWZXKzdaBWwpmQ7MkIm4l6z32MOCvZN0s7E4WGIuAQyV9S9JBEfF0eaWaDZ9bCmZD83z6LeAbEXFB7xn66Sl2JHqsNSuMWwpm6+b3wMckbQ4gabKk1wzQU+xi+u6xttKzwBbVK9ls6BwKZusgIq4DLgVuSb17Xkn2gb4X8GdJdwJnkPVICzALOFdSG1nPtH25DPi/6ZHtGn4AAAA4SURBVLGnvtBspXIvqWZmlnNLwczMcg4FMzPLORTMzCznUDAzs5xDwczMcg4FMzPLORTMzCz3/wHzA7bVFnXeiAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "     preATA\n",
            "0     128.0\n",
            "1       2.0\n",
            "2       2.0\n",
            "3       2.0\n",
            "4       2.0\n",
            "..      ...\n",
            "741    20.6\n",
            "754    23.0\n",
            "769    29.6\n",
            "773    18.6\n",
            "774    18.2\n",
            "\n",
            "[327 rows x 1 columns]\n",
            "Min :  1.0\n",
            "Median :  23.0\n",
            "Max :  1509.0\n",
            "Mean :  59.71987767584094\n",
            "std :  140.46520829903992\n",
            "     preATA\n",
            "12      1.0\n",
            "27    139.0\n",
            "41     53.0\n",
            "44     49.0\n",
            "47    291.0\n",
            "..      ...\n",
            "714    22.1\n",
            "729    26.1\n",
            "734    21.5\n",
            "747    30.0\n",
            "759    94.9\n",
            "\n",
            "[304 rows x 1 columns]\n",
            "Min :  1.0\n",
            "Median :  24.0\n",
            "Max :  2180.0\n",
            "Mean :  66.27809210526314\n",
            "std :  195.9058623512752\n",
            "Compare the mean between two groups\n",
            "\n",
            "\n",
            "등분산 여부 : levene 0.103854127857098 p-value : 0.7473597335978639\n",
            "\n",
            "\n",
            "T값 :  -0.4849963252490429 p-value : 0.6278478132541939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpoG7plWeeQH",
        "outputId": "fde97f94-137e-48cd-ba30-a324488f3475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# x = [0, 1, 2]\n",
        "# y = Dose_main['Op'].value_counts()\n",
        "\n",
        "# a = [0, 1, 2]\n",
        "# b = Dose_main[Dose_main['Fubinary'] == 0]['Op'].value_counts()\n",
        "sns.boxplot(x=(1-Dose_main['Fubinary']), y=Dose_main['preTSH'])\n",
        "\n",
        "# plt.boxplot(x, y, label='Success')\n",
        "# plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('stimulated TSH of patients')\n",
        "# plt.legend(title=\"Result\")\n",
        "plt.xticks([0,1],['Success', 'Failure'])\n",
        "plt.xlabel('result')\n",
        "plt.show()\n",
        "\n",
        "base_stat(Dose_main[Dose_main['Fubinary'] == 0]['preTSH'], Dose_main[Dose_main['Fubinary'] == 1]['preTSH'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "     preTSH\n",
            "0      1.00\n",
            "1     69.80\n",
            "2     61.20\n",
            "3     59.89\n",
            "4     58.46\n",
            "..      ...\n",
            "741   70.54\n",
            "754   84.45\n",
            "769   59.57\n",
            "773   58.95\n",
            "774   41.87\n",
            "\n",
            "[327 rows x 1 columns]\n",
            "Min :  1.0\n",
            "Median :  58.49\n",
            "Max :  116.64\n",
            "Mean :  58.68703363914374\n",
            "std :  20.042206049005554\n",
            "     preTSH\n",
            "12    51.32\n",
            "27    57.75\n",
            "41    61.40\n",
            "44    64.19\n",
            "47    59.91\n",
            "..      ...\n",
            "714    0.01\n",
            "729   32.50\n",
            "734    0.37\n",
            "747   23.77\n",
            "759   68.44\n",
            "\n",
            "[304 rows x 1 columns]\n",
            "Min :  0.01\n",
            "Median :  52.0\n",
            "Max :  115.57\n",
            "Mean :  50.03659868421052\n",
            "std :  25.718070229172117\n",
            "Compare the mean between two groups\n",
            "\n",
            "\n",
            "등분산 여부 : levene 23.147254800508836 p-value : 1.8804161665386569e-06\n",
            "\n",
            "\n",
            "T값 :  4.680988137719117 p-value : 3.5683486175293654e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGozW_EPWf8Q",
        "outputId": "a12b2e5a-42fe-4448-f0a0-679bdd8f668e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "Dose_main['dose'].value_counts().sort_index(ascending=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30       1\n",
              "50      15\n",
              "80      75\n",
              "100    237\n",
              "150    271\n",
              "180     26\n",
              "200      3\n",
              "300      2\n",
              "350      1\n",
              "Name: dose, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uknHDjJBfByl",
        "outputId": "207316d0-d53d-4930-c7a6-4ee937530067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "x = ['30', '50', '80', '100', '150', '180', '200', '300', '350']\n",
        "y = Dose_main['dose'].value_counts().sort_index(ascending=True)\n",
        "\n",
        "a = ['30', '50', '80', '100', '150', '180', '200', '300', '350']\n",
        "b = Dose_main[Dose_main['Fubinary'] == 0]['dose'].value_counts().sort_index(ascending=True)\n",
        "plt.bar(x, y, label='Success')\n",
        "plt.bar(a, b, label='Failure')\n",
        "\n",
        "plt.title('Doses of patients')\n",
        "plt.legend(title=\"Result\")\n",
        "plt.xticks(['30', '50', '80', '100', '150', '180', '200', '300', '350'])\n",
        "plt.show()\n",
        "\n",
        "Xtab(index=Dose_main['dose'], columns=Dose_main['Fubinary'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fubinary    0    1  All\n",
            "dose                   \n",
            "30          1    0    1\n",
            "50          1   14   15\n",
            "80         31   44   75\n",
            "100       108  129  237\n",
            "150       161  110  271\n",
            "180        19    7   26\n",
            "200         3    0    3\n",
            "300         2    0    2\n",
            "350         1    0    1\n",
            "All       327  304  631\n",
            "카이제곱값:  36.72745158363891 P-values : 0.005685852137159882 \n",
            "\n",
            "Fubinary         0         1       All\n",
            "dose                                  \n",
            "30        0.001585  0.000000  0.001585\n",
            "50        0.001585  0.022187  0.023772\n",
            "80        0.049128  0.069731  0.118859\n",
            "100       0.171157  0.204437  0.375594\n",
            "150       0.255151  0.174326  0.429477\n",
            "180       0.030111  0.011094  0.041204\n",
            "200       0.004754  0.000000  0.004754\n",
            "300       0.003170  0.000000  0.003170\n",
            "350       0.001585  0.000000  0.001585\n",
            "All       0.518225  0.481775  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl-H35QLORV9",
        "outputId": "ae4447cd-0e4e-46df-f256-653427a7f74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "Dose_main['Fubinary'].value_counts().sort_index(ascending=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    327\n",
              "1    304\n",
              "Name: Fubinary, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfSZV0XiQvDO"
      },
      "source": [
        "# Dose_main['Op'] = Dose_main['Op'].replace({'TT':1, 'TT CND':2, 'TT RND':3})\n",
        "# Dose_main['pT'] = Dose_main['pT'].replace({'1a':1, '1b':2, '2':3, '3a':4, '4a':5})\n",
        "# Dose_main['cT'] = Dose_main['cT'].replace({'1a':1, '1b':2, '2':3, '3a':4, '3b':5, '4a':6})\n",
        "# Dose_main['pN'] = Dose_main['pN'].replace({'0':1, '1a':1, '1b':2})"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MQp95StQjHW",
        "outputId": "7ea37c28-a14a-4d25-ee33-7f3fcdc2ec69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "## Losgistic regression\n",
        "\n",
        "# age MinMax scaling\n",
        "Dose_main['age_MM'] = (Dose_main['age'] - min(Dose_main['age']))/(max(Dose_main['age']) - min(Dose_main['age']))\n",
        "\n",
        "# Sex Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.sex)], axis=1)\n",
        "Dose_main.rename(columns={1:'Male'}, inplace=True)\n",
        "Dose_main.rename(columns={2:'Female'}, inplace=True)\n",
        "\n",
        "# HTN Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.HTN)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoHTN'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'HTN'}, inplace=True)\n",
        "\n",
        "# DM Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.DM)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoDM'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'DM'}, inplace=True)\n",
        "\n",
        "# TB Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.TB)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoTB'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'TB'}, inplace=True)\n",
        "\n",
        "# Hepatitis Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.Hepatitis)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoHepatitis'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'Hepatitis'}, inplace=True)\n",
        "\n",
        "# OtherthyroidD Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.OtherthyroidD)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoOtherthyroidD'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'OtherthyroidD'}, inplace=True)\n",
        "\n",
        "# Op Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.Op)], axis=1)\n",
        "\n",
        "'''\n",
        "# Biopsy Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.biopsy)], axis=1)\n",
        "Dose_main.rename(columns={1:'Papillary'}, inplace=True)\n",
        "Dose_main.rename(columns={2:'Follicular'}, inplace=True)\n",
        "'''\n",
        "\n",
        "# Tumorsize MinMax scaling\n",
        "Dose_main['tumorsize_MM'] = (np.log(Dose_main['tumorsize']) - min(np.log(Dose_main['tumorsize'])))/(max(np.log(Dose_main['tumorsize'])) - min(np.log(Dose_main['tumorsize'])))\n",
        "\n",
        "# pT Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.pT)], axis=1)\n",
        "Dose_main.rename(columns={'1a':'pT1a'}, inplace=True)\n",
        "Dose_main.rename(columns={'1b':'pT1b'}, inplace=True)\n",
        "Dose_main.rename(columns={'2':'pT2'}, inplace=True)\n",
        "Dose_main.rename(columns={'3a':'pT3a'}, inplace=True)\n",
        "Dose_main.rename(columns={'4a':'pT4a'}, inplace=True)\n",
        "\n",
        "# pN Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.pN)], axis=1)\n",
        "Dose_main.rename(columns={'0':'N0'}, inplace=True)\n",
        "Dose_main.rename(columns={'1a':'N1a'}, inplace=True)\n",
        "Dose_main.rename(columns={'1b':'N1b'}, inplace=True)\n",
        "Dose_main.rename(columns={' 1b':'N1b'}, inplace=True)\n",
        "\n",
        "# pM Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.pM)], axis=1)\n",
        "Dose_main.rename(columns={0:'M0'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'M1'}, inplace=True)\n",
        "\n",
        "# cT Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.cT)], axis=1)\n",
        "Dose_main.rename(columns={'1a':'cT1a'}, inplace=True)\n",
        "Dose_main.rename(columns={'1b':'cT1b'}, inplace=True)\n",
        "Dose_main.rename(columns={'2':'cT2'}, inplace=True)\n",
        "Dose_main.rename(columns={'3a':'cT3a'}, inplace=True)\n",
        "Dose_main.rename(columns={'3b':'cT3b'}, inplace=True)\n",
        "Dose_main.rename(columns={'4a':'cT4a'}, inplace=True)\n",
        "\n",
        "\n",
        "# preTg MinMax scaling\n",
        "Dose_main['preTg_MM'] = (np.log(Dose_main['preTg']) - min(np.log(Dose_main['preTg'])))/(max(np.log(Dose_main['preTg'])) - min(np.log(Dose_main['preTg'])))\n",
        "\n",
        "# preATA MinMax scaling\n",
        "Dose_main['preATA_MM'] = (np.log(Dose_main['preATA']) - min(np.log(Dose_main['preATA'])))/(max(np.log(Dose_main['preATA']) )- min(np.log(Dose_main['preATA'])))\n",
        "\n",
        "# preTSH MinMax scaling\n",
        "Dose_main['preTSH_MM'] = (Dose_main['preTSH'] - min(Dose_main['preTSH']))/(max(Dose_main['preTSH']) - min(Dose_main['preTSH']))\n",
        "\n",
        "'''\n",
        "# preparation Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.preparation)], axis=1)\n",
        "Dose_main.rename(columns={1:'THW'}, inplace=True)\n",
        "Dose_main.rename(columns={2:'rhTSH'}, inplace=True)\n",
        "'''\n",
        "\n",
        "# Fubinary Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.Fubinary)], axis=1)\n",
        "Dose_main.rename(columns={0:'AblationSuccess'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'AblationFailure'}, inplace=True)\n",
        "\n",
        "'''\n",
        "# dose Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.dose)], axis=1)\n",
        "Dose_main.rename(columns={30:'30'}, inplace=True)\n",
        "Dose_main.rename(columns={50:'50'}, inplace=True)\n",
        "Dose_main.rename(columns={80:'80'}, inplace=True)\n",
        "Dose_main.rename(columns={100:'100'}, inplace=True)\n",
        "Dose_main.rename(columns={150:'150'}, inplace=True)\n",
        "Dose_main.rename(columns={180:'180'}, inplace=True)\n",
        "Dose_main.rename(columns={200:'200'}, inplace=True)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# model = LogisticRegression()\n",
        "# model.fit(features, labels)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# dose Onehot encoding\\nDose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.dose)], axis=1)\\nDose_main.rename(columns={30:'30'}, inplace=True)\\nDose_main.rename(columns={50:'50'}, inplace=True)\\nDose_main.rename(columns={80:'80'}, inplace=True)\\nDose_main.rename(columns={100:'100'}, inplace=True)\\nDose_main.rename(columns={150:'150'}, inplace=True)\\nDose_main.rename(columns={180:'180'}, inplace=True)\\nDose_main.rename(columns={200:'200'}, inplace=True)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NpD2Nkb3-W9"
      },
      "source": [
        "# age MinMax scaling\n",
        "Dose_main['age_MM'] = (Dose_main['age'] - min(Dose_main['age']))/(max(Dose_main['age']) - min(Dose_main['age']))\n",
        "\n",
        "# Sex Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.sex)], axis=1)\n",
        "Dose_main.rename(columns={1:'Male'}, inplace=True)\n",
        "Dose_main.rename(columns={2:'Female'}, inplace=True)\n",
        "\n",
        "# HTN Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.HTN)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoHTN'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'HTN'}, inplace=True)\n",
        "\n",
        "# DM Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.DM)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoDM'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'DM'}, inplace=True)\n",
        "\n",
        "# TB Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.TB)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoTB'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'TB'}, inplace=True)\n",
        "\n",
        "# Hepatitis Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.Hepatitis)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoHepatitis'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'Hepatitis'}, inplace=True)\n",
        "\n",
        "# OtherthyroidD Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.OtherthyroidD)], axis=1)\n",
        "Dose_main.rename(columns={0:'NoOtherthyroidD'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'OtherthyroidD'}, inplace=True)\n",
        "\n",
        "# Op Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.Op)], axis=1)\n",
        "\n",
        "'''\n",
        "# Biopsy Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.biopsy)], axis=1)\n",
        "Dose_main.rename(columns={1:'Papillary'}, inplace=True)\n",
        "Dose_main.rename(columns={2:'Follicular'}, inplace=True)\n",
        "'''\n",
        "\n",
        "# Tumorsize MinMax scaling\n",
        "Dose_main['tumorsize_MM'] = (np.log(Dose_main['tumorsize']) - min(np.log(Dose_main['tumorsize'])))/(max(np.log(Dose_main['tumorsize'])) - min(np.log(Dose_main['tumorsize'])))\n",
        "\n",
        "# pT Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.pT)], axis=1)\n",
        "Dose_main.rename(columns={'1a':'pT1a'}, inplace=True)\n",
        "Dose_main.rename(columns={'1b':'pT1b'}, inplace=True)\n",
        "Dose_main.rename(columns={'2':'pT2'}, inplace=True)\n",
        "Dose_main.rename(columns={'3a':'pT3a'}, inplace=True)\n",
        "Dose_main.rename(columns={'4a':'pT4a'}, inplace=True)\n",
        "\n",
        "# pN Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.pN)], axis=1)\n",
        "Dose_main.rename(columns={'0':'N0'}, inplace=True)\n",
        "Dose_main.rename(columns={'1a':'N1a'}, inplace=True)\n",
        "Dose_main.rename(columns={'1b':'N1b'}, inplace=True)\n",
        "Dose_main.rename(columns={' 1b':'N1b'}, inplace=True)\n",
        "\n",
        "# pM Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.pM)], axis=1)\n",
        "Dose_main.rename(columns={0:'M0'}, inplace=True)\n",
        "Dose_main.rename(columns={1:'M1'}, inplace=True)\n",
        "\n",
        "# cT Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.cT)], axis=1)\n",
        "Dose_main.rename(columns={'1a':'cT1a'}, inplace=True)\n",
        "Dose_main.rename(columns={'1b':'cT1b'}, inplace=True)\n",
        "Dose_main.rename(columns={'2':'cT2'}, inplace=True)\n",
        "Dose_main.rename(columns={'3a':'cT3a'}, inplace=True)\n",
        "Dose_main.rename(columns={'3b':'cT3b'}, inplace=True)\n",
        "Dose_main.rename(columns={'4a':'cT4a'}, inplace=True)\n",
        "\n",
        "\n",
        "# preTg MinMax scaling\n",
        "Dose_main['preTg_MM'] = (np.log(Dose_main['preTg']) - min(np.log(Dose_main['preTg'])))/(max(np.log(Dose_main['preTg'])) - min(np.log(Dose_main['preTg'])))\n",
        "\n",
        "# preATA MinMax scaling\n",
        "Dose_main['preATA_MM'] = (np.log(Dose_main['preATA']) - min(np.log(Dose_main['preATA'])))/(max(np.log(Dose_main['preATA']) )- min(np.log(Dose_main['preATA'])))\n",
        "\n",
        "# preTSH MinMax scaling\n",
        "Dose_main['preTSH_MM'] = (Dose_main['preTSH'] - min(Dose_main['preTSH']))/(max(Dose_main['preTSH']) - min(Dose_main['preTSH']))\n",
        "\n",
        "'''\n",
        "# preparation Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.preparation)], axis=1)\n",
        "Dose_main.rename(columns={1:'THW'}, inplace=True)\n",
        "Dose_main.rename(columns={2:'rhTSH'}, inplace=True)\n",
        "'''\n",
        "\n",
        "# Fubinary Onehot encoding\n",
        "# Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.Fubinary)], axis=1)\n",
        "# Dose_main.rename(columns={0:'AblationSuccess'}, inplace=True)\n",
        "# Dose_main.rename(columns={1:'AblationFailure'}, inplace=True)\n",
        "\n",
        "'''\n",
        "# dose Onehot encoding\n",
        "Dose_main = pd.concat([Dose_main, pd.get_dummies(Dose_main.dose)], axis=1)\n",
        "Dose_main.rename(columns={30:'30'}, inplace=True)\n",
        "Dose_main.rename(columns={50:'50'}, inplace=True)\n",
        "Dose_main.rename(columns={80:'80'}, inplace=True)\n",
        "Dose_main.rename(columns={100:'100'}, inplace=True)\n",
        "Dose_main.rename(columns={150:'150'}, inplace=True)\n",
        "Dose_main.rename(columns={180:'180'}, inplace=True)\n",
        "Dose_main.rename(columns={200:'200'}, inplace=True)\n",
        "'''\n",
        "\n",
        "\n",
        "# Delete original columns\n",
        "Dose_main = Dose_main.drop(['age', 'sex', 'HTN', 'DM', 'TB', 'Hepatitis', 'OtherthyroidD', 'Op', 'biopsy', 'tumorsize', 'pT', 'pN', 'pM', 'cT', 'preTg', 'preATA', 'preTSH'], axis='columns')\n",
        "\n",
        "#Dose_main.describe()\n",
        "#Dose_main.columns\n",
        "\n",
        "Dose_main_X = Dose_main[['dose', 'age_MM', 'Male', 'Female', 'NoHTN', 'NoDM', 'NoTB',\n",
        "       'NoHepatitis', 'NoOtherthyroidD', 'TT', 'TT CND', 'TT RND',\n",
        "       'tumorsize_MM', 'pT1a', 'pT1b', 'pT2', 'pT3a', 'pT4a', 'N0', 'N1a',\n",
        "       'N1b', 'M0', 'M1', 'cT1a', 'cT1b', 'cT2', 'cT3a', 'cT3b', 'cT4a',\n",
        "       'preTg_MM', 'preATA_MM', 'preTSH_MM',]]\n",
        "\n",
        "Dose_main_Y = Dose_main[['Fubinary']]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiiCDwAZXiug"
      },
      "source": [
        "# # split dataset\n",
        "# X_train, X_test, y_train, y_test = train_test_split(Dose_main_X, Dose_main_Y, test_size= 0.2, shuffle=True, random_state=20200819)\n",
        "# X_train = np.asarray(X_train, dtype=np.float32)\n",
        "# y_train = np.asarray(y_train, dtype=np.float32)\n",
        "# X_test = np.asarray(X_test, dtype=np.float32)\n",
        "# y_test = np.asarray(y_test, dtype=np.float32)\n",
        "\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras.layers import Input, Dense\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from keras.utils import np_utils\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "# from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# import numpy\n",
        "\n",
        "# # Create model\n",
        "# def create_model():\n",
        "#     model = Sequential()\n",
        "#     #model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "#     #model.add(Dropout(0.25))\n",
        "#     model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "#     model.add(Dropout(0.25))\n",
        "#     #model.add(Dense(16, activation='relu'))\n",
        "#     #model.add(Dropout(0.25))\n",
        "#     #model.add(Dense(8, activation='relu'))\n",
        "#     #model.add(Dropout(0.25))\n",
        "#     model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
        "#     opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "#     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # fix random seed for reproducibility\n",
        "# seed = 20200819\n",
        "# numpy.random.seed(seed)\n",
        "\n",
        "# # 모델 실행\n",
        "# model=create_model()\n",
        "# model.fit(X_train, y_train, epochs=1000, batch_size=X_train.shape[0])\n",
        "# #model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
        "# #kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "# #results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
        "\n",
        "# model.evaluate(X_test, y_test)\n",
        "# print(model.metrics_names)\n",
        "\n",
        "# yhat = model.predict(X_test)[:,1]\n",
        "# for idx in range(len(yhat)):\n",
        "#     if yhat[idx] >= 0.5:\n",
        "#         yhat[idx] = 1\n",
        "#     else:\n",
        "#         yhat[idx] = 0 \n",
        "\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# confusion_matrix(y_test[:,1], yhat)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y1NoaHpeclQ",
        "outputId": "e497c9c3-2d2d-4853-905d-59cb785684f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "# split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(Dose_main_X, Dose_main_Y, test_size= 0.2, shuffle=True, random_state=20200819)\n",
        "X_train = np.asarray(X_train, dtype=np.float32)\n",
        "y_train = np.asarray(y_train, dtype=np.float32)\n",
        "X_test = np.asarray(X_test, dtype=np.float32)\n",
        "y_test = np.asarray(y_test, dtype=np.float32)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "\n",
        "# Create model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.25))    \n",
        "    #model.add(Dense(4, input_dim=X_train.shape[1], activation='sigmoid'))\n",
        "    #model.add(Dropout(0.25))\n",
        "    #model.add(Dense(2, activation='relu'))\n",
        "    #model.add(Dropout(0.25))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    opt = optimizers.Adam(learning_rate=0.04)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 20200819\n",
        "np.random.seed(seed)\n",
        "\n",
        "# 모델 실행\n",
        "model=create_model()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                3456      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,521\n",
            "Trainable params: 3,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwwiPTwJf58g",
        "outputId": "f6982676-c8a6-47a4-c968-1155940cdbab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytdHdLmPgxFY"
      },
      "source": [
        "# 출력 폴더를 생성\n",
        "odir = \"output\"\n",
        "if not os.path.exists(odir):\n",
        "    os.mkdir(odir)\n",
        "weight_path = odir + \"/weights.hdf5\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuSlokaJf2BL",
        "outputId": "73c34eff-712d-458c-db22-655a47b4fec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# hist = model.fit(X_train, y_train, epochs=1000, batch_size=X_train.shape[0])\n",
        "\n",
        "hist = model.fit(X_train, y_train, epochs=1000, batch_size=X_train.shape[0], validation_split= 0.2,\n",
        "                 callbacks=[ModelCheckpoint(monitor='val_accuracy', filepath=weight_path, verbose=1, save_best_only=True)\n",
        "                            #ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_delta=1e-5)\n",
        "                            #EarlyStopping(monitor='accuracy', mode='min', verbose=1, patience=100)\n",
        "                            ])\n",
        "\n",
        "#model.fit(X_train, y_train, epochs=1000, batch_size=X_train.shape[0]//10)\n",
        "#model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
        "#kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "#results = cross_val_score(model, X_train, y_train, cv=kfold)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 7.6320 - accuracy: 0.5087\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.46535, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 7.6320 - accuracy: 0.5087 - val_loss: 9.4524 - val_accuracy: 0.4653\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.2955 - accuracy: 0.5136\n",
            "Epoch 00002: val_accuracy did not improve from 0.46535\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 9.2955 - accuracy: 0.5136 - val_loss: 6.6300 - val_accuracy: 0.4653\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 6.6676 - accuracy: 0.5136\n",
            "Epoch 00003: val_accuracy improved from 0.46535 to 0.52475, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.6676 - accuracy: 0.5136 - val_loss: 0.6898 - val_accuracy: 0.5248\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.3019 - accuracy: 0.5161\n",
            "Epoch 00004: val_accuracy improved from 0.52475 to 0.53465, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 3.3019 - accuracy: 0.5161 - val_loss: 4.5112 - val_accuracy: 0.5347\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 4.6636 - accuracy: 0.5161\n",
            "Epoch 00005: val_accuracy did not improve from 0.53465\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.6636 - accuracy: 0.5161 - val_loss: 5.1594 - val_accuracy: 0.5347\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.1825 - accuracy: 0.5037\n",
            "Epoch 00006: val_accuracy did not improve from 0.53465\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.1825 - accuracy: 0.5037 - val_loss: 3.6059 - val_accuracy: 0.5347\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.5129 - accuracy: 0.5136\n",
            "Epoch 00007: val_accuracy improved from 0.53465 to 0.55446, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 3.5129 - accuracy: 0.5136 - val_loss: 1.1162 - val_accuracy: 0.5545\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1959 - accuracy: 0.4541\n",
            "Epoch 00008: val_accuracy did not improve from 0.55446\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.1959 - accuracy: 0.4541 - val_loss: 1.6712 - val_accuracy: 0.4653\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.6242 - accuracy: 0.5236\n",
            "Epoch 00009: val_accuracy did not improve from 0.55446\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.6242 - accuracy: 0.5236 - val_loss: 2.8694 - val_accuracy: 0.4653\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9396 - accuracy: 0.5558\n",
            "Epoch 00010: val_accuracy did not improve from 0.55446\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.9396 - accuracy: 0.5558 - val_loss: 2.7909 - val_accuracy: 0.4653\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.1788 - accuracy: 0.4988\n",
            "Epoch 00011: val_accuracy did not improve from 0.55446\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.1788 - accuracy: 0.4988 - val_loss: 1.8391 - val_accuracy: 0.4653\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2456 - accuracy: 0.5484\n",
            "Epoch 00012: val_accuracy did not improve from 0.55446\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2456 - accuracy: 0.5484 - val_loss: 0.7572 - val_accuracy: 0.4752\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3953 - accuracy: 0.5335\n",
            "Epoch 00013: val_accuracy improved from 0.55446 to 0.56436, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.3953 - accuracy: 0.5335 - val_loss: 0.9531 - val_accuracy: 0.5644\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3588 - accuracy: 0.5285\n",
            "Epoch 00014: val_accuracy did not improve from 0.56436\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3588 - accuracy: 0.5285 - val_loss: 1.5117 - val_accuracy: 0.5347\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7060 - accuracy: 0.5211\n",
            "Epoch 00015: val_accuracy did not improve from 0.56436\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7060 - accuracy: 0.5211 - val_loss: 1.6673 - val_accuracy: 0.5347\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6452 - accuracy: 0.4938\n",
            "Epoch 00016: val_accuracy did not improve from 0.56436\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.6452 - accuracy: 0.4938 - val_loss: 1.4721 - val_accuracy: 0.5347\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4549 - accuracy: 0.5310\n",
            "Epoch 00017: val_accuracy did not improve from 0.56436\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.4549 - accuracy: 0.5310 - val_loss: 1.0842 - val_accuracy: 0.5644\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2407 - accuracy: 0.5310\n",
            "Epoch 00018: val_accuracy did not improve from 0.56436\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.2407 - accuracy: 0.5310 - val_loss: 0.7462 - val_accuracy: 0.5545\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9324 - accuracy: 0.5335\n",
            "Epoch 00019: val_accuracy improved from 0.56436 to 0.57426, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.9324 - accuracy: 0.5335 - val_loss: 0.6820 - val_accuracy: 0.5743\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9120 - accuracy: 0.5211\n",
            "Epoch 00020: val_accuracy did not improve from 0.57426\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9120 - accuracy: 0.5211 - val_loss: 0.8126 - val_accuracy: 0.4653\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9069 - accuracy: 0.5236\n",
            "Epoch 00021: val_accuracy did not improve from 0.57426\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9069 - accuracy: 0.5236 - val_loss: 0.9133 - val_accuracy: 0.4653\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8871 - accuracy: 0.5434\n",
            "Epoch 00022: val_accuracy did not improve from 0.57426\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8871 - accuracy: 0.5434 - val_loss: 0.9191 - val_accuracy: 0.4653\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9123 - accuracy: 0.5037\n",
            "Epoch 00023: val_accuracy did not improve from 0.57426\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9123 - accuracy: 0.5037 - val_loss: 0.8611 - val_accuracy: 0.4653\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8898 - accuracy: 0.5186\n",
            "Epoch 00024: val_accuracy did not improve from 0.57426\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8898 - accuracy: 0.5186 - val_loss: 0.7859 - val_accuracy: 0.4653\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8102 - accuracy: 0.4963\n",
            "Epoch 00025: val_accuracy did not improve from 0.57426\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8102 - accuracy: 0.4963 - val_loss: 0.7247 - val_accuracy: 0.4653\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7249 - accuracy: 0.5261\n",
            "Epoch 00026: val_accuracy did not improve from 0.57426\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7249 - accuracy: 0.5261 - val_loss: 0.6894 - val_accuracy: 0.5149\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.5658\n",
            "Epoch 00027: val_accuracy improved from 0.57426 to 0.61386, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6821 - accuracy: 0.5658 - val_loss: 0.6772 - val_accuracy: 0.6139\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.6030\n",
            "Epoch 00028: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6759 - accuracy: 0.6030 - val_loss: 0.6775 - val_accuracy: 0.5545\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6694 - accuracy: 0.5732\n",
            "Epoch 00029: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6694 - accuracy: 0.5732 - val_loss: 0.6798 - val_accuracy: 0.5941\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6708 - accuracy: 0.5955\n",
            "Epoch 00030: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6708 - accuracy: 0.5955 - val_loss: 0.6811 - val_accuracy: 0.5941\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.6253\n",
            "Epoch 00031: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6630 - accuracy: 0.6253 - val_loss: 0.6820 - val_accuracy: 0.5842\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6736 - accuracy: 0.5583\n",
            "Epoch 00032: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6736 - accuracy: 0.5583 - val_loss: 0.6823 - val_accuracy: 0.5743\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6744 - accuracy: 0.5633\n",
            "Epoch 00033: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6744 - accuracy: 0.5633 - val_loss: 0.6821 - val_accuracy: 0.5842\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.5732\n",
            "Epoch 00034: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6695 - accuracy: 0.5732 - val_loss: 0.6816 - val_accuracy: 0.5842\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.5682\n",
            "Epoch 00035: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6722 - accuracy: 0.5682 - val_loss: 0.6811 - val_accuracy: 0.5941\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6710 - accuracy: 0.5881\n",
            "Epoch 00036: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6710 - accuracy: 0.5881 - val_loss: 0.6806 - val_accuracy: 0.6040\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.5931\n",
            "Epoch 00037: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6696 - accuracy: 0.5931 - val_loss: 0.6803 - val_accuracy: 0.5743\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6679 - accuracy: 0.5980\n",
            "Epoch 00038: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6679 - accuracy: 0.5980 - val_loss: 0.6800 - val_accuracy: 0.5842\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6686 - accuracy: 0.6278\n",
            "Epoch 00039: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6686 - accuracy: 0.6278 - val_loss: 0.6797 - val_accuracy: 0.5743\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.6129\n",
            "Epoch 00040: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6661 - accuracy: 0.6129 - val_loss: 0.6795 - val_accuracy: 0.5743\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.6203\n",
            "Epoch 00041: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6692 - accuracy: 0.6203 - val_loss: 0.6793 - val_accuracy: 0.5644\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.6203\n",
            "Epoch 00042: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6657 - accuracy: 0.6203 - val_loss: 0.6791 - val_accuracy: 0.5644\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6670 - accuracy: 0.6005\n",
            "Epoch 00043: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6670 - accuracy: 0.6005 - val_loss: 0.6789 - val_accuracy: 0.5743\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.6278\n",
            "Epoch 00044: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6674 - accuracy: 0.6278 - val_loss: 0.6788 - val_accuracy: 0.5743\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6678 - accuracy: 0.6154\n",
            "Epoch 00045: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6678 - accuracy: 0.6154 - val_loss: 0.6786 - val_accuracy: 0.5743\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6644 - accuracy: 0.6154\n",
            "Epoch 00046: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6644 - accuracy: 0.6154 - val_loss: 0.6784 - val_accuracy: 0.5743\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6611 - accuracy: 0.6203\n",
            "Epoch 00047: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6611 - accuracy: 0.6203 - val_loss: 0.6783 - val_accuracy: 0.5743\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.6129\n",
            "Epoch 00048: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6657 - accuracy: 0.6129 - val_loss: 0.6781 - val_accuracy: 0.5743\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6655 - accuracy: 0.6079\n",
            "Epoch 00049: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6655 - accuracy: 0.6079 - val_loss: 0.6780 - val_accuracy: 0.5842\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6623 - accuracy: 0.6203\n",
            "Epoch 00050: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6623 - accuracy: 0.6203 - val_loss: 0.6778 - val_accuracy: 0.5842\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6599 - accuracy: 0.6402\n",
            "Epoch 00051: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6599 - accuracy: 0.6402 - val_loss: 0.6776 - val_accuracy: 0.5842\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6712 - accuracy: 0.6129\n",
            "Epoch 00052: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6712 - accuracy: 0.6129 - val_loss: 0.6775 - val_accuracy: 0.5842\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6595 - accuracy: 0.6228\n",
            "Epoch 00053: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6595 - accuracy: 0.6228 - val_loss: 0.6774 - val_accuracy: 0.5842\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6708 - accuracy: 0.5931\n",
            "Epoch 00054: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6708 - accuracy: 0.5931 - val_loss: 0.6773 - val_accuracy: 0.5842\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6628 - accuracy: 0.6228\n",
            "Epoch 00055: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6628 - accuracy: 0.6228 - val_loss: 0.6772 - val_accuracy: 0.5842\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6629 - accuracy: 0.6104\n",
            "Epoch 00056: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6629 - accuracy: 0.6104 - val_loss: 0.6771 - val_accuracy: 0.5743\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6654 - accuracy: 0.6253\n",
            "Epoch 00057: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6654 - accuracy: 0.6253 - val_loss: 0.6770 - val_accuracy: 0.5743\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6617 - accuracy: 0.6253\n",
            "Epoch 00058: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6617 - accuracy: 0.6253 - val_loss: 0.6769 - val_accuracy: 0.5743\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.6253\n",
            "Epoch 00059: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6610 - accuracy: 0.6253 - val_loss: 0.6769 - val_accuracy: 0.5743\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6667 - accuracy: 0.6154\n",
            "Epoch 00060: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6667 - accuracy: 0.6154 - val_loss: 0.6768 - val_accuracy: 0.5743\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6708 - accuracy: 0.6179\n",
            "Epoch 00061: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6708 - accuracy: 0.6179 - val_loss: 0.6767 - val_accuracy: 0.5743\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6623 - accuracy: 0.6104\n",
            "Epoch 00062: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6623 - accuracy: 0.6104 - val_loss: 0.6765 - val_accuracy: 0.5743\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6601 - accuracy: 0.6228\n",
            "Epoch 00063: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6601 - accuracy: 0.6228 - val_loss: 0.6763 - val_accuracy: 0.5743\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6597 - accuracy: 0.6253\n",
            "Epoch 00064: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6597 - accuracy: 0.6253 - val_loss: 0.6762 - val_accuracy: 0.5743\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6605 - accuracy: 0.6203\n",
            "Epoch 00065: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6605 - accuracy: 0.6203 - val_loss: 0.6760 - val_accuracy: 0.5743\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6647 - accuracy: 0.6203\n",
            "Epoch 00066: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6647 - accuracy: 0.6203 - val_loss: 0.6758 - val_accuracy: 0.5743\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6574 - accuracy: 0.6328\n",
            "Epoch 00067: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6574 - accuracy: 0.6328 - val_loss: 0.6756 - val_accuracy: 0.5743\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.6402\n",
            "Epoch 00068: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6553 - accuracy: 0.6402 - val_loss: 0.6755 - val_accuracy: 0.5743\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6660 - accuracy: 0.6129\n",
            "Epoch 00069: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6660 - accuracy: 0.6129 - val_loss: 0.6754 - val_accuracy: 0.5842\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6503 - accuracy: 0.6228\n",
            "Epoch 00070: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6503 - accuracy: 0.6228 - val_loss: 0.6753 - val_accuracy: 0.5842\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6582 - accuracy: 0.6228\n",
            "Epoch 00071: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6582 - accuracy: 0.6228 - val_loss: 0.6752 - val_accuracy: 0.5842\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6588 - accuracy: 0.6328\n",
            "Epoch 00072: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6588 - accuracy: 0.6328 - val_loss: 0.6751 - val_accuracy: 0.5842\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6532 - accuracy: 0.6203\n",
            "Epoch 00073: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6532 - accuracy: 0.6203 - val_loss: 0.6750 - val_accuracy: 0.5842\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6560 - accuracy: 0.6179\n",
            "Epoch 00074: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6560 - accuracy: 0.6179 - val_loss: 0.6749 - val_accuracy: 0.5842\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6605 - accuracy: 0.6253\n",
            "Epoch 00075: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6605 - accuracy: 0.6253 - val_loss: 0.6748 - val_accuracy: 0.5842\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6575 - accuracy: 0.6154\n",
            "Epoch 00076: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6575 - accuracy: 0.6154 - val_loss: 0.6747 - val_accuracy: 0.5842\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.6228\n",
            "Epoch 00077: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6528 - accuracy: 0.6228 - val_loss: 0.6746 - val_accuracy: 0.5842\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6601 - accuracy: 0.6303\n",
            "Epoch 00078: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6601 - accuracy: 0.6303 - val_loss: 0.6745 - val_accuracy: 0.5842\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6530 - accuracy: 0.6203\n",
            "Epoch 00079: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6530 - accuracy: 0.6203 - val_loss: 0.6745 - val_accuracy: 0.5842\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6475 - accuracy: 0.6452\n",
            "Epoch 00080: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6475 - accuracy: 0.6452 - val_loss: 0.6744 - val_accuracy: 0.5842\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6477 - accuracy: 0.6352\n",
            "Epoch 00081: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6477 - accuracy: 0.6352 - val_loss: 0.6744 - val_accuracy: 0.5842\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.6526\n",
            "Epoch 00082: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6456 - accuracy: 0.6526 - val_loss: 0.6744 - val_accuracy: 0.5842\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6524 - accuracy: 0.6328\n",
            "Epoch 00083: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6524 - accuracy: 0.6328 - val_loss: 0.6744 - val_accuracy: 0.5842\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6543 - accuracy: 0.6129\n",
            "Epoch 00084: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6543 - accuracy: 0.6129 - val_loss: 0.6744 - val_accuracy: 0.5842\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6535 - accuracy: 0.6179\n",
            "Epoch 00085: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6535 - accuracy: 0.6179 - val_loss: 0.6743 - val_accuracy: 0.5842\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.6154\n",
            "Epoch 00086: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6608 - accuracy: 0.6154 - val_loss: 0.6742 - val_accuracy: 0.5842\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6575 - accuracy: 0.6055\n",
            "Epoch 00087: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6575 - accuracy: 0.6055 - val_loss: 0.6739 - val_accuracy: 0.5842\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.6203\n",
            "Epoch 00088: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6528 - accuracy: 0.6203 - val_loss: 0.6737 - val_accuracy: 0.5842\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6550 - accuracy: 0.6253\n",
            "Epoch 00089: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6550 - accuracy: 0.6253 - val_loss: 0.6735 - val_accuracy: 0.5743\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6545 - accuracy: 0.6228\n",
            "Epoch 00090: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6545 - accuracy: 0.6228 - val_loss: 0.6733 - val_accuracy: 0.5743\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6489 - accuracy: 0.6303\n",
            "Epoch 00091: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6489 - accuracy: 0.6303 - val_loss: 0.6732 - val_accuracy: 0.5743\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6471 - accuracy: 0.6501\n",
            "Epoch 00092: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6471 - accuracy: 0.6501 - val_loss: 0.6732 - val_accuracy: 0.5743\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.6154\n",
            "Epoch 00093: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6528 - accuracy: 0.6154 - val_loss: 0.6732 - val_accuracy: 0.5743\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.6476\n",
            "Epoch 00094: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6458 - accuracy: 0.6476 - val_loss: 0.6733 - val_accuracy: 0.5644\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.6030\n",
            "Epoch 00095: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6579 - accuracy: 0.6030 - val_loss: 0.6733 - val_accuracy: 0.5644\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6482 - accuracy: 0.6352\n",
            "Epoch 00096: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6482 - accuracy: 0.6352 - val_loss: 0.6733 - val_accuracy: 0.5743\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.6402\n",
            "Epoch 00097: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6451 - accuracy: 0.6402 - val_loss: 0.6734 - val_accuracy: 0.5743\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6500 - accuracy: 0.6278\n",
            "Epoch 00098: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6500 - accuracy: 0.6278 - val_loss: 0.6736 - val_accuracy: 0.5743\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6479 - accuracy: 0.6328\n",
            "Epoch 00099: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6479 - accuracy: 0.6328 - val_loss: 0.6738 - val_accuracy: 0.5743\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6464 - accuracy: 0.6228\n",
            "Epoch 00100: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6464 - accuracy: 0.6228 - val_loss: 0.6740 - val_accuracy: 0.5743\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.6303\n",
            "Epoch 00101: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6496 - accuracy: 0.6303 - val_loss: 0.6739 - val_accuracy: 0.5743\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.6005\n",
            "Epoch 00102: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6564 - accuracy: 0.6005 - val_loss: 0.6736 - val_accuracy: 0.5743\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6546 - accuracy: 0.6055\n",
            "Epoch 00103: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6546 - accuracy: 0.6055 - val_loss: 0.6732 - val_accuracy: 0.5743\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6411 - accuracy: 0.6377\n",
            "Epoch 00104: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6411 - accuracy: 0.6377 - val_loss: 0.6727 - val_accuracy: 0.5743\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6468 - accuracy: 0.6253\n",
            "Epoch 00105: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6468 - accuracy: 0.6253 - val_loss: 0.6724 - val_accuracy: 0.5644\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.6328\n",
            "Epoch 00106: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6531 - accuracy: 0.6328 - val_loss: 0.6721 - val_accuracy: 0.5644\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6465 - accuracy: 0.6352\n",
            "Epoch 00107: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6465 - accuracy: 0.6352 - val_loss: 0.6719 - val_accuracy: 0.5644\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.6253\n",
            "Epoch 00108: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6513 - accuracy: 0.6253 - val_loss: 0.6717 - val_accuracy: 0.5644\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6518 - accuracy: 0.6203\n",
            "Epoch 00109: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6518 - accuracy: 0.6203 - val_loss: 0.6716 - val_accuracy: 0.5743\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6005\n",
            "Epoch 00110: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6495 - accuracy: 0.6005 - val_loss: 0.6714 - val_accuracy: 0.5743\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6532 - accuracy: 0.6079\n",
            "Epoch 00111: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6532 - accuracy: 0.6079 - val_loss: 0.6713 - val_accuracy: 0.5941\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.6377\n",
            "Epoch 00112: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6392 - accuracy: 0.6377 - val_loss: 0.6712 - val_accuracy: 0.5743\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.6179\n",
            "Epoch 00113: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6418 - accuracy: 0.6179 - val_loss: 0.6712 - val_accuracy: 0.5743\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6359 - accuracy: 0.6352\n",
            "Epoch 00114: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6359 - accuracy: 0.6352 - val_loss: 0.6714 - val_accuracy: 0.5644\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6490 - accuracy: 0.6030\n",
            "Epoch 00115: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6490 - accuracy: 0.6030 - val_loss: 0.6715 - val_accuracy: 0.5644\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6466 - accuracy: 0.6129\n",
            "Epoch 00116: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6466 - accuracy: 0.6129 - val_loss: 0.6715 - val_accuracy: 0.5743\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.6328\n",
            "Epoch 00117: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6418 - accuracy: 0.6328 - val_loss: 0.6713 - val_accuracy: 0.5743\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6427 - accuracy: 0.6452\n",
            "Epoch 00118: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6427 - accuracy: 0.6452 - val_loss: 0.6710 - val_accuracy: 0.5644\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6444 - accuracy: 0.6328\n",
            "Epoch 00119: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6444 - accuracy: 0.6328 - val_loss: 0.6707 - val_accuracy: 0.5743\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6426 - accuracy: 0.6203\n",
            "Epoch 00120: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6426 - accuracy: 0.6203 - val_loss: 0.6703 - val_accuracy: 0.5842\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.6476\n",
            "Epoch 00121: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6458 - accuracy: 0.6476 - val_loss: 0.6700 - val_accuracy: 0.5842\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6349 - accuracy: 0.6377\n",
            "Epoch 00122: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6349 - accuracy: 0.6377 - val_loss: 0.6698 - val_accuracy: 0.5842\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.6154\n",
            "Epoch 00123: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6445 - accuracy: 0.6154 - val_loss: 0.6697 - val_accuracy: 0.5842\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.6501\n",
            "Epoch 00124: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6435 - accuracy: 0.6501 - val_loss: 0.6697 - val_accuracy: 0.5842\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.6452\n",
            "Epoch 00125: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6370 - accuracy: 0.6452 - val_loss: 0.6699 - val_accuracy: 0.5842\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.6278\n",
            "Epoch 00126: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6407 - accuracy: 0.6278 - val_loss: 0.6699 - val_accuracy: 0.5842\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6310 - accuracy: 0.6402\n",
            "Epoch 00127: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6310 - accuracy: 0.6402 - val_loss: 0.6700 - val_accuracy: 0.5941\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6314 - accuracy: 0.6600\n",
            "Epoch 00128: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6314 - accuracy: 0.6600 - val_loss: 0.6701 - val_accuracy: 0.5842\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.6551\n",
            "Epoch 00129: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6300 - accuracy: 0.6551 - val_loss: 0.6701 - val_accuracy: 0.5842\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.6228\n",
            "Epoch 00130: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6533 - accuracy: 0.6228 - val_loss: 0.6695 - val_accuracy: 0.5941\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6345 - accuracy: 0.6476\n",
            "Epoch 00131: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6345 - accuracy: 0.6476 - val_loss: 0.6689 - val_accuracy: 0.5941\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6468 - accuracy: 0.6203\n",
            "Epoch 00132: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6468 - accuracy: 0.6203 - val_loss: 0.6681 - val_accuracy: 0.6040\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6331 - accuracy: 0.6402\n",
            "Epoch 00133: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6331 - accuracy: 0.6402 - val_loss: 0.6678 - val_accuracy: 0.5941\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.6551\n",
            "Epoch 00134: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6360 - accuracy: 0.6551 - val_loss: 0.6676 - val_accuracy: 0.6040\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.6228\n",
            "Epoch 00135: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6367 - accuracy: 0.6228 - val_loss: 0.6676 - val_accuracy: 0.5941\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.6576\n",
            "Epoch 00136: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6328 - accuracy: 0.6576 - val_loss: 0.6677 - val_accuracy: 0.5941\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.6526\n",
            "Epoch 00137: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6249 - accuracy: 0.6526 - val_loss: 0.6685 - val_accuracy: 0.6040\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6250 - accuracy: 0.6526\n",
            "Epoch 00138: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6250 - accuracy: 0.6526 - val_loss: 0.6698 - val_accuracy: 0.6040\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6303 - accuracy: 0.6551\n",
            "Epoch 00139: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6303 - accuracy: 0.6551 - val_loss: 0.6708 - val_accuracy: 0.5842\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.6600\n",
            "Epoch 00140: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6317 - accuracy: 0.6600 - val_loss: 0.6710 - val_accuracy: 0.5842\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6212 - accuracy: 0.6749\n",
            "Epoch 00141: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6212 - accuracy: 0.6749 - val_loss: 0.6710 - val_accuracy: 0.5842\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6459 - accuracy: 0.6203\n",
            "Epoch 00142: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6459 - accuracy: 0.6203 - val_loss: 0.6690 - val_accuracy: 0.6040\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6449 - accuracy: 0.6402\n",
            "Epoch 00143: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6449 - accuracy: 0.6402 - val_loss: 0.6670 - val_accuracy: 0.6040\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6271 - accuracy: 0.6625\n",
            "Epoch 00144: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6271 - accuracy: 0.6625 - val_loss: 0.6661 - val_accuracy: 0.6040\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6426 - accuracy: 0.6253\n",
            "Epoch 00145: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6426 - accuracy: 0.6253 - val_loss: 0.6660 - val_accuracy: 0.5941\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.6352\n",
            "Epoch 00146: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6364 - accuracy: 0.6352 - val_loss: 0.6660 - val_accuracy: 0.6040\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.6402\n",
            "Epoch 00147: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6319 - accuracy: 0.6402 - val_loss: 0.6662 - val_accuracy: 0.6040\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6239 - accuracy: 0.6526\n",
            "Epoch 00148: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6239 - accuracy: 0.6526 - val_loss: 0.6672 - val_accuracy: 0.6040\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6257 - accuracy: 0.6501\n",
            "Epoch 00149: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6257 - accuracy: 0.6501 - val_loss: 0.6688 - val_accuracy: 0.6139\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6298 - accuracy: 0.6476\n",
            "Epoch 00150: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6298 - accuracy: 0.6476 - val_loss: 0.6696 - val_accuracy: 0.6040\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6350 - accuracy: 0.6452\n",
            "Epoch 00151: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6350 - accuracy: 0.6452 - val_loss: 0.6686 - val_accuracy: 0.6139\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6325 - accuracy: 0.6402\n",
            "Epoch 00152: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6325 - accuracy: 0.6402 - val_loss: 0.6669 - val_accuracy: 0.6139\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6261 - accuracy: 0.6352\n",
            "Epoch 00153: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6261 - accuracy: 0.6352 - val_loss: 0.6652 - val_accuracy: 0.6040\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.6700\n",
            "Epoch 00154: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6246 - accuracy: 0.6700 - val_loss: 0.6646 - val_accuracy: 0.6040\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6225 - accuracy: 0.6526\n",
            "Epoch 00155: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6225 - accuracy: 0.6526 - val_loss: 0.6644 - val_accuracy: 0.6040\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6356 - accuracy: 0.6253\n",
            "Epoch 00156: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6356 - accuracy: 0.6253 - val_loss: 0.6643 - val_accuracy: 0.6040\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.6749\n",
            "Epoch 00157: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6188 - accuracy: 0.6749 - val_loss: 0.6649 - val_accuracy: 0.6040\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.6328\n",
            "Epoch 00158: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6217 - accuracy: 0.6328 - val_loss: 0.6652 - val_accuracy: 0.6139\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.6576\n",
            "Epoch 00159: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6317 - accuracy: 0.6576 - val_loss: 0.6645 - val_accuracy: 0.5941\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6326 - accuracy: 0.6303\n",
            "Epoch 00160: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6326 - accuracy: 0.6303 - val_loss: 0.6631 - val_accuracy: 0.6040\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.6526\n",
            "Epoch 00161: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6168 - accuracy: 0.6526 - val_loss: 0.6621 - val_accuracy: 0.6040\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.6576\n",
            "Epoch 00162: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6194 - accuracy: 0.6576 - val_loss: 0.6613 - val_accuracy: 0.6040\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.6551\n",
            "Epoch 00163: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6219 - accuracy: 0.6551 - val_loss: 0.6608 - val_accuracy: 0.6040\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6159 - accuracy: 0.6650\n",
            "Epoch 00164: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6159 - accuracy: 0.6650 - val_loss: 0.6606 - val_accuracy: 0.6040\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6271 - accuracy: 0.6551\n",
            "Epoch 00165: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6271 - accuracy: 0.6551 - val_loss: 0.6602 - val_accuracy: 0.6040\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.6774\n",
            "Epoch 00166: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6217 - accuracy: 0.6774 - val_loss: 0.6599 - val_accuracy: 0.6040\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6119 - accuracy: 0.6576\n",
            "Epoch 00167: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6119 - accuracy: 0.6576 - val_loss: 0.6593 - val_accuracy: 0.6040\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.6725\n",
            "Epoch 00168: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6183 - accuracy: 0.6725 - val_loss: 0.6584 - val_accuracy: 0.6040\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6186 - accuracy: 0.6526\n",
            "Epoch 00169: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6186 - accuracy: 0.6526 - val_loss: 0.6576 - val_accuracy: 0.6040\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.6675\n",
            "Epoch 00170: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6246 - accuracy: 0.6675 - val_loss: 0.6569 - val_accuracy: 0.6139\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6185 - accuracy: 0.6501\n",
            "Epoch 00171: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6185 - accuracy: 0.6501 - val_loss: 0.6565 - val_accuracy: 0.6040\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.6303\n",
            "Epoch 00172: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6179 - accuracy: 0.6303 - val_loss: 0.6564 - val_accuracy: 0.6040\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6140 - accuracy: 0.6625\n",
            "Epoch 00173: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6140 - accuracy: 0.6625 - val_loss: 0.6567 - val_accuracy: 0.6040\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.6501\n",
            "Epoch 00174: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6191 - accuracy: 0.6501 - val_loss: 0.6570 - val_accuracy: 0.6139\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6230 - accuracy: 0.6774\n",
            "Epoch 00175: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6230 - accuracy: 0.6774 - val_loss: 0.6565 - val_accuracy: 0.6139\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6146 - accuracy: 0.6675\n",
            "Epoch 00176: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6146 - accuracy: 0.6675 - val_loss: 0.6554 - val_accuracy: 0.6040\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6196 - accuracy: 0.6476\n",
            "Epoch 00177: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6196 - accuracy: 0.6476 - val_loss: 0.6541 - val_accuracy: 0.6040\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.6625\n",
            "Epoch 00178: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6213 - accuracy: 0.6625 - val_loss: 0.6532 - val_accuracy: 0.6040\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6197 - accuracy: 0.6600\n",
            "Epoch 00179: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6197 - accuracy: 0.6600 - val_loss: 0.6526 - val_accuracy: 0.6040\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.6749\n",
            "Epoch 00180: val_accuracy did not improve from 0.61386\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6139 - accuracy: 0.6749 - val_loss: 0.6529 - val_accuracy: 0.6040\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6080 - accuracy: 0.6625\n",
            "Epoch 00181: val_accuracy improved from 0.61386 to 0.62376, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6080 - accuracy: 0.6625 - val_loss: 0.6539 - val_accuracy: 0.6238\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.6600\n",
            "Epoch 00182: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6125 - accuracy: 0.6600 - val_loss: 0.6541 - val_accuracy: 0.6238\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.6600\n",
            "Epoch 00183: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6148 - accuracy: 0.6600 - val_loss: 0.6526 - val_accuracy: 0.6238\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6137 - accuracy: 0.6501\n",
            "Epoch 00184: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6137 - accuracy: 0.6501 - val_loss: 0.6507 - val_accuracy: 0.6139\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6071 - accuracy: 0.6923\n",
            "Epoch 00185: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6071 - accuracy: 0.6923 - val_loss: 0.6494 - val_accuracy: 0.6040\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6145 - accuracy: 0.6427\n",
            "Epoch 00186: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6145 - accuracy: 0.6427 - val_loss: 0.6486 - val_accuracy: 0.6139\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6085 - accuracy: 0.6799\n",
            "Epoch 00187: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6085 - accuracy: 0.6799 - val_loss: 0.6480 - val_accuracy: 0.6040\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.6452\n",
            "Epoch 00188: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6142 - accuracy: 0.6452 - val_loss: 0.6473 - val_accuracy: 0.6040\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6124 - accuracy: 0.6526\n",
            "Epoch 00189: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6124 - accuracy: 0.6526 - val_loss: 0.6468 - val_accuracy: 0.6238\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6118 - accuracy: 0.6700\n",
            "Epoch 00190: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6118 - accuracy: 0.6700 - val_loss: 0.6461 - val_accuracy: 0.6040\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.6402\n",
            "Epoch 00191: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6074 - accuracy: 0.6402 - val_loss: 0.6455 - val_accuracy: 0.6040\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6053 - accuracy: 0.6799\n",
            "Epoch 00192: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6053 - accuracy: 0.6799 - val_loss: 0.6459 - val_accuracy: 0.6238\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.6849\n",
            "Epoch 00193: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6045 - accuracy: 0.6849 - val_loss: 0.6464 - val_accuracy: 0.6238\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6167 - accuracy: 0.6749\n",
            "Epoch 00194: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6167 - accuracy: 0.6749 - val_loss: 0.6442 - val_accuracy: 0.6238\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.6749\n",
            "Epoch 00195: val_accuracy did not improve from 0.62376\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6045 - accuracy: 0.6749 - val_loss: 0.6418 - val_accuracy: 0.6139\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6088 - accuracy: 0.6849\n",
            "Epoch 00196: val_accuracy improved from 0.62376 to 0.63366, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6088 - accuracy: 0.6849 - val_loss: 0.6410 - val_accuracy: 0.6337\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.6600\n",
            "Epoch 00197: val_accuracy did not improve from 0.63366\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6139 - accuracy: 0.6600 - val_loss: 0.6400 - val_accuracy: 0.6238\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5920 - accuracy: 0.6998\n",
            "Epoch 00198: val_accuracy did not improve from 0.63366\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5920 - accuracy: 0.6998 - val_loss: 0.6417 - val_accuracy: 0.6238\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6147 - accuracy: 0.6675\n",
            "Epoch 00199: val_accuracy did not improve from 0.63366\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6147 - accuracy: 0.6675 - val_loss: 0.6403 - val_accuracy: 0.6238\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.6650\n",
            "Epoch 00200: val_accuracy did not improve from 0.63366\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5941 - accuracy: 0.6650 - val_loss: 0.6382 - val_accuracy: 0.6139\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6011 - accuracy: 0.6675\n",
            "Epoch 00201: val_accuracy improved from 0.63366 to 0.64356, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6011 - accuracy: 0.6675 - val_loss: 0.6366 - val_accuracy: 0.6436\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.6749\n",
            "Epoch 00202: val_accuracy did not improve from 0.64356\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6091 - accuracy: 0.6749 - val_loss: 0.6363 - val_accuracy: 0.6337\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.6725\n",
            "Epoch 00203: val_accuracy did not improve from 0.64356\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5997 - accuracy: 0.6725 - val_loss: 0.6355 - val_accuracy: 0.6436\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5948 - accuracy: 0.6749\n",
            "Epoch 00204: val_accuracy did not improve from 0.64356\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5948 - accuracy: 0.6749 - val_loss: 0.6362 - val_accuracy: 0.6238\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5895 - accuracy: 0.6948\n",
            "Epoch 00205: val_accuracy did not improve from 0.64356\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5895 - accuracy: 0.6948 - val_loss: 0.6369 - val_accuracy: 0.6238\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5884 - accuracy: 0.6948\n",
            "Epoch 00206: val_accuracy did not improve from 0.64356\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5884 - accuracy: 0.6948 - val_loss: 0.6342 - val_accuracy: 0.6337\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5931 - accuracy: 0.6873\n",
            "Epoch 00207: val_accuracy improved from 0.64356 to 0.66337, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.5931 - accuracy: 0.6873 - val_loss: 0.6328 - val_accuracy: 0.6634\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.6774\n",
            "Epoch 00208: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5984 - accuracy: 0.6774 - val_loss: 0.6324 - val_accuracy: 0.6436\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5887 - accuracy: 0.7072\n",
            "Epoch 00209: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5887 - accuracy: 0.7072 - val_loss: 0.6324 - val_accuracy: 0.6436\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.6923\n",
            "Epoch 00210: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5756 - accuracy: 0.6923 - val_loss: 0.6351 - val_accuracy: 0.6040\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.7146\n",
            "Epoch 00211: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5950 - accuracy: 0.7146 - val_loss: 0.6319 - val_accuracy: 0.6139\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5854 - accuracy: 0.6898\n",
            "Epoch 00212: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5854 - accuracy: 0.6898 - val_loss: 0.6301 - val_accuracy: 0.6535\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5886 - accuracy: 0.6948\n",
            "Epoch 00213: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5886 - accuracy: 0.6948 - val_loss: 0.6309 - val_accuracy: 0.6436\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5987 - accuracy: 0.6873\n",
            "Epoch 00214: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5987 - accuracy: 0.6873 - val_loss: 0.6290 - val_accuracy: 0.6436\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.6973\n",
            "Epoch 00215: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5807 - accuracy: 0.6973 - val_loss: 0.6321 - val_accuracy: 0.6139\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.6948\n",
            "Epoch 00216: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5883 - accuracy: 0.6948 - val_loss: 0.6288 - val_accuracy: 0.6139\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5768 - accuracy: 0.7022\n",
            "Epoch 00217: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5768 - accuracy: 0.7022 - val_loss: 0.6268 - val_accuracy: 0.6535\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5860 - accuracy: 0.6898\n",
            "Epoch 00218: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5860 - accuracy: 0.6898 - val_loss: 0.6269 - val_accuracy: 0.6436\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5899 - accuracy: 0.6873\n",
            "Epoch 00219: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5899 - accuracy: 0.6873 - val_loss: 0.6261 - val_accuracy: 0.6238\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.6898\n",
            "Epoch 00220: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5806 - accuracy: 0.6898 - val_loss: 0.6320 - val_accuracy: 0.6139\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5835 - accuracy: 0.7047\n",
            "Epoch 00221: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5835 - accuracy: 0.7047 - val_loss: 0.6221 - val_accuracy: 0.6238\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5703 - accuracy: 0.7246\n",
            "Epoch 00222: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5703 - accuracy: 0.7246 - val_loss: 0.6214 - val_accuracy: 0.6436\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.7196\n",
            "Epoch 00223: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5716 - accuracy: 0.7196 - val_loss: 0.6178 - val_accuracy: 0.6535\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.7072\n",
            "Epoch 00224: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5877 - accuracy: 0.7072 - val_loss: 0.6187 - val_accuracy: 0.6436\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5720 - accuracy: 0.7097\n",
            "Epoch 00225: val_accuracy did not improve from 0.66337\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5720 - accuracy: 0.7097 - val_loss: 0.6179 - val_accuracy: 0.6436\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5714 - accuracy: 0.7146\n",
            "Epoch 00226: val_accuracy improved from 0.66337 to 0.67327, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.5714 - accuracy: 0.7146 - val_loss: 0.6149 - val_accuracy: 0.6733\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.6998\n",
            "Epoch 00227: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5822 - accuracy: 0.6998 - val_loss: 0.6172 - val_accuracy: 0.6535\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.6824\n",
            "Epoch 00228: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5874 - accuracy: 0.6824 - val_loss: 0.6164 - val_accuracy: 0.6535\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.7122\n",
            "Epoch 00229: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5665 - accuracy: 0.7122 - val_loss: 0.6238 - val_accuracy: 0.6139\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.6948\n",
            "Epoch 00230: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6074 - accuracy: 0.6948 - val_loss: 0.6184 - val_accuracy: 0.6436\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.7196\n",
            "Epoch 00231: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5630 - accuracy: 0.7196 - val_loss: 0.6158 - val_accuracy: 0.6535\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7146\n",
            "Epoch 00232: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5664 - accuracy: 0.7146 - val_loss: 0.6152 - val_accuracy: 0.6535\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.6973\n",
            "Epoch 00233: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5610 - accuracy: 0.6973 - val_loss: 0.6138 - val_accuracy: 0.6634\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.7022\n",
            "Epoch 00234: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5739 - accuracy: 0.7022 - val_loss: 0.6099 - val_accuracy: 0.6337\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.7320\n",
            "Epoch 00235: val_accuracy did not improve from 0.67327\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5563 - accuracy: 0.7320 - val_loss: 0.6091 - val_accuracy: 0.6436\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5511 - accuracy: 0.7320\n",
            "Epoch 00236: val_accuracy improved from 0.67327 to 0.68317, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.5511 - accuracy: 0.7320 - val_loss: 0.6097 - val_accuracy: 0.6832\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.7221\n",
            "Epoch 00237: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5554 - accuracy: 0.7221 - val_loss: 0.6092 - val_accuracy: 0.6733\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.7122\n",
            "Epoch 00238: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5640 - accuracy: 0.7122 - val_loss: 0.6098 - val_accuracy: 0.6337\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7072\n",
            "Epoch 00239: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5664 - accuracy: 0.7072 - val_loss: 0.6114 - val_accuracy: 0.6733\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.7196\n",
            "Epoch 00240: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5531 - accuracy: 0.7196 - val_loss: 0.6175 - val_accuracy: 0.6337\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5660 - accuracy: 0.6923\n",
            "Epoch 00241: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5660 - accuracy: 0.6923 - val_loss: 0.6091 - val_accuracy: 0.6535\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5499 - accuracy: 0.7444\n",
            "Epoch 00242: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5499 - accuracy: 0.7444 - val_loss: 0.6101 - val_accuracy: 0.6337\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5512 - accuracy: 0.7419\n",
            "Epoch 00243: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5512 - accuracy: 0.7419 - val_loss: 0.6086 - val_accuracy: 0.6733\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5560 - accuracy: 0.7171\n",
            "Epoch 00244: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5560 - accuracy: 0.7171 - val_loss: 0.6099 - val_accuracy: 0.6634\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5578 - accuracy: 0.7022\n",
            "Epoch 00245: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5578 - accuracy: 0.7022 - val_loss: 0.6048 - val_accuracy: 0.6337\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5556 - accuracy: 0.7196\n",
            "Epoch 00246: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5556 - accuracy: 0.7196 - val_loss: 0.6026 - val_accuracy: 0.6436\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.7320\n",
            "Epoch 00247: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5437 - accuracy: 0.7320 - val_loss: 0.6077 - val_accuracy: 0.6733\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5505 - accuracy: 0.7196\n",
            "Epoch 00248: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5505 - accuracy: 0.7196 - val_loss: 0.6012 - val_accuracy: 0.6832\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.7419\n",
            "Epoch 00249: val_accuracy did not improve from 0.68317\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5474 - accuracy: 0.7419 - val_loss: 0.6010 - val_accuracy: 0.6436\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.7171\n",
            "Epoch 00250: val_accuracy improved from 0.68317 to 0.69307, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.5569 - accuracy: 0.7171 - val_loss: 0.6034 - val_accuracy: 0.6931\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5430 - accuracy: 0.7270\n",
            "Epoch 00251: val_accuracy did not improve from 0.69307\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5430 - accuracy: 0.7270 - val_loss: 0.5972 - val_accuracy: 0.6931\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.7320\n",
            "Epoch 00252: val_accuracy did not improve from 0.69307\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5497 - accuracy: 0.7320 - val_loss: 0.6029 - val_accuracy: 0.6931\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5519 - accuracy: 0.7295\n",
            "Epoch 00253: val_accuracy improved from 0.69307 to 0.70297, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5519 - accuracy: 0.7295 - val_loss: 0.5967 - val_accuracy: 0.7030\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5498 - accuracy: 0.7345\n",
            "Epoch 00254: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5498 - accuracy: 0.7345 - val_loss: 0.5975 - val_accuracy: 0.6931\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5546 - accuracy: 0.7345\n",
            "Epoch 00255: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5546 - accuracy: 0.7345 - val_loss: 0.5970 - val_accuracy: 0.6733\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.7122\n",
            "Epoch 00256: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5656 - accuracy: 0.7122 - val_loss: 0.5905 - val_accuracy: 0.6535\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5370 - accuracy: 0.7543\n",
            "Epoch 00257: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5370 - accuracy: 0.7543 - val_loss: 0.6083 - val_accuracy: 0.7030\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5400 - accuracy: 0.7345\n",
            "Epoch 00258: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5400 - accuracy: 0.7345 - val_loss: 0.5911 - val_accuracy: 0.6634\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.7370\n",
            "Epoch 00259: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5372 - accuracy: 0.7370 - val_loss: 0.5972 - val_accuracy: 0.6535\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.7295\n",
            "Epoch 00260: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5524 - accuracy: 0.7295 - val_loss: 0.5915 - val_accuracy: 0.6634\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5487 - accuracy: 0.7122\n",
            "Epoch 00261: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5487 - accuracy: 0.7122 - val_loss: 0.6005 - val_accuracy: 0.6931\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.7022\n",
            "Epoch 00262: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5496 - accuracy: 0.7022 - val_loss: 0.5895 - val_accuracy: 0.6733\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.7270\n",
            "Epoch 00263: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5550 - accuracy: 0.7270 - val_loss: 0.5856 - val_accuracy: 0.6733\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.7593\n",
            "Epoch 00264: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5255 - accuracy: 0.7593 - val_loss: 0.5895 - val_accuracy: 0.7030\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.7419\n",
            "Epoch 00265: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5356 - accuracy: 0.7419 - val_loss: 0.5867 - val_accuracy: 0.6733\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.7643\n",
            "Epoch 00266: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5394 - accuracy: 0.7643 - val_loss: 0.5810 - val_accuracy: 0.6535\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5400 - accuracy: 0.7543\n",
            "Epoch 00267: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5400 - accuracy: 0.7543 - val_loss: 0.5901 - val_accuracy: 0.7030\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.7543\n",
            "Epoch 00268: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5316 - accuracy: 0.7543 - val_loss: 0.5835 - val_accuracy: 0.6634\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.7444\n",
            "Epoch 00269: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5293 - accuracy: 0.7444 - val_loss: 0.5839 - val_accuracy: 0.6733\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.7469\n",
            "Epoch 00270: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5228 - accuracy: 0.7469 - val_loss: 0.5982 - val_accuracy: 0.6832\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.7146\n",
            "Epoch 00271: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5416 - accuracy: 0.7146 - val_loss: 0.5805 - val_accuracy: 0.6634\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.7519\n",
            "Epoch 00272: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5351 - accuracy: 0.7519 - val_loss: 0.5888 - val_accuracy: 0.6832\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.7643\n",
            "Epoch 00273: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5261 - accuracy: 0.7643 - val_loss: 0.6085 - val_accuracy: 0.6535\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5557 - accuracy: 0.7022\n",
            "Epoch 00274: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5557 - accuracy: 0.7022 - val_loss: 0.5784 - val_accuracy: 0.6733\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.7593\n",
            "Epoch 00275: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5238 - accuracy: 0.7593 - val_loss: 0.5934 - val_accuracy: 0.7030\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.7320\n",
            "Epoch 00276: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5411 - accuracy: 0.7320 - val_loss: 0.5809 - val_accuracy: 0.6931\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.7295\n",
            "Epoch 00277: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5306 - accuracy: 0.7295 - val_loss: 0.5934 - val_accuracy: 0.6733\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.7171\n",
            "Epoch 00278: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5299 - accuracy: 0.7171 - val_loss: 0.5742 - val_accuracy: 0.6634\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.7295\n",
            "Epoch 00279: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5372 - accuracy: 0.7295 - val_loss: 0.5727 - val_accuracy: 0.6634\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.7419\n",
            "Epoch 00280: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5347 - accuracy: 0.7419 - val_loss: 0.5829 - val_accuracy: 0.7030\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.7072\n",
            "Epoch 00281: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5531 - accuracy: 0.7072 - val_loss: 0.5672 - val_accuracy: 0.6733\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.7494\n",
            "Epoch 00282: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5125 - accuracy: 0.7494 - val_loss: 0.5695 - val_accuracy: 0.6733\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.7742\n",
            "Epoch 00283: val_accuracy did not improve from 0.70297\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5076 - accuracy: 0.7742 - val_loss: 0.5924 - val_accuracy: 0.6832\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5241 - accuracy: 0.7494\n",
            "Epoch 00284: val_accuracy improved from 0.70297 to 0.71287, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5241 - accuracy: 0.7494 - val_loss: 0.5652 - val_accuracy: 0.7129\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.7469\n",
            "Epoch 00285: val_accuracy did not improve from 0.71287\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5051 - accuracy: 0.7469 - val_loss: 0.5716 - val_accuracy: 0.7030\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.7519\n",
            "Epoch 00286: val_accuracy did not improve from 0.71287\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.5372 - accuracy: 0.7519 - val_loss: 0.5695 - val_accuracy: 0.7129\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.7643\n",
            "Epoch 00287: val_accuracy did not improve from 0.71287\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5073 - accuracy: 0.7643 - val_loss: 0.5687 - val_accuracy: 0.7129\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.7742\n",
            "Epoch 00288: val_accuracy did not improve from 0.71287\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5005 - accuracy: 0.7742 - val_loss: 0.5573 - val_accuracy: 0.6733\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.7320\n",
            "Epoch 00289: val_accuracy did not improve from 0.71287\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5282 - accuracy: 0.7320 - val_loss: 0.5548 - val_accuracy: 0.6832\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.7643\n",
            "Epoch 00290: val_accuracy improved from 0.71287 to 0.74257, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5086 - accuracy: 0.7643 - val_loss: 0.5566 - val_accuracy: 0.7426\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.7618\n",
            "Epoch 00291: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5026 - accuracy: 0.7618 - val_loss: 0.5561 - val_accuracy: 0.6733\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.7667\n",
            "Epoch 00292: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5043 - accuracy: 0.7667 - val_loss: 0.5573 - val_accuracy: 0.6832\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5171 - accuracy: 0.7494\n",
            "Epoch 00293: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5171 - accuracy: 0.7494 - val_loss: 0.5654 - val_accuracy: 0.6931\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.7618\n",
            "Epoch 00294: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5040 - accuracy: 0.7618 - val_loss: 0.5587 - val_accuracy: 0.7030\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.7568\n",
            "Epoch 00295: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5058 - accuracy: 0.7568 - val_loss: 0.5575 - val_accuracy: 0.6733\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.7494\n",
            "Epoch 00296: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5146 - accuracy: 0.7494 - val_loss: 0.5679 - val_accuracy: 0.7030\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.7469\n",
            "Epoch 00297: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5187 - accuracy: 0.7469 - val_loss: 0.5538 - val_accuracy: 0.6832\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5017 - accuracy: 0.7643\n",
            "Epoch 00298: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5017 - accuracy: 0.7643 - val_loss: 0.5533 - val_accuracy: 0.6931\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4961 - accuracy: 0.7717\n",
            "Epoch 00299: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4961 - accuracy: 0.7717 - val_loss: 0.5555 - val_accuracy: 0.7228\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.7345\n",
            "Epoch 00300: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5295 - accuracy: 0.7345 - val_loss: 0.5576 - val_accuracy: 0.6931\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.7370\n",
            "Epoch 00301: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5228 - accuracy: 0.7370 - val_loss: 0.5550 - val_accuracy: 0.7228\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.7494\n",
            "Epoch 00302: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5177 - accuracy: 0.7494 - val_loss: 0.5772 - val_accuracy: 0.7030\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.7593\n",
            "Epoch 00303: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5133 - accuracy: 0.7593 - val_loss: 0.5595 - val_accuracy: 0.7228\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.7568\n",
            "Epoch 00304: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5217 - accuracy: 0.7568 - val_loss: 0.5470 - val_accuracy: 0.7030\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.7643\n",
            "Epoch 00305: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5026 - accuracy: 0.7643 - val_loss: 0.5652 - val_accuracy: 0.7129\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.7469\n",
            "Epoch 00306: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5051 - accuracy: 0.7469 - val_loss: 0.5504 - val_accuracy: 0.6931\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.7643\n",
            "Epoch 00307: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5094 - accuracy: 0.7643 - val_loss: 0.5487 - val_accuracy: 0.7030\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4884 - accuracy: 0.7841\n",
            "Epoch 00308: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4884 - accuracy: 0.7841 - val_loss: 0.5783 - val_accuracy: 0.7030\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.7593\n",
            "Epoch 00309: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5146 - accuracy: 0.7593 - val_loss: 0.5624 - val_accuracy: 0.7129\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5336 - accuracy: 0.7370\n",
            "Epoch 00310: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5336 - accuracy: 0.7370 - val_loss: 0.5522 - val_accuracy: 0.7030\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.7593\n",
            "Epoch 00311: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5081 - accuracy: 0.7593 - val_loss: 0.5786 - val_accuracy: 0.7129\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5303 - accuracy: 0.7097\n",
            "Epoch 00312: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5303 - accuracy: 0.7097 - val_loss: 0.5415 - val_accuracy: 0.7030\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4881 - accuracy: 0.7618\n",
            "Epoch 00313: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4881 - accuracy: 0.7618 - val_loss: 0.5373 - val_accuracy: 0.7129\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.7667\n",
            "Epoch 00314: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4902 - accuracy: 0.7667 - val_loss: 0.5477 - val_accuracy: 0.7129\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 0.7345\n",
            "Epoch 00315: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5159 - accuracy: 0.7345 - val_loss: 0.5376 - val_accuracy: 0.7228\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.7816\n",
            "Epoch 00316: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4792 - accuracy: 0.7816 - val_loss: 0.5398 - val_accuracy: 0.7129\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.7643\n",
            "Epoch 00317: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5047 - accuracy: 0.7643 - val_loss: 0.5517 - val_accuracy: 0.6733\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.7667\n",
            "Epoch 00318: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4815 - accuracy: 0.7667 - val_loss: 0.5415 - val_accuracy: 0.7129\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.7543\n",
            "Epoch 00319: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5049 - accuracy: 0.7543 - val_loss: 0.5412 - val_accuracy: 0.7228\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.7568\n",
            "Epoch 00320: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4964 - accuracy: 0.7568 - val_loss: 0.5357 - val_accuracy: 0.7327\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4911 - accuracy: 0.7643\n",
            "Epoch 00321: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4911 - accuracy: 0.7643 - val_loss: 0.5448 - val_accuracy: 0.6931\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4978 - accuracy: 0.7469\n",
            "Epoch 00322: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4978 - accuracy: 0.7469 - val_loss: 0.5359 - val_accuracy: 0.7327\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4941 - accuracy: 0.7568\n",
            "Epoch 00323: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4941 - accuracy: 0.7568 - val_loss: 0.5365 - val_accuracy: 0.7129\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.7494\n",
            "Epoch 00324: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4920 - accuracy: 0.7494 - val_loss: 0.5570 - val_accuracy: 0.7030\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.7593\n",
            "Epoch 00325: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4956 - accuracy: 0.7593 - val_loss: 0.5371 - val_accuracy: 0.7228\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.7444\n",
            "Epoch 00326: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4990 - accuracy: 0.7444 - val_loss: 0.5313 - val_accuracy: 0.7030\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.7667\n",
            "Epoch 00327: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4758 - accuracy: 0.7667 - val_loss: 0.5684 - val_accuracy: 0.7228\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.7444\n",
            "Epoch 00328: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5037 - accuracy: 0.7444 - val_loss: 0.5446 - val_accuracy: 0.7426\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.7494\n",
            "Epoch 00329: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5166 - accuracy: 0.7494 - val_loss: 0.5277 - val_accuracy: 0.6931\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4754 - accuracy: 0.7940\n",
            "Epoch 00330: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4754 - accuracy: 0.7940 - val_loss: 0.5905 - val_accuracy: 0.6931\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.7395\n",
            "Epoch 00331: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5135 - accuracy: 0.7395 - val_loss: 0.5373 - val_accuracy: 0.7327\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4983 - accuracy: 0.7792\n",
            "Epoch 00332: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4983 - accuracy: 0.7792 - val_loss: 0.5298 - val_accuracy: 0.7327\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.7767\n",
            "Epoch 00333: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4947 - accuracy: 0.7767 - val_loss: 0.5762 - val_accuracy: 0.7327\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.7370\n",
            "Epoch 00334: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5140 - accuracy: 0.7370 - val_loss: 0.5223 - val_accuracy: 0.7030\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4824 - accuracy: 0.7692\n",
            "Epoch 00335: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4824 - accuracy: 0.7692 - val_loss: 0.5285 - val_accuracy: 0.7327\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.7916\n",
            "Epoch 00336: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4846 - accuracy: 0.7916 - val_loss: 0.5709 - val_accuracy: 0.7327\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.7643\n",
            "Epoch 00337: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4833 - accuracy: 0.7643 - val_loss: 0.5278 - val_accuracy: 0.6931\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.7940\n",
            "Epoch 00338: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4752 - accuracy: 0.7940 - val_loss: 0.5279 - val_accuracy: 0.7327\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4851 - accuracy: 0.7916\n",
            "Epoch 00339: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4851 - accuracy: 0.7916 - val_loss: 0.5466 - val_accuracy: 0.7129\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.7593\n",
            "Epoch 00340: val_accuracy did not improve from 0.74257\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4695 - accuracy: 0.7593 - val_loss: 0.5212 - val_accuracy: 0.7030\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.7692\n",
            "Epoch 00341: val_accuracy improved from 0.74257 to 0.75248, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4860 - accuracy: 0.7692 - val_loss: 0.5199 - val_accuracy: 0.7525\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4716 - accuracy: 0.7717\n",
            "Epoch 00342: val_accuracy did not improve from 0.75248\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4716 - accuracy: 0.7717 - val_loss: 0.5252 - val_accuracy: 0.7327\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.7295\n",
            "Epoch 00343: val_accuracy did not improve from 0.75248\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5035 - accuracy: 0.7295 - val_loss: 0.5160 - val_accuracy: 0.7327\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.8040\n",
            "Epoch 00344: val_accuracy did not improve from 0.75248\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4584 - accuracy: 0.8040 - val_loss: 0.5185 - val_accuracy: 0.7228\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4940 - accuracy: 0.7618\n",
            "Epoch 00345: val_accuracy did not improve from 0.75248\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4940 - accuracy: 0.7618 - val_loss: 0.5323 - val_accuracy: 0.7327\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4716 - accuracy: 0.7841\n",
            "Epoch 00346: val_accuracy improved from 0.75248 to 0.76238, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4716 - accuracy: 0.7841 - val_loss: 0.5138 - val_accuracy: 0.7624\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4904 - accuracy: 0.7519\n",
            "Epoch 00347: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4904 - accuracy: 0.7519 - val_loss: 0.5120 - val_accuracy: 0.7624\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4760 - accuracy: 0.7816\n",
            "Epoch 00348: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4760 - accuracy: 0.7816 - val_loss: 0.5743 - val_accuracy: 0.6931\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.7320\n",
            "Epoch 00349: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5230 - accuracy: 0.7320 - val_loss: 0.5421 - val_accuracy: 0.7525\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.7543\n",
            "Epoch 00350: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5050 - accuracy: 0.7543 - val_loss: 0.5150 - val_accuracy: 0.7426\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.8065\n",
            "Epoch 00351: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4537 - accuracy: 0.8065 - val_loss: 0.6383 - val_accuracy: 0.6139\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.7196\n",
            "Epoch 00352: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5391 - accuracy: 0.7196 - val_loss: 0.5343 - val_accuracy: 0.7426\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.7519\n",
            "Epoch 00353: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5060 - accuracy: 0.7519 - val_loss: 0.5476 - val_accuracy: 0.7327\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.7792\n",
            "Epoch 00354: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4977 - accuracy: 0.7792 - val_loss: 0.5582 - val_accuracy: 0.7426\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.7593\n",
            "Epoch 00355: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5062 - accuracy: 0.7593 - val_loss: 0.5307 - val_accuracy: 0.7327\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4732 - accuracy: 0.7643\n",
            "Epoch 00356: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4732 - accuracy: 0.7643 - val_loss: 0.5260 - val_accuracy: 0.7624\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4768 - accuracy: 0.7866\n",
            "Epoch 00357: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4768 - accuracy: 0.7866 - val_loss: 0.5007 - val_accuracy: 0.7426\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.7940\n",
            "Epoch 00358: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4454 - accuracy: 0.7940 - val_loss: 0.5370 - val_accuracy: 0.7426\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4879 - accuracy: 0.7593\n",
            "Epoch 00359: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4879 - accuracy: 0.7593 - val_loss: 0.5304 - val_accuracy: 0.7426\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.7618\n",
            "Epoch 00360: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5081 - accuracy: 0.7618 - val_loss: 0.5137 - val_accuracy: 0.7426\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4908 - accuracy: 0.7692\n",
            "Epoch 00361: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4908 - accuracy: 0.7692 - val_loss: 0.5742 - val_accuracy: 0.6832\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4960 - accuracy: 0.7395\n",
            "Epoch 00362: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4960 - accuracy: 0.7395 - val_loss: 0.5024 - val_accuracy: 0.7228\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.7742\n",
            "Epoch 00363: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4626 - accuracy: 0.7742 - val_loss: 0.5230 - val_accuracy: 0.7624\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.7568\n",
            "Epoch 00364: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5031 - accuracy: 0.7568 - val_loss: 0.5127 - val_accuracy: 0.7228\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.7792\n",
            "Epoch 00365: val_accuracy did not improve from 0.76238\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4610 - accuracy: 0.7792 - val_loss: 0.5166 - val_accuracy: 0.7327\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.7866\n",
            "Epoch 00366: val_accuracy improved from 0.76238 to 0.77228, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4621 - accuracy: 0.7866 - val_loss: 0.5109 - val_accuracy: 0.7723\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4788 - accuracy: 0.7792\n",
            "Epoch 00367: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4788 - accuracy: 0.7792 - val_loss: 0.5000 - val_accuracy: 0.7030\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4457 - accuracy: 0.7965\n",
            "Epoch 00368: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4457 - accuracy: 0.7965 - val_loss: 0.5543 - val_accuracy: 0.7426\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5074 - accuracy: 0.7519\n",
            "Epoch 00369: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5074 - accuracy: 0.7519 - val_loss: 0.5250 - val_accuracy: 0.7624\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.7667\n",
            "Epoch 00370: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4969 - accuracy: 0.7667 - val_loss: 0.4951 - val_accuracy: 0.7723\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4495 - accuracy: 0.7891\n",
            "Epoch 00371: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4495 - accuracy: 0.7891 - val_loss: 0.5767 - val_accuracy: 0.7030\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.7469\n",
            "Epoch 00372: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5042 - accuracy: 0.7469 - val_loss: 0.4939 - val_accuracy: 0.7723\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.8040\n",
            "Epoch 00373: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4481 - accuracy: 0.8040 - val_loss: 0.5065 - val_accuracy: 0.7624\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.7916\n",
            "Epoch 00374: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4628 - accuracy: 0.7916 - val_loss: 0.5352 - val_accuracy: 0.7228\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4561 - accuracy: 0.7792\n",
            "Epoch 00375: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4561 - accuracy: 0.7792 - val_loss: 0.5126 - val_accuracy: 0.7426\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4429 - accuracy: 0.8040\n",
            "Epoch 00376: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4429 - accuracy: 0.8040 - val_loss: 0.5001 - val_accuracy: 0.7723\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.7891\n",
            "Epoch 00377: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4527 - accuracy: 0.7891 - val_loss: 0.4930 - val_accuracy: 0.7228\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8065\n",
            "Epoch 00378: val_accuracy did not improve from 0.77228\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4287 - accuracy: 0.8065 - val_loss: 0.5217 - val_accuracy: 0.7129\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.7692\n",
            "Epoch 00379: val_accuracy improved from 0.77228 to 0.78218, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4686 - accuracy: 0.7692 - val_loss: 0.5051 - val_accuracy: 0.7822\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.7643\n",
            "Epoch 00380: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4762 - accuracy: 0.7643 - val_loss: 0.4956 - val_accuracy: 0.7327\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.7891\n",
            "Epoch 00381: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4481 - accuracy: 0.7891 - val_loss: 0.5356 - val_accuracy: 0.7327\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.7643\n",
            "Epoch 00382: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4762 - accuracy: 0.7643 - val_loss: 0.4864 - val_accuracy: 0.7624\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.8015\n",
            "Epoch 00383: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4489 - accuracy: 0.8015 - val_loss: 0.4858 - val_accuracy: 0.7822\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.7816\n",
            "Epoch 00384: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4635 - accuracy: 0.7816 - val_loss: 0.5153 - val_accuracy: 0.7228\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4703 - accuracy: 0.7568\n",
            "Epoch 00385: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4703 - accuracy: 0.7568 - val_loss: 0.4823 - val_accuracy: 0.7624\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.8089\n",
            "Epoch 00386: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4514 - accuracy: 0.8089 - val_loss: 0.4883 - val_accuracy: 0.7525\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.8089\n",
            "Epoch 00387: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4380 - accuracy: 0.8089 - val_loss: 0.5225 - val_accuracy: 0.7327\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4591 - accuracy: 0.7742\n",
            "Epoch 00388: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4591 - accuracy: 0.7742 - val_loss: 0.4833 - val_accuracy: 0.7426\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.7940\n",
            "Epoch 00389: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4488 - accuracy: 0.7940 - val_loss: 0.4914 - val_accuracy: 0.7822\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.8015\n",
            "Epoch 00390: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4559 - accuracy: 0.8015 - val_loss: 0.5082 - val_accuracy: 0.7228\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.7816\n",
            "Epoch 00391: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4610 - accuracy: 0.7816 - val_loss: 0.4797 - val_accuracy: 0.7525\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4396 - accuracy: 0.7965\n",
            "Epoch 00392: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4396 - accuracy: 0.7965 - val_loss: 0.4873 - val_accuracy: 0.7624\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.8189\n",
            "Epoch 00393: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4380 - accuracy: 0.8189 - val_loss: 0.5028 - val_accuracy: 0.7426\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.8015\n",
            "Epoch 00394: val_accuracy did not improve from 0.78218\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4430 - accuracy: 0.8015 - val_loss: 0.4935 - val_accuracy: 0.7228\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.7891\n",
            "Epoch 00395: val_accuracy improved from 0.78218 to 0.79208, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4447 - accuracy: 0.7891 - val_loss: 0.4951 - val_accuracy: 0.7921\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.7717\n",
            "Epoch 00396: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4790 - accuracy: 0.7717 - val_loss: 0.4799 - val_accuracy: 0.7624\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4342 - accuracy: 0.7841\n",
            "Epoch 00397: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4342 - accuracy: 0.7841 - val_loss: 0.4949 - val_accuracy: 0.7228\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.8015\n",
            "Epoch 00398: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4293 - accuracy: 0.8015 - val_loss: 0.4781 - val_accuracy: 0.7822\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.7990\n",
            "Epoch 00399: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4448 - accuracy: 0.7990 - val_loss: 0.4790 - val_accuracy: 0.7624\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.8040\n",
            "Epoch 00400: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4352 - accuracy: 0.8040 - val_loss: 0.4997 - val_accuracy: 0.7228\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4576 - accuracy: 0.7717\n",
            "Epoch 00401: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4576 - accuracy: 0.7717 - val_loss: 0.4793 - val_accuracy: 0.7921\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.7990\n",
            "Epoch 00402: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4440 - accuracy: 0.7990 - val_loss: 0.4790 - val_accuracy: 0.7624\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4286 - accuracy: 0.7940\n",
            "Epoch 00403: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4286 - accuracy: 0.7940 - val_loss: 0.4953 - val_accuracy: 0.7129\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.7990\n",
            "Epoch 00404: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4273 - accuracy: 0.7990 - val_loss: 0.4764 - val_accuracy: 0.7921\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4233 - accuracy: 0.8189\n",
            "Epoch 00405: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4233 - accuracy: 0.8189 - val_loss: 0.4748 - val_accuracy: 0.7624\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4127 - accuracy: 0.8238\n",
            "Epoch 00406: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4127 - accuracy: 0.8238 - val_loss: 0.4948 - val_accuracy: 0.7228\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4260 - accuracy: 0.8139\n",
            "Epoch 00407: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4260 - accuracy: 0.8139 - val_loss: 0.4800 - val_accuracy: 0.7822\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4484 - accuracy: 0.7916\n",
            "Epoch 00408: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4484 - accuracy: 0.7916 - val_loss: 0.4740 - val_accuracy: 0.7723\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.8089\n",
            "Epoch 00409: val_accuracy did not improve from 0.79208\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4171 - accuracy: 0.8089 - val_loss: 0.4862 - val_accuracy: 0.7525\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8114\n",
            "Epoch 00410: val_accuracy improved from 0.79208 to 0.80198, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4149 - accuracy: 0.8114 - val_loss: 0.4714 - val_accuracy: 0.8020\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4219 - accuracy: 0.8337\n",
            "Epoch 00411: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4219 - accuracy: 0.8337 - val_loss: 0.4687 - val_accuracy: 0.7822\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8164\n",
            "Epoch 00412: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4150 - accuracy: 0.8164 - val_loss: 0.4929 - val_accuracy: 0.7327\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4342 - accuracy: 0.7940\n",
            "Epoch 00413: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4342 - accuracy: 0.7940 - val_loss: 0.4791 - val_accuracy: 0.7822\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4547 - accuracy: 0.7891\n",
            "Epoch 00414: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4547 - accuracy: 0.7891 - val_loss: 0.4629 - val_accuracy: 0.7723\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4338 - accuracy: 0.8139\n",
            "Epoch 00415: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4338 - accuracy: 0.8139 - val_loss: 0.4862 - val_accuracy: 0.7228\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.7792\n",
            "Epoch 00416: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4409 - accuracy: 0.7792 - val_loss: 0.4622 - val_accuracy: 0.7921\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.8089\n",
            "Epoch 00417: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4345 - accuracy: 0.8089 - val_loss: 0.4569 - val_accuracy: 0.7921\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4403 - accuracy: 0.8189\n",
            "Epoch 00418: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4403 - accuracy: 0.8189 - val_loss: 0.5116 - val_accuracy: 0.7426\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.7717\n",
            "Epoch 00419: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4605 - accuracy: 0.7717 - val_loss: 0.4930 - val_accuracy: 0.7921\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4655 - accuracy: 0.7792\n",
            "Epoch 00420: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4655 - accuracy: 0.7792 - val_loss: 0.4704 - val_accuracy: 0.7921\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.8164\n",
            "Epoch 00421: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4291 - accuracy: 0.8164 - val_loss: 0.5868 - val_accuracy: 0.7030\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.7667\n",
            "Epoch 00422: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5093 - accuracy: 0.7667 - val_loss: 0.4822 - val_accuracy: 0.7921\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.7742\n",
            "Epoch 00423: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4628 - accuracy: 0.7742 - val_loss: 0.4637 - val_accuracy: 0.8020\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4389 - accuracy: 0.8040\n",
            "Epoch 00424: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4389 - accuracy: 0.8040 - val_loss: 0.5901 - val_accuracy: 0.6931\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.7494\n",
            "Epoch 00425: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5198 - accuracy: 0.7494 - val_loss: 0.4887 - val_accuracy: 0.7822\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4593 - accuracy: 0.7891\n",
            "Epoch 00426: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4593 - accuracy: 0.7891 - val_loss: 0.4735 - val_accuracy: 0.8020\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4341 - accuracy: 0.8065\n",
            "Epoch 00427: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4341 - accuracy: 0.8065 - val_loss: 0.6066 - val_accuracy: 0.6634\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.7519\n",
            "Epoch 00428: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5006 - accuracy: 0.7519 - val_loss: 0.4609 - val_accuracy: 0.8020\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4219 - accuracy: 0.8139\n",
            "Epoch 00429: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4219 - accuracy: 0.8139 - val_loss: 0.5050 - val_accuracy: 0.7723\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.7742\n",
            "Epoch 00430: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4847 - accuracy: 0.7742 - val_loss: 0.4995 - val_accuracy: 0.7228\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.7816\n",
            "Epoch 00431: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4420 - accuracy: 0.7816 - val_loss: 0.4767 - val_accuracy: 0.7426\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4067 - accuracy: 0.8189\n",
            "Epoch 00432: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4067 - accuracy: 0.8189 - val_loss: 0.4761 - val_accuracy: 0.7921\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4348 - accuracy: 0.8065\n",
            "Epoch 00433: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4348 - accuracy: 0.8065 - val_loss: 0.4601 - val_accuracy: 0.7723\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4022 - accuracy: 0.8337\n",
            "Epoch 00434: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4022 - accuracy: 0.8337 - val_loss: 0.4910 - val_accuracy: 0.7327\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.7990\n",
            "Epoch 00435: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4124 - accuracy: 0.7990 - val_loss: 0.4610 - val_accuracy: 0.8020\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.8462\n",
            "Epoch 00436: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4190 - accuracy: 0.8462 - val_loss: 0.4566 - val_accuracy: 0.8020\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.8362\n",
            "Epoch 00437: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4036 - accuracy: 0.8362 - val_loss: 0.4871 - val_accuracy: 0.7228\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4313 - accuracy: 0.7891\n",
            "Epoch 00438: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4313 - accuracy: 0.7891 - val_loss: 0.4576 - val_accuracy: 0.8020\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8288\n",
            "Epoch 00439: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4150 - accuracy: 0.8288 - val_loss: 0.4567 - val_accuracy: 0.7723\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4073 - accuracy: 0.8164\n",
            "Epoch 00440: val_accuracy did not improve from 0.80198\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4073 - accuracy: 0.8164 - val_loss: 0.4945 - val_accuracy: 0.7327\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4086 - accuracy: 0.8065\n",
            "Epoch 00441: val_accuracy improved from 0.80198 to 0.81188, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4086 - accuracy: 0.8065 - val_loss: 0.4561 - val_accuracy: 0.8119\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4238 - accuracy: 0.8139\n",
            "Epoch 00442: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4238 - accuracy: 0.8139 - val_loss: 0.4511 - val_accuracy: 0.8020\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.8288\n",
            "Epoch 00443: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4080 - accuracy: 0.8288 - val_loss: 0.5236 - val_accuracy: 0.7129\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.7519\n",
            "Epoch 00444: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4785 - accuracy: 0.7519 - val_loss: 0.4802 - val_accuracy: 0.7921\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.7940\n",
            "Epoch 00445: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4519 - accuracy: 0.7940 - val_loss: 0.4552 - val_accuracy: 0.7921\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4041 - accuracy: 0.8213\n",
            "Epoch 00446: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4041 - accuracy: 0.8213 - val_loss: 0.5919 - val_accuracy: 0.6931\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.7618\n",
            "Epoch 00447: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4990 - accuracy: 0.7618 - val_loss: 0.4575 - val_accuracy: 0.8020\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4093 - accuracy: 0.8238\n",
            "Epoch 00448: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4093 - accuracy: 0.8238 - val_loss: 0.4591 - val_accuracy: 0.8020\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4226 - accuracy: 0.8164\n",
            "Epoch 00449: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4226 - accuracy: 0.8164 - val_loss: 0.5185 - val_accuracy: 0.7426\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.8015\n",
            "Epoch 00450: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4347 - accuracy: 0.8015 - val_loss: 0.4550 - val_accuracy: 0.7723\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.8164\n",
            "Epoch 00451: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4108 - accuracy: 0.8164 - val_loss: 0.4615 - val_accuracy: 0.8020\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.8139\n",
            "Epoch 00452: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4318 - accuracy: 0.8139 - val_loss: 0.4599 - val_accuracy: 0.7723\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4042 - accuracy: 0.8387\n",
            "Epoch 00453: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4042 - accuracy: 0.8387 - val_loss: 0.5099 - val_accuracy: 0.7228\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4640 - accuracy: 0.7692\n",
            "Epoch 00454: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4640 - accuracy: 0.7692 - val_loss: 0.4581 - val_accuracy: 0.8020\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4141 - accuracy: 0.8189\n",
            "Epoch 00455: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4141 - accuracy: 0.8189 - val_loss: 0.4438 - val_accuracy: 0.8119\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8189\n",
            "Epoch 00456: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4199 - accuracy: 0.8189 - val_loss: 0.4963 - val_accuracy: 0.7228\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.8015\n",
            "Epoch 00457: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4235 - accuracy: 0.8015 - val_loss: 0.4474 - val_accuracy: 0.8020\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3964 - accuracy: 0.8412\n",
            "Epoch 00458: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3964 - accuracy: 0.8412 - val_loss: 0.4532 - val_accuracy: 0.7822\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4071 - accuracy: 0.8313\n",
            "Epoch 00459: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4071 - accuracy: 0.8313 - val_loss: 0.4889 - val_accuracy: 0.7228\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.8040\n",
            "Epoch 00460: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4294 - accuracy: 0.8040 - val_loss: 0.4492 - val_accuracy: 0.7822\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4101 - accuracy: 0.8189\n",
            "Epoch 00461: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4101 - accuracy: 0.8189 - val_loss: 0.4478 - val_accuracy: 0.8020\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.8114\n",
            "Epoch 00462: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4273 - accuracy: 0.8114 - val_loss: 0.4918 - val_accuracy: 0.7228\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4378 - accuracy: 0.7841\n",
            "Epoch 00463: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4378 - accuracy: 0.7841 - val_loss: 0.4448 - val_accuracy: 0.8020\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 00464: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4002 - accuracy: 0.8238 - val_loss: 0.4451 - val_accuracy: 0.8020\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.8412\n",
            "Epoch 00465: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3939 - accuracy: 0.8412 - val_loss: 0.4940 - val_accuracy: 0.7228\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4160 - accuracy: 0.8015\n",
            "Epoch 00466: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4160 - accuracy: 0.8015 - val_loss: 0.4531 - val_accuracy: 0.7822\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8189\n",
            "Epoch 00467: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3949 - accuracy: 0.8189 - val_loss: 0.4541 - val_accuracy: 0.8020\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4233 - accuracy: 0.7990\n",
            "Epoch 00468: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4233 - accuracy: 0.7990 - val_loss: 0.4624 - val_accuracy: 0.7723\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.7965\n",
            "Epoch 00469: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4489 - accuracy: 0.7965 - val_loss: 0.4600 - val_accuracy: 0.7921\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3966 - accuracy: 0.8213\n",
            "Epoch 00470: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3966 - accuracy: 0.8213 - val_loss: 0.4493 - val_accuracy: 0.8119\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4041 - accuracy: 0.8238\n",
            "Epoch 00471: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4041 - accuracy: 0.8238 - val_loss: 0.4692 - val_accuracy: 0.7624\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8337\n",
            "Epoch 00472: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4029 - accuracy: 0.8337 - val_loss: 0.4949 - val_accuracy: 0.7228\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4114 - accuracy: 0.8089\n",
            "Epoch 00473: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4114 - accuracy: 0.8089 - val_loss: 0.4518 - val_accuracy: 0.8020\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.8164\n",
            "Epoch 00474: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4220 - accuracy: 0.8164 - val_loss: 0.4407 - val_accuracy: 0.8020\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4017 - accuracy: 0.8387\n",
            "Epoch 00475: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4017 - accuracy: 0.8387 - val_loss: 0.4736 - val_accuracy: 0.7327\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.8089\n",
            "Epoch 00476: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4119 - accuracy: 0.8089 - val_loss: 0.4454 - val_accuracy: 0.8020\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4099 - accuracy: 0.8263\n",
            "Epoch 00477: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4099 - accuracy: 0.8263 - val_loss: 0.4545 - val_accuracy: 0.7525\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.8213\n",
            "Epoch 00478: val_accuracy did not improve from 0.81188\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4187 - accuracy: 0.8213 - val_loss: 0.4647 - val_accuracy: 0.7525\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8040\n",
            "Epoch 00479: val_accuracy improved from 0.81188 to 0.82178, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4102 - accuracy: 0.8040 - val_loss: 0.4390 - val_accuracy: 0.8218\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3814 - accuracy: 0.8511\n",
            "Epoch 00480: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3814 - accuracy: 0.8511 - val_loss: 0.4379 - val_accuracy: 0.8218\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8337\n",
            "Epoch 00481: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4129 - accuracy: 0.8337 - val_loss: 0.4559 - val_accuracy: 0.7525\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4047 - accuracy: 0.8089\n",
            "Epoch 00482: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4047 - accuracy: 0.8089 - val_loss: 0.4426 - val_accuracy: 0.8119\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3960 - accuracy: 0.8362\n",
            "Epoch 00483: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3960 - accuracy: 0.8362 - val_loss: 0.4499 - val_accuracy: 0.7921\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8238\n",
            "Epoch 00484: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3865 - accuracy: 0.8238 - val_loss: 0.4696 - val_accuracy: 0.7327\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.8263\n",
            "Epoch 00485: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3862 - accuracy: 0.8263 - val_loss: 0.4539 - val_accuracy: 0.8020\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.8114\n",
            "Epoch 00486: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4131 - accuracy: 0.8114 - val_loss: 0.4371 - val_accuracy: 0.7921\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.8288\n",
            "Epoch 00487: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3921 - accuracy: 0.8288 - val_loss: 0.4604 - val_accuracy: 0.7426\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.8089\n",
            "Epoch 00488: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4131 - accuracy: 0.8089 - val_loss: 0.4525 - val_accuracy: 0.8020\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.8164\n",
            "Epoch 00489: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4157 - accuracy: 0.8164 - val_loss: 0.4522 - val_accuracy: 0.7921\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3913 - accuracy: 0.8139\n",
            "Epoch 00490: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3913 - accuracy: 0.8139 - val_loss: 0.4777 - val_accuracy: 0.7327\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.8189\n",
            "Epoch 00491: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4089 - accuracy: 0.8189 - val_loss: 0.4594 - val_accuracy: 0.8020\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4530 - accuracy: 0.7891\n",
            "Epoch 00492: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4530 - accuracy: 0.7891 - val_loss: 0.4333 - val_accuracy: 0.8119\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3873 - accuracy: 0.8437\n",
            "Epoch 00493: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3873 - accuracy: 0.8437 - val_loss: 0.5145 - val_accuracy: 0.7129\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4242 - accuracy: 0.7866\n",
            "Epoch 00494: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4242 - accuracy: 0.7866 - val_loss: 0.4578 - val_accuracy: 0.8119\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4214 - accuracy: 0.8089\n",
            "Epoch 00495: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4214 - accuracy: 0.8089 - val_loss: 0.4407 - val_accuracy: 0.8119\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4126 - accuracy: 0.8238\n",
            "Epoch 00496: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4126 - accuracy: 0.8238 - val_loss: 0.5516 - val_accuracy: 0.7129\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.7494\n",
            "Epoch 00497: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4584 - accuracy: 0.7494 - val_loss: 0.4458 - val_accuracy: 0.8020\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8313\n",
            "Epoch 00498: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3997 - accuracy: 0.8313 - val_loss: 0.4299 - val_accuracy: 0.8119\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.8164\n",
            "Epoch 00499: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4139 - accuracy: 0.8164 - val_loss: 0.5263 - val_accuracy: 0.7129\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4436 - accuracy: 0.7742\n",
            "Epoch 00500: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4436 - accuracy: 0.7742 - val_loss: 0.4457 - val_accuracy: 0.8020\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8139\n",
            "Epoch 00501: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4118 - accuracy: 0.8139 - val_loss: 0.4370 - val_accuracy: 0.8020\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4078 - accuracy: 0.8213\n",
            "Epoch 00502: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4078 - accuracy: 0.8213 - val_loss: 0.5387 - val_accuracy: 0.7228\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.7643\n",
            "Epoch 00503: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4578 - accuracy: 0.7643 - val_loss: 0.4264 - val_accuracy: 0.8218\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8189\n",
            "Epoch 00504: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4118 - accuracy: 0.8189 - val_loss: 0.4364 - val_accuracy: 0.8020\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.8238\n",
            "Epoch 00505: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4094 - accuracy: 0.8238 - val_loss: 0.4624 - val_accuracy: 0.7525\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.8189\n",
            "Epoch 00506: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4112 - accuracy: 0.8189 - val_loss: 0.4281 - val_accuracy: 0.8020\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8089\n",
            "Epoch 00507: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3894 - accuracy: 0.8089 - val_loss: 0.4285 - val_accuracy: 0.8020\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.8412\n",
            "Epoch 00508: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3891 - accuracy: 0.8412 - val_loss: 0.4477 - val_accuracy: 0.7723\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8536\n",
            "Epoch 00509: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3654 - accuracy: 0.8536 - val_loss: 0.4393 - val_accuracy: 0.7921\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3903 - accuracy: 0.8213\n",
            "Epoch 00510: val_accuracy did not improve from 0.82178\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3903 - accuracy: 0.8213 - val_loss: 0.4222 - val_accuracy: 0.8218\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.8362\n",
            "Epoch 00511: val_accuracy improved from 0.82178 to 0.83168, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.3901 - accuracy: 0.8362 - val_loss: 0.4259 - val_accuracy: 0.8317\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.8313\n",
            "Epoch 00512: val_accuracy did not improve from 0.83168\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3777 - accuracy: 0.8313 - val_loss: 0.4196 - val_accuracy: 0.8317\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8412\n",
            "Epoch 00513: val_accuracy did not improve from 0.83168\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3654 - accuracy: 0.8412 - val_loss: 0.4204 - val_accuracy: 0.8218\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8412\n",
            "Epoch 00514: val_accuracy did not improve from 0.83168\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3875 - accuracy: 0.8412 - val_loss: 0.4350 - val_accuracy: 0.8020\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8561\n",
            "Epoch 00515: val_accuracy did not improve from 0.83168\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3637 - accuracy: 0.8561 - val_loss: 0.4278 - val_accuracy: 0.8317\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.8437\n",
            "Epoch 00516: val_accuracy did not improve from 0.83168\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3465 - accuracy: 0.8437 - val_loss: 0.4251 - val_accuracy: 0.8317\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.8486\n",
            "Epoch 00517: val_accuracy did not improve from 0.83168\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3642 - accuracy: 0.8486 - val_loss: 0.4262 - val_accuracy: 0.8317\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3657 - accuracy: 0.8462\n",
            "Epoch 00518: val_accuracy did not improve from 0.83168\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3657 - accuracy: 0.8462 - val_loss: 0.4180 - val_accuracy: 0.8317\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8313\n",
            "Epoch 00519: val_accuracy did not improve from 0.83168\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3759 - accuracy: 0.8313 - val_loss: 0.4147 - val_accuracy: 0.8317\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.8337\n",
            "Epoch 00520: val_accuracy improved from 0.83168 to 0.84158, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.3856 - accuracy: 0.8337 - val_loss: 0.4215 - val_accuracy: 0.8416\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3888 - accuracy: 0.8238\n",
            "Epoch 00521: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3888 - accuracy: 0.8238 - val_loss: 0.4380 - val_accuracy: 0.7921\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3992 - accuracy: 0.8213\n",
            "Epoch 00522: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3992 - accuracy: 0.8213 - val_loss: 0.4145 - val_accuracy: 0.8317\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.8486\n",
            "Epoch 00523: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3632 - accuracy: 0.8486 - val_loss: 0.4138 - val_accuracy: 0.8119\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8362\n",
            "Epoch 00524: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3970 - accuracy: 0.8362 - val_loss: 0.4285 - val_accuracy: 0.8119\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3873 - accuracy: 0.8089\n",
            "Epoch 00525: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3873 - accuracy: 0.8089 - val_loss: 0.4164 - val_accuracy: 0.8119\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.8387\n",
            "Epoch 00526: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3835 - accuracy: 0.8387 - val_loss: 0.4164 - val_accuracy: 0.8317\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3782 - accuracy: 0.8337\n",
            "Epoch 00527: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3782 - accuracy: 0.8337 - val_loss: 0.4329 - val_accuracy: 0.7921\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3693 - accuracy: 0.8288\n",
            "Epoch 00528: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3693 - accuracy: 0.8288 - val_loss: 0.4274 - val_accuracy: 0.8020\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.8065\n",
            "Epoch 00529: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4201 - accuracy: 0.8065 - val_loss: 0.4317 - val_accuracy: 0.8020\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3741 - accuracy: 0.8337\n",
            "Epoch 00530: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3741 - accuracy: 0.8337 - val_loss: 0.4315 - val_accuracy: 0.8119\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3873 - accuracy: 0.8189\n",
            "Epoch 00531: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3873 - accuracy: 0.8189 - val_loss: 0.4283 - val_accuracy: 0.8020\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.8164\n",
            "Epoch 00532: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4119 - accuracy: 0.8164 - val_loss: 0.4442 - val_accuracy: 0.7723\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.8288\n",
            "Epoch 00533: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3710 - accuracy: 0.8288 - val_loss: 0.4172 - val_accuracy: 0.8317\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3700 - accuracy: 0.8511\n",
            "Epoch 00534: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3700 - accuracy: 0.8511 - val_loss: 0.4295 - val_accuracy: 0.8119\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3605 - accuracy: 0.8536\n",
            "Epoch 00535: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3605 - accuracy: 0.8536 - val_loss: 0.4266 - val_accuracy: 0.8218\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.8337\n",
            "Epoch 00536: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3683 - accuracy: 0.8337 - val_loss: 0.4143 - val_accuracy: 0.8317\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3666 - accuracy: 0.8536\n",
            "Epoch 00537: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3666 - accuracy: 0.8536 - val_loss: 0.4095 - val_accuracy: 0.8317\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.8610\n",
            "Epoch 00538: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3671 - accuracy: 0.8610 - val_loss: 0.4301 - val_accuracy: 0.8020\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.8263\n",
            "Epoch 00539: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3727 - accuracy: 0.8263 - val_loss: 0.4207 - val_accuracy: 0.8317\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.8561\n",
            "Epoch 00540: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3690 - accuracy: 0.8561 - val_loss: 0.4307 - val_accuracy: 0.8020\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3801 - accuracy: 0.8337\n",
            "Epoch 00541: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3801 - accuracy: 0.8337 - val_loss: 0.4235 - val_accuracy: 0.8218\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.8511\n",
            "Epoch 00542: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3431 - accuracy: 0.8511 - val_loss: 0.4076 - val_accuracy: 0.8218\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.8462\n",
            "Epoch 00543: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3740 - accuracy: 0.8462 - val_loss: 0.4228 - val_accuracy: 0.8119\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3634 - accuracy: 0.8486\n",
            "Epoch 00544: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3634 - accuracy: 0.8486 - val_loss: 0.4130 - val_accuracy: 0.8317\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.8462\n",
            "Epoch 00545: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3674 - accuracy: 0.8462 - val_loss: 0.4367 - val_accuracy: 0.8119\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3859 - accuracy: 0.8486\n",
            "Epoch 00546: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3859 - accuracy: 0.8486 - val_loss: 0.4181 - val_accuracy: 0.8317\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3608 - accuracy: 0.8462\n",
            "Epoch 00547: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3608 - accuracy: 0.8462 - val_loss: 0.4108 - val_accuracy: 0.8119\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.8610\n",
            "Epoch 00548: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3684 - accuracy: 0.8610 - val_loss: 0.4328 - val_accuracy: 0.7921\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.8189\n",
            "Epoch 00549: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3740 - accuracy: 0.8189 - val_loss: 0.4067 - val_accuracy: 0.8119\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.8412\n",
            "Epoch 00550: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3789 - accuracy: 0.8412 - val_loss: 0.4395 - val_accuracy: 0.7822\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3814 - accuracy: 0.8486\n",
            "Epoch 00551: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3814 - accuracy: 0.8486 - val_loss: 0.4184 - val_accuracy: 0.8218\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.8362\n",
            "Epoch 00552: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3679 - accuracy: 0.8362 - val_loss: 0.4115 - val_accuracy: 0.8119\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.8238\n",
            "Epoch 00553: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3951 - accuracy: 0.8238 - val_loss: 0.4449 - val_accuracy: 0.7822\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8362\n",
            "Epoch 00554: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3715 - accuracy: 0.8362 - val_loss: 0.4098 - val_accuracy: 0.8317\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.8313\n",
            "Epoch 00555: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3845 - accuracy: 0.8313 - val_loss: 0.4158 - val_accuracy: 0.8119\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.8387\n",
            "Epoch 00556: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3658 - accuracy: 0.8387 - val_loss: 0.4576 - val_accuracy: 0.7426\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4075 - accuracy: 0.7940\n",
            "Epoch 00557: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4075 - accuracy: 0.7940 - val_loss: 0.4145 - val_accuracy: 0.8317\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.8511\n",
            "Epoch 00558: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3588 - accuracy: 0.8511 - val_loss: 0.4064 - val_accuracy: 0.8218\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.8263\n",
            "Epoch 00559: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3998 - accuracy: 0.8263 - val_loss: 0.4045 - val_accuracy: 0.8317\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3843 - accuracy: 0.8437\n",
            "Epoch 00560: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3843 - accuracy: 0.8437 - val_loss: 0.4068 - val_accuracy: 0.8416\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.8610\n",
            "Epoch 00561: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3572 - accuracy: 0.8610 - val_loss: 0.4406 - val_accuracy: 0.7624\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4033 - accuracy: 0.8040\n",
            "Epoch 00562: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4033 - accuracy: 0.8040 - val_loss: 0.4133 - val_accuracy: 0.8119\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3553 - accuracy: 0.8635\n",
            "Epoch 00563: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3553 - accuracy: 0.8635 - val_loss: 0.4164 - val_accuracy: 0.8218\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3557 - accuracy: 0.8561\n",
            "Epoch 00564: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3557 - accuracy: 0.8561 - val_loss: 0.4177 - val_accuracy: 0.8218\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.8511\n",
            "Epoch 00565: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3684 - accuracy: 0.8511 - val_loss: 0.4053 - val_accuracy: 0.8218\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.8610\n",
            "Epoch 00566: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3555 - accuracy: 0.8610 - val_loss: 0.4207 - val_accuracy: 0.8218\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3645 - accuracy: 0.8313\n",
            "Epoch 00567: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3645 - accuracy: 0.8313 - val_loss: 0.4018 - val_accuracy: 0.8416\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.8486\n",
            "Epoch 00568: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3679 - accuracy: 0.8486 - val_loss: 0.4029 - val_accuracy: 0.8416\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3482 - accuracy: 0.8586\n",
            "Epoch 00569: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3482 - accuracy: 0.8586 - val_loss: 0.4195 - val_accuracy: 0.8020\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8462\n",
            "Epoch 00570: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3527 - accuracy: 0.8462 - val_loss: 0.4021 - val_accuracy: 0.8218\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8586\n",
            "Epoch 00571: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3667 - accuracy: 0.8586 - val_loss: 0.4244 - val_accuracy: 0.8020\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.8859\n",
            "Epoch 00572: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3355 - accuracy: 0.8859 - val_loss: 0.4127 - val_accuracy: 0.8317\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8561\n",
            "Epoch 00573: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3406 - accuracy: 0.8561 - val_loss: 0.3990 - val_accuracy: 0.8317\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.8660\n",
            "Epoch 00574: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3387 - accuracy: 0.8660 - val_loss: 0.4419 - val_accuracy: 0.7822\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.8437\n",
            "Epoch 00575: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3621 - accuracy: 0.8437 - val_loss: 0.4016 - val_accuracy: 0.8119\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3623 - accuracy: 0.8462\n",
            "Epoch 00576: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3623 - accuracy: 0.8462 - val_loss: 0.4144 - val_accuracy: 0.8317\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.8387\n",
            "Epoch 00577: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3576 - accuracy: 0.8387 - val_loss: 0.4182 - val_accuracy: 0.8218\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3463 - accuracy: 0.8536\n",
            "Epoch 00578: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3463 - accuracy: 0.8536 - val_loss: 0.4010 - val_accuracy: 0.8119\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3705 - accuracy: 0.8561\n",
            "Epoch 00579: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3705 - accuracy: 0.8561 - val_loss: 0.4420 - val_accuracy: 0.7723\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.8412\n",
            "Epoch 00580: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3622 - accuracy: 0.8412 - val_loss: 0.4243 - val_accuracy: 0.8119\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4227 - accuracy: 0.7940\n",
            "Epoch 00581: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4227 - accuracy: 0.7940 - val_loss: 0.4106 - val_accuracy: 0.8218\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.8635\n",
            "Epoch 00582: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3386 - accuracy: 0.8635 - val_loss: 0.4327 - val_accuracy: 0.7723\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8313\n",
            "Epoch 00583: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3771 - accuracy: 0.8313 - val_loss: 0.4308 - val_accuracy: 0.8020\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4217 - accuracy: 0.7965\n",
            "Epoch 00584: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4217 - accuracy: 0.7965 - val_loss: 0.4014 - val_accuracy: 0.8416\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3853 - accuracy: 0.8412\n",
            "Epoch 00585: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3853 - accuracy: 0.8412 - val_loss: 0.4466 - val_accuracy: 0.7525\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4039 - accuracy: 0.8114\n",
            "Epoch 00586: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4039 - accuracy: 0.8114 - val_loss: 0.4258 - val_accuracy: 0.8020\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4135 - accuracy: 0.8164\n",
            "Epoch 00587: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4135 - accuracy: 0.8164 - val_loss: 0.4515 - val_accuracy: 0.7426\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.7965\n",
            "Epoch 00588: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4329 - accuracy: 0.7965 - val_loss: 0.4407 - val_accuracy: 0.8020\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.8139\n",
            "Epoch 00589: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4159 - accuracy: 0.8139 - val_loss: 0.4352 - val_accuracy: 0.8119\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8139\n",
            "Epoch 00590: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4003 - accuracy: 0.8139 - val_loss: 0.4492 - val_accuracy: 0.7525\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8387\n",
            "Epoch 00591: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3865 - accuracy: 0.8387 - val_loss: 0.4520 - val_accuracy: 0.7525\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3905 - accuracy: 0.8238\n",
            "Epoch 00592: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3905 - accuracy: 0.8238 - val_loss: 0.4283 - val_accuracy: 0.8020\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4581 - accuracy: 0.7940\n",
            "Epoch 00593: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4581 - accuracy: 0.7940 - val_loss: 0.4148 - val_accuracy: 0.8218\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4799 - accuracy: 0.8040\n",
            "Epoch 00594: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4799 - accuracy: 0.8040 - val_loss: 0.4073 - val_accuracy: 0.8416\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4169 - accuracy: 0.8387\n",
            "Epoch 00595: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4169 - accuracy: 0.8387 - val_loss: 0.4004 - val_accuracy: 0.8119\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.8337\n",
            "Epoch 00596: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3898 - accuracy: 0.8337 - val_loss: 0.5024 - val_accuracy: 0.7129\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.7816\n",
            "Epoch 00597: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4379 - accuracy: 0.7816 - val_loss: 0.4085 - val_accuracy: 0.8317\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.8462\n",
            "Epoch 00598: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3802 - accuracy: 0.8462 - val_loss: 0.4017 - val_accuracy: 0.8119\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4155 - accuracy: 0.8089\n",
            "Epoch 00599: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4155 - accuracy: 0.8089 - val_loss: 0.5850 - val_accuracy: 0.6436\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5957 - accuracy: 0.6700\n",
            "Epoch 00600: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5957 - accuracy: 0.6700 - val_loss: 0.4310 - val_accuracy: 0.8119\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.7891\n",
            "Epoch 00601: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4361 - accuracy: 0.7891 - val_loss: 0.4207 - val_accuracy: 0.8218\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.7816\n",
            "Epoch 00602: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4535 - accuracy: 0.7816 - val_loss: 0.4968 - val_accuracy: 0.7228\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4551 - accuracy: 0.7717\n",
            "Epoch 00603: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4551 - accuracy: 0.7717 - val_loss: 0.3915 - val_accuracy: 0.8317\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3552 - accuracy: 0.8610\n",
            "Epoch 00604: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3552 - accuracy: 0.8610 - val_loss: 0.4070 - val_accuracy: 0.8218\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3736 - accuracy: 0.8412\n",
            "Epoch 00605: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3736 - accuracy: 0.8412 - val_loss: 0.4129 - val_accuracy: 0.8218\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3702 - accuracy: 0.8561\n",
            "Epoch 00606: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3702 - accuracy: 0.8561 - val_loss: 0.4640 - val_accuracy: 0.7327\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3982 - accuracy: 0.8040\n",
            "Epoch 00607: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3982 - accuracy: 0.8040 - val_loss: 0.3982 - val_accuracy: 0.8020\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3704 - accuracy: 0.8536\n",
            "Epoch 00608: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3704 - accuracy: 0.8536 - val_loss: 0.4046 - val_accuracy: 0.8218\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4059 - accuracy: 0.8139\n",
            "Epoch 00609: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4059 - accuracy: 0.8139 - val_loss: 0.4008 - val_accuracy: 0.8317\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3947 - accuracy: 0.8189\n",
            "Epoch 00610: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3947 - accuracy: 0.8189 - val_loss: 0.4110 - val_accuracy: 0.8119\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4174 - accuracy: 0.8189\n",
            "Epoch 00611: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4174 - accuracy: 0.8189 - val_loss: 0.4016 - val_accuracy: 0.8416\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.8288\n",
            "Epoch 00612: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3926 - accuracy: 0.8288 - val_loss: 0.4052 - val_accuracy: 0.8317\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3933 - accuracy: 0.8114\n",
            "Epoch 00613: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3933 - accuracy: 0.8114 - val_loss: 0.4054 - val_accuracy: 0.8119\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.8511\n",
            "Epoch 00614: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3764 - accuracy: 0.8511 - val_loss: 0.4549 - val_accuracy: 0.7822\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8263\n",
            "Epoch 00615: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3931 - accuracy: 0.8263 - val_loss: 0.4339 - val_accuracy: 0.7921\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.8511\n",
            "Epoch 00616: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3488 - accuracy: 0.8511 - val_loss: 0.4215 - val_accuracy: 0.8119\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8387\n",
            "Epoch 00617: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3910 - accuracy: 0.8387 - val_loss: 0.4117 - val_accuracy: 0.8218\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3917 - accuracy: 0.8387\n",
            "Epoch 00618: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3917 - accuracy: 0.8387 - val_loss: 0.4043 - val_accuracy: 0.8119\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3989 - accuracy: 0.8337\n",
            "Epoch 00619: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3989 - accuracy: 0.8337 - val_loss: 0.4170 - val_accuracy: 0.8218\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.8486\n",
            "Epoch 00620: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3620 - accuracy: 0.8486 - val_loss: 0.4273 - val_accuracy: 0.8218\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8362\n",
            "Epoch 00621: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3742 - accuracy: 0.8362 - val_loss: 0.4089 - val_accuracy: 0.8317\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.8660\n",
            "Epoch 00622: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3438 - accuracy: 0.8660 - val_loss: 0.3909 - val_accuracy: 0.8416\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3675 - accuracy: 0.8387\n",
            "Epoch 00623: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3675 - accuracy: 0.8387 - val_loss: 0.4374 - val_accuracy: 0.7723\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8362\n",
            "Epoch 00624: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3896 - accuracy: 0.8362 - val_loss: 0.4117 - val_accuracy: 0.8020\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.8313\n",
            "Epoch 00625: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3835 - accuracy: 0.8313 - val_loss: 0.4063 - val_accuracy: 0.8317\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.8437\n",
            "Epoch 00626: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3524 - accuracy: 0.8437 - val_loss: 0.4881 - val_accuracy: 0.7228\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4203 - accuracy: 0.7866\n",
            "Epoch 00627: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4203 - accuracy: 0.7866 - val_loss: 0.4908 - val_accuracy: 0.7921\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4903 - accuracy: 0.7643\n",
            "Epoch 00628: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4903 - accuracy: 0.7643 - val_loss: 0.4707 - val_accuracy: 0.8119\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4724 - accuracy: 0.7667\n",
            "Epoch 00629: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4724 - accuracy: 0.7667 - val_loss: 0.5805 - val_accuracy: 0.6337\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5266 - accuracy: 0.7345\n",
            "Epoch 00630: val_accuracy did not improve from 0.84158\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5266 - accuracy: 0.7345 - val_loss: 0.4123 - val_accuracy: 0.8119\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.8238\n",
            "Epoch 00631: val_accuracy improved from 0.84158 to 0.85149, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3785 - accuracy: 0.8238 - val_loss: 0.3795 - val_accuracy: 0.8515\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4020 - accuracy: 0.8288\n",
            "Epoch 00632: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4020 - accuracy: 0.8288 - val_loss: 0.3941 - val_accuracy: 0.8218\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4372 - accuracy: 0.7940\n",
            "Epoch 00633: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4372 - accuracy: 0.7940 - val_loss: 0.4098 - val_accuracy: 0.8020\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8164\n",
            "Epoch 00634: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4028 - accuracy: 0.8164 - val_loss: 0.4374 - val_accuracy: 0.7723\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4353 - accuracy: 0.7916\n",
            "Epoch 00635: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4353 - accuracy: 0.7916 - val_loss: 0.4359 - val_accuracy: 0.8020\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4442 - accuracy: 0.8114\n",
            "Epoch 00636: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4442 - accuracy: 0.8114 - val_loss: 0.4276 - val_accuracy: 0.8119\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4442 - accuracy: 0.8089\n",
            "Epoch 00637: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4442 - accuracy: 0.8089 - val_loss: 0.3875 - val_accuracy: 0.8416\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3739 - accuracy: 0.8412\n",
            "Epoch 00638: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3739 - accuracy: 0.8412 - val_loss: 0.3949 - val_accuracy: 0.8515\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3743 - accuracy: 0.8536\n",
            "Epoch 00639: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3743 - accuracy: 0.8536 - val_loss: 0.3913 - val_accuracy: 0.8416\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8784\n",
            "Epoch 00640: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3400 - accuracy: 0.8784 - val_loss: 0.4681 - val_accuracy: 0.7327\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.7792\n",
            "Epoch 00641: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4159 - accuracy: 0.7792 - val_loss: 0.3989 - val_accuracy: 0.8317\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.8511\n",
            "Epoch 00642: val_accuracy did not improve from 0.85149\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3566 - accuracy: 0.8511 - val_loss: 0.3800 - val_accuracy: 0.8416\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3589 - accuracy: 0.8586\n",
            "Epoch 00643: val_accuracy improved from 0.85149 to 0.86139, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.3589 - accuracy: 0.8586 - val_loss: 0.3900 - val_accuracy: 0.8614\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.8437\n",
            "Epoch 00644: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3777 - accuracy: 0.8437 - val_loss: 0.3904 - val_accuracy: 0.8119\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.8313\n",
            "Epoch 00645: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3868 - accuracy: 0.8313 - val_loss: 0.4047 - val_accuracy: 0.8416\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.8610\n",
            "Epoch 00646: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3343 - accuracy: 0.8610 - val_loss: 0.4434 - val_accuracy: 0.7723\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.8313\n",
            "Epoch 00647: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3732 - accuracy: 0.8313 - val_loss: 0.4076 - val_accuracy: 0.8317\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.8511\n",
            "Epoch 00648: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3452 - accuracy: 0.8511 - val_loss: 0.3851 - val_accuracy: 0.8515\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8486\n",
            "Epoch 00649: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3689 - accuracy: 0.8486 - val_loss: 0.3898 - val_accuracy: 0.8317\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3595 - accuracy: 0.8561\n",
            "Epoch 00650: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3595 - accuracy: 0.8561 - val_loss: 0.3851 - val_accuracy: 0.8515\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.8437\n",
            "Epoch 00651: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3773 - accuracy: 0.8437 - val_loss: 0.3905 - val_accuracy: 0.8119\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.8263\n",
            "Epoch 00652: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4112 - accuracy: 0.8263 - val_loss: 0.4324 - val_accuracy: 0.7822\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4383 - accuracy: 0.7891\n",
            "Epoch 00653: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4383 - accuracy: 0.7891 - val_loss: 0.4276 - val_accuracy: 0.8218\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4363 - accuracy: 0.7965\n",
            "Epoch 00654: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4363 - accuracy: 0.7965 - val_loss: 0.3859 - val_accuracy: 0.8317\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3605 - accuracy: 0.8635\n",
            "Epoch 00655: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3605 - accuracy: 0.8635 - val_loss: 0.5561 - val_accuracy: 0.6931\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.7717\n",
            "Epoch 00656: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4491 - accuracy: 0.7717 - val_loss: 0.4114 - val_accuracy: 0.8218\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3460 - accuracy: 0.8635\n",
            "Epoch 00657: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3460 - accuracy: 0.8635 - val_loss: 0.4487 - val_accuracy: 0.7822\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.8040\n",
            "Epoch 00658: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4380 - accuracy: 0.8040 - val_loss: 0.4378 - val_accuracy: 0.7921\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.8189\n",
            "Epoch 00659: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4336 - accuracy: 0.8189 - val_loss: 0.4184 - val_accuracy: 0.8218\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8561\n",
            "Epoch 00660: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3406 - accuracy: 0.8561 - val_loss: 0.4090 - val_accuracy: 0.8119\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3720 - accuracy: 0.8387\n",
            "Epoch 00661: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3720 - accuracy: 0.8387 - val_loss: 0.3937 - val_accuracy: 0.8218\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4047 - accuracy: 0.8065\n",
            "Epoch 00662: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4047 - accuracy: 0.8065 - val_loss: 0.3847 - val_accuracy: 0.8416\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8313\n",
            "Epoch 00663: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3973 - accuracy: 0.8313 - val_loss: 0.4378 - val_accuracy: 0.7525\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.7667\n",
            "Epoch 00664: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4491 - accuracy: 0.7667 - val_loss: 0.4038 - val_accuracy: 0.8119\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4258 - accuracy: 0.8040\n",
            "Epoch 00665: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4258 - accuracy: 0.8040 - val_loss: 0.4015 - val_accuracy: 0.8416\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.8486\n",
            "Epoch 00666: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3555 - accuracy: 0.8486 - val_loss: 0.5483 - val_accuracy: 0.6832\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4531 - accuracy: 0.7618\n",
            "Epoch 00667: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4531 - accuracy: 0.7618 - val_loss: 0.4211 - val_accuracy: 0.8020\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8362\n",
            "Epoch 00668: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3872 - accuracy: 0.8362 - val_loss: 0.4492 - val_accuracy: 0.8020\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4353 - accuracy: 0.7965\n",
            "Epoch 00669: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4353 - accuracy: 0.7965 - val_loss: 0.4440 - val_accuracy: 0.7921\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.8189\n",
            "Epoch 00670: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4192 - accuracy: 0.8189 - val_loss: 0.4491 - val_accuracy: 0.7723\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.7742\n",
            "Epoch 00671: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4422 - accuracy: 0.7742 - val_loss: 0.4069 - val_accuracy: 0.8020\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8561\n",
            "Epoch 00672: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3718 - accuracy: 0.8561 - val_loss: 0.4030 - val_accuracy: 0.8317\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.8437\n",
            "Epoch 00673: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3765 - accuracy: 0.8437 - val_loss: 0.4504 - val_accuracy: 0.7426\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.7816\n",
            "Epoch 00674: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4221 - accuracy: 0.7816 - val_loss: 0.4357 - val_accuracy: 0.8020\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.8164\n",
            "Epoch 00675: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4124 - accuracy: 0.8164 - val_loss: 0.3998 - val_accuracy: 0.8218\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8635\n",
            "Epoch 00676: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3527 - accuracy: 0.8635 - val_loss: 0.5217 - val_accuracy: 0.7327\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.7643\n",
            "Epoch 00677: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4537 - accuracy: 0.7643 - val_loss: 0.3785 - val_accuracy: 0.8416\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3534 - accuracy: 0.8635\n",
            "Epoch 00678: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3534 - accuracy: 0.8635 - val_loss: 0.3848 - val_accuracy: 0.8416\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3650 - accuracy: 0.8561\n",
            "Epoch 00679: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3650 - accuracy: 0.8561 - val_loss: 0.3990 - val_accuracy: 0.8416\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3962 - accuracy: 0.8040\n",
            "Epoch 00680: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3962 - accuracy: 0.8040 - val_loss: 0.3727 - val_accuracy: 0.8515\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8387\n",
            "Epoch 00681: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3828 - accuracy: 0.8387 - val_loss: 0.3771 - val_accuracy: 0.8515\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8511\n",
            "Epoch 00682: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3696 - accuracy: 0.8511 - val_loss: 0.4209 - val_accuracy: 0.8119\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.7990\n",
            "Epoch 00683: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4100 - accuracy: 0.7990 - val_loss: 0.4429 - val_accuracy: 0.7426\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8561\n",
            "Epoch 00684: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3476 - accuracy: 0.8561 - val_loss: 0.4029 - val_accuracy: 0.8218\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8635\n",
            "Epoch 00685: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3646 - accuracy: 0.8635 - val_loss: 0.4026 - val_accuracy: 0.8416\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3938 - accuracy: 0.8263\n",
            "Epoch 00686: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3938 - accuracy: 0.8263 - val_loss: 0.4035 - val_accuracy: 0.8416\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.8313\n",
            "Epoch 00687: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4015 - accuracy: 0.8313 - val_loss: 0.4011 - val_accuracy: 0.8515\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.8486\n",
            "Epoch 00688: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3890 - accuracy: 0.8486 - val_loss: 0.4113 - val_accuracy: 0.8218\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3984 - accuracy: 0.8288\n",
            "Epoch 00689: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3984 - accuracy: 0.8288 - val_loss: 0.4070 - val_accuracy: 0.8317\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.8313\n",
            "Epoch 00690: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4015 - accuracy: 0.8313 - val_loss: 0.3996 - val_accuracy: 0.8515\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4041 - accuracy: 0.8362\n",
            "Epoch 00691: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4041 - accuracy: 0.8362 - val_loss: 0.3974 - val_accuracy: 0.8515\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8561\n",
            "Epoch 00692: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3696 - accuracy: 0.8561 - val_loss: 0.3982 - val_accuracy: 0.8515\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.8610\n",
            "Epoch 00693: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3683 - accuracy: 0.8610 - val_loss: 0.3962 - val_accuracy: 0.8515\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3605 - accuracy: 0.8561\n",
            "Epoch 00694: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3605 - accuracy: 0.8561 - val_loss: 0.3953 - val_accuracy: 0.8515\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.8437\n",
            "Epoch 00695: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3738 - accuracy: 0.8437 - val_loss: 0.3906 - val_accuracy: 0.8317\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.8313\n",
            "Epoch 00696: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3927 - accuracy: 0.8313 - val_loss: 0.3890 - val_accuracy: 0.8614\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3730 - accuracy: 0.8586\n",
            "Epoch 00697: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3730 - accuracy: 0.8586 - val_loss: 0.3882 - val_accuracy: 0.8515\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8486\n",
            "Epoch 00698: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3973 - accuracy: 0.8486 - val_loss: 0.3835 - val_accuracy: 0.8515\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3659 - accuracy: 0.8511\n",
            "Epoch 00699: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3659 - accuracy: 0.8511 - val_loss: 0.3834 - val_accuracy: 0.8515\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4058 - accuracy: 0.8263\n",
            "Epoch 00700: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4058 - accuracy: 0.8263 - val_loss: 0.3806 - val_accuracy: 0.8515\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8437\n",
            "Epoch 00701: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3696 - accuracy: 0.8437 - val_loss: 0.3785 - val_accuracy: 0.8515\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.8635\n",
            "Epoch 00702: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3656 - accuracy: 0.8635 - val_loss: 0.3809 - val_accuracy: 0.8515\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3673 - accuracy: 0.8635\n",
            "Epoch 00703: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3673 - accuracy: 0.8635 - val_loss: 0.3827 - val_accuracy: 0.8614\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.8586\n",
            "Epoch 00704: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3570 - accuracy: 0.8586 - val_loss: 0.3793 - val_accuracy: 0.8515\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3870 - accuracy: 0.8486\n",
            "Epoch 00705: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3870 - accuracy: 0.8486 - val_loss: 0.3780 - val_accuracy: 0.8515\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3665 - accuracy: 0.8486\n",
            "Epoch 00706: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3665 - accuracy: 0.8486 - val_loss: 0.3839 - val_accuracy: 0.8416\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.8610\n",
            "Epoch 00707: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3567 - accuracy: 0.8610 - val_loss: 0.4050 - val_accuracy: 0.8218\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.8362\n",
            "Epoch 00708: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4010 - accuracy: 0.8362 - val_loss: 0.3961 - val_accuracy: 0.8119\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.8486\n",
            "Epoch 00709: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3883 - accuracy: 0.8486 - val_loss: 0.3924 - val_accuracy: 0.8515\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3617 - accuracy: 0.8635\n",
            "Epoch 00710: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3617 - accuracy: 0.8635 - val_loss: 0.4456 - val_accuracy: 0.7525\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.7965\n",
            "Epoch 00711: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4119 - accuracy: 0.7965 - val_loss: 0.4321 - val_accuracy: 0.8020\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.7767\n",
            "Epoch 00712: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4503 - accuracy: 0.7767 - val_loss: 0.3930 - val_accuracy: 0.8119\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.8412\n",
            "Epoch 00713: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3856 - accuracy: 0.8412 - val_loss: 0.6372 - val_accuracy: 0.6337\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.7146\n",
            "Epoch 00714: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5741 - accuracy: 0.7146 - val_loss: 0.4586 - val_accuracy: 0.7921\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4698 - accuracy: 0.7643\n",
            "Epoch 00715: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4698 - accuracy: 0.7643 - val_loss: 0.5139 - val_accuracy: 0.7822\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.7841\n",
            "Epoch 00716: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5073 - accuracy: 0.7841 - val_loss: 0.5136 - val_accuracy: 0.7525\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.6551\n",
            "Epoch 00717: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5965 - accuracy: 0.6551 - val_loss: 0.5052 - val_accuracy: 0.7822\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5317 - accuracy: 0.7692\n",
            "Epoch 00718: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5317 - accuracy: 0.7692 - val_loss: 0.5867 - val_accuracy: 0.6733\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.6873\n",
            "Epoch 00719: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5992 - accuracy: 0.6873 - val_loss: 0.5089 - val_accuracy: 0.7822\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4988 - accuracy: 0.7990\n",
            "Epoch 00720: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4988 - accuracy: 0.7990 - val_loss: 0.5127 - val_accuracy: 0.7426\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.6849\n",
            "Epoch 00721: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5680 - accuracy: 0.6849 - val_loss: 0.4884 - val_accuracy: 0.8020\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4865 - accuracy: 0.7916\n",
            "Epoch 00722: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4865 - accuracy: 0.7916 - val_loss: 0.5187 - val_accuracy: 0.7327\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.7444\n",
            "Epoch 00723: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5101 - accuracy: 0.7444 - val_loss: 0.4089 - val_accuracy: 0.8218\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4130 - accuracy: 0.8387\n",
            "Epoch 00724: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4130 - accuracy: 0.8387 - val_loss: 0.6923 - val_accuracy: 0.6238\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6595 - accuracy: 0.6551\n",
            "Epoch 00725: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6595 - accuracy: 0.6551 - val_loss: 0.4553 - val_accuracy: 0.8119\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.7618\n",
            "Epoch 00726: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4753 - accuracy: 0.7618 - val_loss: 0.5790 - val_accuracy: 0.6931\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.6898\n",
            "Epoch 00727: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5888 - accuracy: 0.6898 - val_loss: 0.4570 - val_accuracy: 0.8119\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.7519\n",
            "Epoch 00728: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5007 - accuracy: 0.7519 - val_loss: 0.5449 - val_accuracy: 0.6931\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.7122\n",
            "Epoch 00729: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5379 - accuracy: 0.7122 - val_loss: 0.3975 - val_accuracy: 0.8218\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3665 - accuracy: 0.8462\n",
            "Epoch 00730: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3665 - accuracy: 0.8462 - val_loss: 0.4127 - val_accuracy: 0.8119\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3889 - accuracy: 0.8462\n",
            "Epoch 00731: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3889 - accuracy: 0.8462 - val_loss: 0.4401 - val_accuracy: 0.7921\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.8015\n",
            "Epoch 00732: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4391 - accuracy: 0.8015 - val_loss: 0.4259 - val_accuracy: 0.8020\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.8065\n",
            "Epoch 00733: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4335 - accuracy: 0.8065 - val_loss: 0.3882 - val_accuracy: 0.8317\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8362\n",
            "Epoch 00734: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3953 - accuracy: 0.8362 - val_loss: 0.4238 - val_accuracy: 0.7921\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4257 - accuracy: 0.8040\n",
            "Epoch 00735: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4257 - accuracy: 0.8040 - val_loss: 0.3842 - val_accuracy: 0.8416\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8462\n",
            "Epoch 00736: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3771 - accuracy: 0.8462 - val_loss: 0.3905 - val_accuracy: 0.8416\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3937 - accuracy: 0.8462\n",
            "Epoch 00737: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3937 - accuracy: 0.8462 - val_loss: 0.4200 - val_accuracy: 0.8020\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.8238\n",
            "Epoch 00738: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3921 - accuracy: 0.8238 - val_loss: 0.4101 - val_accuracy: 0.8218\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3804 - accuracy: 0.8486\n",
            "Epoch 00739: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3804 - accuracy: 0.8486 - val_loss: 0.3882 - val_accuracy: 0.8317\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3893 - accuracy: 0.8387\n",
            "Epoch 00740: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3893 - accuracy: 0.8387 - val_loss: 0.3857 - val_accuracy: 0.8515\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3719 - accuracy: 0.8610\n",
            "Epoch 00741: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3719 - accuracy: 0.8610 - val_loss: 0.3974 - val_accuracy: 0.8416\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4079 - accuracy: 0.8213\n",
            "Epoch 00742: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4079 - accuracy: 0.8213 - val_loss: 0.3891 - val_accuracy: 0.8515\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3717 - accuracy: 0.8610\n",
            "Epoch 00743: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3717 - accuracy: 0.8610 - val_loss: 0.4377 - val_accuracy: 0.7822\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4205 - accuracy: 0.7940\n",
            "Epoch 00744: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4205 - accuracy: 0.7940 - val_loss: 0.4014 - val_accuracy: 0.8515\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.8685\n",
            "Epoch 00745: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3835 - accuracy: 0.8685 - val_loss: 0.3778 - val_accuracy: 0.8515\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3794 - accuracy: 0.8561\n",
            "Epoch 00746: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3794 - accuracy: 0.8561 - val_loss: 0.3876 - val_accuracy: 0.8515\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3857 - accuracy: 0.8536\n",
            "Epoch 00747: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3857 - accuracy: 0.8536 - val_loss: 0.3733 - val_accuracy: 0.8515\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3388 - accuracy: 0.8759\n",
            "Epoch 00748: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3388 - accuracy: 0.8759 - val_loss: 0.3774 - val_accuracy: 0.8614\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.8586\n",
            "Epoch 00749: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3727 - accuracy: 0.8586 - val_loss: 0.4393 - val_accuracy: 0.7624\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.7965\n",
            "Epoch 00750: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4194 - accuracy: 0.7965 - val_loss: 0.3940 - val_accuracy: 0.8317\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8561\n",
            "Epoch 00751: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3775 - accuracy: 0.8561 - val_loss: 0.3845 - val_accuracy: 0.8317\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3829 - accuracy: 0.8362\n",
            "Epoch 00752: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3829 - accuracy: 0.8362 - val_loss: 0.4122 - val_accuracy: 0.8317\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4103 - accuracy: 0.8263\n",
            "Epoch 00753: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4103 - accuracy: 0.8263 - val_loss: 0.3735 - val_accuracy: 0.8416\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3826 - accuracy: 0.8437\n",
            "Epoch 00754: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3826 - accuracy: 0.8437 - val_loss: 0.3731 - val_accuracy: 0.8515\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8462\n",
            "Epoch 00755: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3689 - accuracy: 0.8462 - val_loss: 0.4359 - val_accuracy: 0.7624\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3988 - accuracy: 0.8213\n",
            "Epoch 00756: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3988 - accuracy: 0.8213 - val_loss: 0.3904 - val_accuracy: 0.8416\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8437\n",
            "Epoch 00757: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3973 - accuracy: 0.8437 - val_loss: 0.3815 - val_accuracy: 0.8416\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.8536\n",
            "Epoch 00758: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3810 - accuracy: 0.8536 - val_loss: 0.3986 - val_accuracy: 0.8218\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3853 - accuracy: 0.8114\n",
            "Epoch 00759: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3853 - accuracy: 0.8114 - val_loss: 0.3736 - val_accuracy: 0.8416\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.8536\n",
            "Epoch 00760: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3621 - accuracy: 0.8536 - val_loss: 0.3805 - val_accuracy: 0.8515\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 0.8437\n",
            "Epoch 00761: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3778 - accuracy: 0.8437 - val_loss: 0.4036 - val_accuracy: 0.8119\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.8387\n",
            "Epoch 00762: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3783 - accuracy: 0.8387 - val_loss: 0.3895 - val_accuracy: 0.8515\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.8710\n",
            "Epoch 00763: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3419 - accuracy: 0.8710 - val_loss: 0.3789 - val_accuracy: 0.8515\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3748 - accuracy: 0.8610\n",
            "Epoch 00764: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3748 - accuracy: 0.8610 - val_loss: 0.4013 - val_accuracy: 0.8317\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3640 - accuracy: 0.8462\n",
            "Epoch 00765: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3640 - accuracy: 0.8462 - val_loss: 0.3831 - val_accuracy: 0.8515\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3694 - accuracy: 0.8586\n",
            "Epoch 00766: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3694 - accuracy: 0.8586 - val_loss: 0.3873 - val_accuracy: 0.8416\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8536\n",
            "Epoch 00767: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3478 - accuracy: 0.8536 - val_loss: 0.3887 - val_accuracy: 0.8515\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8586\n",
            "Epoch 00768: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3478 - accuracy: 0.8586 - val_loss: 0.3845 - val_accuracy: 0.8515\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8710\n",
            "Epoch 00769: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3495 - accuracy: 0.8710 - val_loss: 0.3895 - val_accuracy: 0.8416\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.8610\n",
            "Epoch 00770: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3569 - accuracy: 0.8610 - val_loss: 0.3991 - val_accuracy: 0.8416\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3498 - accuracy: 0.8710\n",
            "Epoch 00771: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3498 - accuracy: 0.8710 - val_loss: 0.3906 - val_accuracy: 0.8416\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.8511\n",
            "Epoch 00772: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3690 - accuracy: 0.8511 - val_loss: 0.3900 - val_accuracy: 0.8416\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3754 - accuracy: 0.8486\n",
            "Epoch 00773: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3754 - accuracy: 0.8486 - val_loss: 0.4122 - val_accuracy: 0.8119\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.8412\n",
            "Epoch 00774: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3904 - accuracy: 0.8412 - val_loss: 0.3751 - val_accuracy: 0.8515\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3816 - accuracy: 0.8462\n",
            "Epoch 00775: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3816 - accuracy: 0.8462 - val_loss: 0.3773 - val_accuracy: 0.8515\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.8586\n",
            "Epoch 00776: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3584 - accuracy: 0.8586 - val_loss: 0.4132 - val_accuracy: 0.8020\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3615 - accuracy: 0.8412\n",
            "Epoch 00777: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3615 - accuracy: 0.8412 - val_loss: 0.3777 - val_accuracy: 0.8317\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4056 - accuracy: 0.8189\n",
            "Epoch 00778: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4056 - accuracy: 0.8189 - val_loss: 0.3732 - val_accuracy: 0.8515\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.8437\n",
            "Epoch 00779: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3899 - accuracy: 0.8437 - val_loss: 0.4638 - val_accuracy: 0.7228\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4093 - accuracy: 0.8114\n",
            "Epoch 00780: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4093 - accuracy: 0.8114 - val_loss: 0.3921 - val_accuracy: 0.8317\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.8238\n",
            "Epoch 00781: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3985 - accuracy: 0.8238 - val_loss: 0.3909 - val_accuracy: 0.8317\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3964 - accuracy: 0.8337\n",
            "Epoch 00782: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3964 - accuracy: 0.8337 - val_loss: 0.4139 - val_accuracy: 0.8119\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8288\n",
            "Epoch 00783: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3791 - accuracy: 0.8288 - val_loss: 0.3750 - val_accuracy: 0.8515\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.8635\n",
            "Epoch 00784: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3544 - accuracy: 0.8635 - val_loss: 0.3693 - val_accuracy: 0.8416\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.8635\n",
            "Epoch 00785: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3497 - accuracy: 0.8635 - val_loss: 0.3761 - val_accuracy: 0.8515\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.8610\n",
            "Epoch 00786: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3569 - accuracy: 0.8610 - val_loss: 0.3943 - val_accuracy: 0.8218\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.8263\n",
            "Epoch 00787: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3757 - accuracy: 0.8263 - val_loss: 0.3700 - val_accuracy: 0.8416\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.8635\n",
            "Epoch 00788: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3576 - accuracy: 0.8635 - val_loss: 0.3665 - val_accuracy: 0.8614\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8635\n",
            "Epoch 00789: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3559 - accuracy: 0.8635 - val_loss: 0.3873 - val_accuracy: 0.8317\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.8437\n",
            "Epoch 00790: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3788 - accuracy: 0.8437 - val_loss: 0.3761 - val_accuracy: 0.8416\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.8412\n",
            "Epoch 00791: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3851 - accuracy: 0.8412 - val_loss: 0.3872 - val_accuracy: 0.8614\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.8685\n",
            "Epoch 00792: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3576 - accuracy: 0.8685 - val_loss: 0.3755 - val_accuracy: 0.8614\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.8635\n",
            "Epoch 00793: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3499 - accuracy: 0.8635 - val_loss: 0.3677 - val_accuracy: 0.8614\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.8660\n",
            "Epoch 00794: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3671 - accuracy: 0.8660 - val_loss: 0.3645 - val_accuracy: 0.8515\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.8784\n",
            "Epoch 00795: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3497 - accuracy: 0.8784 - val_loss: 0.3818 - val_accuracy: 0.8614\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3733 - accuracy: 0.8685\n",
            "Epoch 00796: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3733 - accuracy: 0.8685 - val_loss: 0.3904 - val_accuracy: 0.8515\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3247 - accuracy: 0.8834\n",
            "Epoch 00797: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3247 - accuracy: 0.8834 - val_loss: 0.3712 - val_accuracy: 0.8614\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8685\n",
            "Epoch 00798: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3454 - accuracy: 0.8685 - val_loss: 0.3687 - val_accuracy: 0.8515\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3644 - accuracy: 0.8437\n",
            "Epoch 00799: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3644 - accuracy: 0.8437 - val_loss: 0.3761 - val_accuracy: 0.8515\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3579 - accuracy: 0.8561\n",
            "Epoch 00800: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3579 - accuracy: 0.8561 - val_loss: 0.3722 - val_accuracy: 0.8515\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.8586\n",
            "Epoch 00801: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3560 - accuracy: 0.8586 - val_loss: 0.3873 - val_accuracy: 0.8416\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.8511\n",
            "Epoch 00802: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3820 - accuracy: 0.8511 - val_loss: 0.3872 - val_accuracy: 0.8416\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.8685\n",
            "Epoch 00803: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3431 - accuracy: 0.8685 - val_loss: 0.3645 - val_accuracy: 0.8515\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.8561\n",
            "Epoch 00804: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3629 - accuracy: 0.8561 - val_loss: 0.3650 - val_accuracy: 0.8515\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8759\n",
            "Epoch 00805: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3445 - accuracy: 0.8759 - val_loss: 0.4038 - val_accuracy: 0.8218\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8486\n",
            "Epoch 00806: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3636 - accuracy: 0.8486 - val_loss: 0.3773 - val_accuracy: 0.8515\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8536\n",
            "Epoch 00807: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3731 - accuracy: 0.8536 - val_loss: 0.3849 - val_accuracy: 0.8515\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3911 - accuracy: 0.8387\n",
            "Epoch 00808: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3911 - accuracy: 0.8387 - val_loss: 0.3822 - val_accuracy: 0.8416\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3472 - accuracy: 0.8561\n",
            "Epoch 00809: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3472 - accuracy: 0.8561 - val_loss: 0.3693 - val_accuracy: 0.8515\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3511 - accuracy: 0.8685\n",
            "Epoch 00810: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3511 - accuracy: 0.8685 - val_loss: 0.3656 - val_accuracy: 0.8515\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8362\n",
            "Epoch 00811: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3896 - accuracy: 0.8362 - val_loss: 0.3907 - val_accuracy: 0.8317\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.8610\n",
            "Epoch 00812: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3573 - accuracy: 0.8610 - val_loss: 0.3753 - val_accuracy: 0.8614\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.8660\n",
            "Epoch 00813: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3480 - accuracy: 0.8660 - val_loss: 0.3608 - val_accuracy: 0.8515\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.8710\n",
            "Epoch 00814: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3421 - accuracy: 0.8710 - val_loss: 0.3742 - val_accuracy: 0.8614\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.8263\n",
            "Epoch 00815: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4051 - accuracy: 0.8263 - val_loss: 0.3676 - val_accuracy: 0.8515\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.8660\n",
            "Epoch 00816: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3431 - accuracy: 0.8660 - val_loss: 0.4222 - val_accuracy: 0.7822\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.7916\n",
            "Epoch 00817: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4156 - accuracy: 0.7916 - val_loss: 0.3775 - val_accuracy: 0.8416\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3561 - accuracy: 0.8660\n",
            "Epoch 00818: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3561 - accuracy: 0.8660 - val_loss: 0.3599 - val_accuracy: 0.8515\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.8610\n",
            "Epoch 00819: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3622 - accuracy: 0.8610 - val_loss: 0.3727 - val_accuracy: 0.8614\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8462\n",
            "Epoch 00820: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3696 - accuracy: 0.8462 - val_loss: 0.3658 - val_accuracy: 0.8416\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.8610\n",
            "Epoch 00821: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3584 - accuracy: 0.8610 - val_loss: 0.3989 - val_accuracy: 0.8218\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3730 - accuracy: 0.8437\n",
            "Epoch 00822: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3730 - accuracy: 0.8437 - val_loss: 0.3772 - val_accuracy: 0.8416\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.8561\n",
            "Epoch 00823: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3551 - accuracy: 0.8561 - val_loss: 0.3771 - val_accuracy: 0.8317\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3746 - accuracy: 0.8511\n",
            "Epoch 00824: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3746 - accuracy: 0.8511 - val_loss: 0.4127 - val_accuracy: 0.7921\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3813 - accuracy: 0.8288\n",
            "Epoch 00825: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3813 - accuracy: 0.8288 - val_loss: 0.3640 - val_accuracy: 0.8515\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3579 - accuracy: 0.8635\n",
            "Epoch 00826: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3579 - accuracy: 0.8635 - val_loss: 0.3632 - val_accuracy: 0.8515\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.8586\n",
            "Epoch 00827: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3480 - accuracy: 0.8586 - val_loss: 0.4126 - val_accuracy: 0.8020\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.8263\n",
            "Epoch 00828: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3941 - accuracy: 0.8263 - val_loss: 0.3897 - val_accuracy: 0.8317\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3991 - accuracy: 0.8313\n",
            "Epoch 00829: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3991 - accuracy: 0.8313 - val_loss: 0.3954 - val_accuracy: 0.8317\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4017 - accuracy: 0.8387\n",
            "Epoch 00830: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4017 - accuracy: 0.8387 - val_loss: 0.3923 - val_accuracy: 0.8317\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8635\n",
            "Epoch 00831: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3523 - accuracy: 0.8635 - val_loss: 0.3748 - val_accuracy: 0.8416\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4040 - accuracy: 0.8362\n",
            "Epoch 00832: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4040 - accuracy: 0.8362 - val_loss: 0.3684 - val_accuracy: 0.8416\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.8337\n",
            "Epoch 00833: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3819 - accuracy: 0.8337 - val_loss: 0.3565 - val_accuracy: 0.8515\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.8586\n",
            "Epoch 00834: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3576 - accuracy: 0.8586 - val_loss: 0.4667 - val_accuracy: 0.7129\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4369 - accuracy: 0.7717\n",
            "Epoch 00835: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4369 - accuracy: 0.7717 - val_loss: 0.4318 - val_accuracy: 0.8020\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4242 - accuracy: 0.8015\n",
            "Epoch 00836: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4242 - accuracy: 0.8015 - val_loss: 0.4430 - val_accuracy: 0.8218\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4480 - accuracy: 0.8114\n",
            "Epoch 00837: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4480 - accuracy: 0.8114 - val_loss: 0.4721 - val_accuracy: 0.7327\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.7295\n",
            "Epoch 00838: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4985 - accuracy: 0.7295 - val_loss: 0.4552 - val_accuracy: 0.8119\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.8189\n",
            "Epoch 00839: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4541 - accuracy: 0.8189 - val_loss: 0.4721 - val_accuracy: 0.8020\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.7816\n",
            "Epoch 00840: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4644 - accuracy: 0.7816 - val_loss: 0.4243 - val_accuracy: 0.8020\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.8065\n",
            "Epoch 00841: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4262 - accuracy: 0.8065 - val_loss: 0.4050 - val_accuracy: 0.8119\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3610 - accuracy: 0.8387\n",
            "Epoch 00842: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3610 - accuracy: 0.8387 - val_loss: 0.4465 - val_accuracy: 0.7525\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4841 - accuracy: 0.7072\n",
            "Epoch 00843: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4841 - accuracy: 0.7072 - val_loss: 0.4455 - val_accuracy: 0.7921\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4774 - accuracy: 0.7593\n",
            "Epoch 00844: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4774 - accuracy: 0.7593 - val_loss: 0.4449 - val_accuracy: 0.8020\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4845 - accuracy: 0.7618\n",
            "Epoch 00845: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4845 - accuracy: 0.7618 - val_loss: 0.4159 - val_accuracy: 0.7723\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.6749\n",
            "Epoch 00846: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5343 - accuracy: 0.6749 - val_loss: 0.3878 - val_accuracy: 0.8218\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4401 - accuracy: 0.8114\n",
            "Epoch 00847: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4401 - accuracy: 0.8114 - val_loss: 0.3862 - val_accuracy: 0.8218\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4180 - accuracy: 0.8189\n",
            "Epoch 00848: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4180 - accuracy: 0.8189 - val_loss: 0.4054 - val_accuracy: 0.8020\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3801 - accuracy: 0.8437\n",
            "Epoch 00849: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3801 - accuracy: 0.8437 - val_loss: 0.4874 - val_accuracy: 0.7129\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4680 - accuracy: 0.7345\n",
            "Epoch 00850: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4680 - accuracy: 0.7345 - val_loss: 0.3981 - val_accuracy: 0.8218\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4240 - accuracy: 0.8189\n",
            "Epoch 00851: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4240 - accuracy: 0.8189 - val_loss: 0.4282 - val_accuracy: 0.8119\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.7841\n",
            "Epoch 00852: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4336 - accuracy: 0.7841 - val_loss: 0.4036 - val_accuracy: 0.8218\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8015\n",
            "Epoch 00853: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4252 - accuracy: 0.8015 - val_loss: 0.3900 - val_accuracy: 0.8317\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8536\n",
            "Epoch 00854: val_accuracy did not improve from 0.86139\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3559 - accuracy: 0.8536 - val_loss: 0.3923 - val_accuracy: 0.8317\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3857 - accuracy: 0.8164\n",
            "Epoch 00855: val_accuracy improved from 0.86139 to 0.87129, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.3857 - accuracy: 0.8164 - val_loss: 0.3655 - val_accuracy: 0.8713\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8313\n",
            "Epoch 00856: val_accuracy improved from 0.87129 to 0.88119, saving model to output/weights.hdf5\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4154 - accuracy: 0.8313 - val_loss: 0.3707 - val_accuracy: 0.8812\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8462\n",
            "Epoch 00857: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3896 - accuracy: 0.8462 - val_loss: 0.3897 - val_accuracy: 0.8218\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4163 - accuracy: 0.7891\n",
            "Epoch 00858: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4163 - accuracy: 0.7891 - val_loss: 0.3593 - val_accuracy: 0.8515\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.8437\n",
            "Epoch 00859: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3825 - accuracy: 0.8437 - val_loss: 0.4140 - val_accuracy: 0.7921\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.8164\n",
            "Epoch 00860: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4032 - accuracy: 0.8164 - val_loss: 0.4060 - val_accuracy: 0.8119\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8313\n",
            "Epoch 00861: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3910 - accuracy: 0.8313 - val_loss: 0.3629 - val_accuracy: 0.8515\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8462\n",
            "Epoch 00862: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3735 - accuracy: 0.8462 - val_loss: 0.4167 - val_accuracy: 0.7723\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.7270\n",
            "Epoch 00863: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4643 - accuracy: 0.7270 - val_loss: 0.3952 - val_accuracy: 0.8020\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4140 - accuracy: 0.8189\n",
            "Epoch 00864: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4140 - accuracy: 0.8189 - val_loss: 0.3785 - val_accuracy: 0.8218\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3936 - accuracy: 0.8362\n",
            "Epoch 00865: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3936 - accuracy: 0.8362 - val_loss: 0.5204 - val_accuracy: 0.7030\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4483 - accuracy: 0.7767\n",
            "Epoch 00866: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4483 - accuracy: 0.7767 - val_loss: 0.3763 - val_accuracy: 0.8416\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.8635\n",
            "Epoch 00867: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3658 - accuracy: 0.8635 - val_loss: 0.3881 - val_accuracy: 0.8218\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3886 - accuracy: 0.8412\n",
            "Epoch 00868: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3886 - accuracy: 0.8412 - val_loss: 0.3869 - val_accuracy: 0.8317\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.8437\n",
            "Epoch 00869: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3683 - accuracy: 0.8437 - val_loss: 0.4622 - val_accuracy: 0.7327\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.7990\n",
            "Epoch 00870: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4112 - accuracy: 0.7990 - val_loss: 0.3735 - val_accuracy: 0.8317\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.8238\n",
            "Epoch 00871: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4106 - accuracy: 0.8238 - val_loss: 0.3727 - val_accuracy: 0.8218\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4009 - accuracy: 0.8362\n",
            "Epoch 00872: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4009 - accuracy: 0.8362 - val_loss: 0.3839 - val_accuracy: 0.8317\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4324 - accuracy: 0.7792\n",
            "Epoch 00873: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4324 - accuracy: 0.7792 - val_loss: 0.3691 - val_accuracy: 0.8515\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.8710\n",
            "Epoch 00874: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3437 - accuracy: 0.8710 - val_loss: 0.4272 - val_accuracy: 0.7426\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.8065\n",
            "Epoch 00875: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4021 - accuracy: 0.8065 - val_loss: 0.3924 - val_accuracy: 0.8317\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.8536\n",
            "Epoch 00876: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3724 - accuracy: 0.8536 - val_loss: 0.3611 - val_accuracy: 0.8515\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.8561\n",
            "Epoch 00877: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3653 - accuracy: 0.8561 - val_loss: 0.4046 - val_accuracy: 0.8119\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.7593\n",
            "Epoch 00878: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4279 - accuracy: 0.7593 - val_loss: 0.3772 - val_accuracy: 0.8317\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3897 - accuracy: 0.8437\n",
            "Epoch 00879: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3897 - accuracy: 0.8437 - val_loss: 0.3855 - val_accuracy: 0.8416\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3513 - accuracy: 0.8610\n",
            "Epoch 00880: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3513 - accuracy: 0.8610 - val_loss: 0.4446 - val_accuracy: 0.7426\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.8089\n",
            "Epoch 00881: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3921 - accuracy: 0.8089 - val_loss: 0.3636 - val_accuracy: 0.8515\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.8610\n",
            "Epoch 00882: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3395 - accuracy: 0.8610 - val_loss: 0.3546 - val_accuracy: 0.8614\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8263\n",
            "Epoch 00883: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4144 - accuracy: 0.8263 - val_loss: 0.3694 - val_accuracy: 0.8416\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4087 - accuracy: 0.8288\n",
            "Epoch 00884: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4087 - accuracy: 0.8288 - val_loss: 0.3533 - val_accuracy: 0.8515\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.8536\n",
            "Epoch 00885: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3570 - accuracy: 0.8536 - val_loss: 0.4347 - val_accuracy: 0.7624\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.7916\n",
            "Epoch 00886: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4221 - accuracy: 0.7916 - val_loss: 0.3926 - val_accuracy: 0.8317\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.8412\n",
            "Epoch 00887: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3846 - accuracy: 0.8412 - val_loss: 0.4273 - val_accuracy: 0.7921\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4039 - accuracy: 0.8362\n",
            "Epoch 00888: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4039 - accuracy: 0.8362 - val_loss: 0.4420 - val_accuracy: 0.8020\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4132 - accuracy: 0.8189\n",
            "Epoch 00889: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4132 - accuracy: 0.8189 - val_loss: 0.4353 - val_accuracy: 0.8020\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.8114\n",
            "Epoch 00890: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4289 - accuracy: 0.8114 - val_loss: 0.4232 - val_accuracy: 0.8020\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.8139\n",
            "Epoch 00891: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4412 - accuracy: 0.8139 - val_loss: 0.3915 - val_accuracy: 0.8416\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3779 - accuracy: 0.8387\n",
            "Epoch 00892: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3779 - accuracy: 0.8387 - val_loss: 0.4418 - val_accuracy: 0.7525\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3887 - accuracy: 0.8213\n",
            "Epoch 00893: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3887 - accuracy: 0.8213 - val_loss: 0.3508 - val_accuracy: 0.8515\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.8660\n",
            "Epoch 00894: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3391 - accuracy: 0.8660 - val_loss: 0.3529 - val_accuracy: 0.8416\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8437\n",
            "Epoch 00895: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3775 - accuracy: 0.8437 - val_loss: 0.3687 - val_accuracy: 0.8416\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8561\n",
            "Epoch 00896: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3548 - accuracy: 0.8561 - val_loss: 0.3770 - val_accuracy: 0.8515\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8759\n",
            "Epoch 00897: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3444 - accuracy: 0.8759 - val_loss: 0.3879 - val_accuracy: 0.8317\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8387\n",
            "Epoch 00898: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3875 - accuracy: 0.8387 - val_loss: 0.3779 - val_accuracy: 0.8515\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.8486\n",
            "Epoch 00899: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3682 - accuracy: 0.8486 - val_loss: 0.3718 - val_accuracy: 0.8515\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8685\n",
            "Epoch 00900: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3332 - accuracy: 0.8685 - val_loss: 0.3921 - val_accuracy: 0.8119\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3568 - accuracy: 0.8536\n",
            "Epoch 00901: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3568 - accuracy: 0.8536 - val_loss: 0.3602 - val_accuracy: 0.8416\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.8486\n",
            "Epoch 00902: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3845 - accuracy: 0.8486 - val_loss: 0.3654 - val_accuracy: 0.8416\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.8437\n",
            "Epoch 00903: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3761 - accuracy: 0.8437 - val_loss: 0.3991 - val_accuracy: 0.8119\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3475 - accuracy: 0.8610\n",
            "Epoch 00904: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3475 - accuracy: 0.8610 - val_loss: 0.3838 - val_accuracy: 0.8515\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.8561\n",
            "Epoch 00905: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3724 - accuracy: 0.8561 - val_loss: 0.3704 - val_accuracy: 0.8515\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.8610\n",
            "Epoch 00906: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3577 - accuracy: 0.8610 - val_loss: 0.3767 - val_accuracy: 0.8515\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3533 - accuracy: 0.8586\n",
            "Epoch 00907: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3533 - accuracy: 0.8586 - val_loss: 0.4007 - val_accuracy: 0.8416\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.8660\n",
            "Epoch 00908: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3415 - accuracy: 0.8660 - val_loss: 0.3808 - val_accuracy: 0.8416\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8660\n",
            "Epoch 00909: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3462 - accuracy: 0.8660 - val_loss: 0.3624 - val_accuracy: 0.8515\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3702 - accuracy: 0.8462\n",
            "Epoch 00910: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3702 - accuracy: 0.8462 - val_loss: 0.3624 - val_accuracy: 0.8515\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8685\n",
            "Epoch 00911: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3401 - accuracy: 0.8685 - val_loss: 0.4243 - val_accuracy: 0.7921\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.8362\n",
            "Epoch 00912: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3784 - accuracy: 0.8362 - val_loss: 0.3682 - val_accuracy: 0.8515\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.8685\n",
            "Epoch 00913: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3531 - accuracy: 0.8685 - val_loss: 0.3774 - val_accuracy: 0.8515\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.8660\n",
            "Epoch 00914: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3554 - accuracy: 0.8660 - val_loss: 0.4228 - val_accuracy: 0.7723\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8437\n",
            "Epoch 00915: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.3667 - accuracy: 0.8437 - val_loss: 0.3759 - val_accuracy: 0.8515\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.8859\n",
            "Epoch 00916: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3170 - accuracy: 0.8859 - val_loss: 0.3589 - val_accuracy: 0.8515\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3361 - accuracy: 0.8734\n",
            "Epoch 00917: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3361 - accuracy: 0.8734 - val_loss: 0.3651 - val_accuracy: 0.8515\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8784\n",
            "Epoch 00918: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3303 - accuracy: 0.8784 - val_loss: 0.3782 - val_accuracy: 0.8515\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8610\n",
            "Epoch 00919: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3462 - accuracy: 0.8610 - val_loss: 0.4002 - val_accuracy: 0.8317\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.8437\n",
            "Epoch 00920: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3668 - accuracy: 0.8437 - val_loss: 0.3669 - val_accuracy: 0.8515\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.8660\n",
            "Epoch 00921: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3536 - accuracy: 0.8660 - val_loss: 0.3615 - val_accuracy: 0.8515\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.8685\n",
            "Epoch 00922: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3368 - accuracy: 0.8685 - val_loss: 0.3995 - val_accuracy: 0.8218\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8362\n",
            "Epoch 00923: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3636 - accuracy: 0.8362 - val_loss: 0.3601 - val_accuracy: 0.8416\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3649 - accuracy: 0.8536\n",
            "Epoch 00924: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3649 - accuracy: 0.8536 - val_loss: 0.3754 - val_accuracy: 0.8515\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3829 - accuracy: 0.8586\n",
            "Epoch 00925: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3829 - accuracy: 0.8586 - val_loss: 0.3860 - val_accuracy: 0.8416\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.8710\n",
            "Epoch 00926: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3425 - accuracy: 0.8710 - val_loss: 0.3881 - val_accuracy: 0.8317\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.8586\n",
            "Epoch 00927: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3365 - accuracy: 0.8586 - val_loss: 0.3509 - val_accuracy: 0.8515\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8511\n",
            "Epoch 00928: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3559 - accuracy: 0.8511 - val_loss: 0.3524 - val_accuracy: 0.8515\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3558 - accuracy: 0.8536\n",
            "Epoch 00929: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3558 - accuracy: 0.8536 - val_loss: 0.3883 - val_accuracy: 0.8218\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.8660\n",
            "Epoch 00930: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3378 - accuracy: 0.8660 - val_loss: 0.3629 - val_accuracy: 0.8515\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3411 - accuracy: 0.8685\n",
            "Epoch 00931: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3411 - accuracy: 0.8685 - val_loss: 0.3643 - val_accuracy: 0.8416\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3748 - accuracy: 0.8462\n",
            "Epoch 00932: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3748 - accuracy: 0.8462 - val_loss: 0.3806 - val_accuracy: 0.8317\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.8685\n",
            "Epoch 00933: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3448 - accuracy: 0.8685 - val_loss: 0.3644 - val_accuracy: 0.8515\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3397 - accuracy: 0.8660\n",
            "Epoch 00934: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3397 - accuracy: 0.8660 - val_loss: 0.3524 - val_accuracy: 0.8515\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3543 - accuracy: 0.8660\n",
            "Epoch 00935: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3543 - accuracy: 0.8660 - val_loss: 0.3542 - val_accuracy: 0.8614\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.8784\n",
            "Epoch 00936: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3305 - accuracy: 0.8784 - val_loss: 0.3736 - val_accuracy: 0.8317\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3367 - accuracy: 0.8710\n",
            "Epoch 00937: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3367 - accuracy: 0.8710 - val_loss: 0.3823 - val_accuracy: 0.8416\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3227 - accuracy: 0.8809\n",
            "Epoch 00938: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3227 - accuracy: 0.8809 - val_loss: 0.3617 - val_accuracy: 0.8416\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3605 - accuracy: 0.8610\n",
            "Epoch 00939: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3605 - accuracy: 0.8610 - val_loss: 0.3613 - val_accuracy: 0.8515\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.8462\n",
            "Epoch 00940: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3532 - accuracy: 0.8462 - val_loss: 0.3693 - val_accuracy: 0.8416\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8660\n",
            "Epoch 00941: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3323 - accuracy: 0.8660 - val_loss: 0.3534 - val_accuracy: 0.8515\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3315 - accuracy: 0.8759\n",
            "Epoch 00942: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3315 - accuracy: 0.8759 - val_loss: 0.3684 - val_accuracy: 0.8515\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3593 - accuracy: 0.8660\n",
            "Epoch 00943: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3593 - accuracy: 0.8660 - val_loss: 0.3749 - val_accuracy: 0.8416\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.8734\n",
            "Epoch 00944: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3307 - accuracy: 0.8734 - val_loss: 0.3544 - val_accuracy: 0.8614\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.8635\n",
            "Epoch 00945: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3481 - accuracy: 0.8635 - val_loss: 0.3550 - val_accuracy: 0.8614\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.8759\n",
            "Epoch 00946: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3302 - accuracy: 0.8759 - val_loss: 0.3661 - val_accuracy: 0.8515\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.8933\n",
            "Epoch 00947: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3026 - accuracy: 0.8933 - val_loss: 0.3739 - val_accuracy: 0.8515\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.8710\n",
            "Epoch 00948: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3274 - accuracy: 0.8710 - val_loss: 0.3641 - val_accuracy: 0.8614\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8511\n",
            "Epoch 00949: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3639 - accuracy: 0.8511 - val_loss: 0.3577 - val_accuracy: 0.8614\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3347 - accuracy: 0.8759\n",
            "Epoch 00950: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3347 - accuracy: 0.8759 - val_loss: 0.3613 - val_accuracy: 0.8614\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3295 - accuracy: 0.8734\n",
            "Epoch 00951: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3295 - accuracy: 0.8734 - val_loss: 0.3553 - val_accuracy: 0.8614\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.8561\n",
            "Epoch 00952: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3427 - accuracy: 0.8561 - val_loss: 0.3658 - val_accuracy: 0.8416\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.8586\n",
            "Epoch 00953: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3468 - accuracy: 0.8586 - val_loss: 0.3649 - val_accuracy: 0.8515\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.8660\n",
            "Epoch 00954: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3360 - accuracy: 0.8660 - val_loss: 0.3613 - val_accuracy: 0.8614\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3558 - accuracy: 0.8586\n",
            "Epoch 00955: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3558 - accuracy: 0.8586 - val_loss: 0.3624 - val_accuracy: 0.8614\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.8734\n",
            "Epoch 00956: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3242 - accuracy: 0.8734 - val_loss: 0.3799 - val_accuracy: 0.8317\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8710\n",
            "Epoch 00957: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3508 - accuracy: 0.8710 - val_loss: 0.3525 - val_accuracy: 0.8515\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3334 - accuracy: 0.8759\n",
            "Epoch 00958: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3334 - accuracy: 0.8759 - val_loss: 0.3709 - val_accuracy: 0.8317\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.8834\n",
            "Epoch 00959: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3365 - accuracy: 0.8834 - val_loss: 0.3543 - val_accuracy: 0.8614\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.8759\n",
            "Epoch 00960: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3214 - accuracy: 0.8759 - val_loss: 0.3770 - val_accuracy: 0.8416\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8660\n",
            "Epoch 00961: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3495 - accuracy: 0.8660 - val_loss: 0.3515 - val_accuracy: 0.8614\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3328 - accuracy: 0.8710\n",
            "Epoch 00962: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3328 - accuracy: 0.8710 - val_loss: 0.3548 - val_accuracy: 0.8614\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.8486\n",
            "Epoch 00963: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3455 - accuracy: 0.8486 - val_loss: 0.3461 - val_accuracy: 0.8614\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8337\n",
            "Epoch 00964: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3646 - accuracy: 0.8337 - val_loss: 0.3782 - val_accuracy: 0.8317\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3416 - accuracy: 0.8710\n",
            "Epoch 00965: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3416 - accuracy: 0.8710 - val_loss: 0.3610 - val_accuracy: 0.8614\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3363 - accuracy: 0.8660\n",
            "Epoch 00966: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3363 - accuracy: 0.8660 - val_loss: 0.3444 - val_accuracy: 0.8614\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3203 - accuracy: 0.8685\n",
            "Epoch 00967: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3203 - accuracy: 0.8685 - val_loss: 0.3628 - val_accuracy: 0.8515\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.8337\n",
            "Epoch 00968: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3802 - accuracy: 0.8337 - val_loss: 0.3530 - val_accuracy: 0.8614\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.8759\n",
            "Epoch 00969: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3196 - accuracy: 0.8759 - val_loss: 0.3692 - val_accuracy: 0.8614\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8734\n",
            "Epoch 00970: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3401 - accuracy: 0.8734 - val_loss: 0.3504 - val_accuracy: 0.8614\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8933\n",
            "Epoch 00971: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3029 - accuracy: 0.8933 - val_loss: 0.3627 - val_accuracy: 0.8416\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.8586\n",
            "Epoch 00972: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3560 - accuracy: 0.8586 - val_loss: 0.3450 - val_accuracy: 0.8515\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8511\n",
            "Epoch 00973: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3523 - accuracy: 0.8511 - val_loss: 0.3625 - val_accuracy: 0.8416\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3279 - accuracy: 0.8759\n",
            "Epoch 00974: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3279 - accuracy: 0.8759 - val_loss: 0.3693 - val_accuracy: 0.8416\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.8685\n",
            "Epoch 00975: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3466 - accuracy: 0.8685 - val_loss: 0.3499 - val_accuracy: 0.8515\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.8685\n",
            "Epoch 00976: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3418 - accuracy: 0.8685 - val_loss: 0.3548 - val_accuracy: 0.8614\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.8660\n",
            "Epoch 00977: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3288 - accuracy: 0.8660 - val_loss: 0.3603 - val_accuracy: 0.8416\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.8685\n",
            "Epoch 00978: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3405 - accuracy: 0.8685 - val_loss: 0.3684 - val_accuracy: 0.8614\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3233 - accuracy: 0.8859\n",
            "Epoch 00979: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3233 - accuracy: 0.8859 - val_loss: 0.3612 - val_accuracy: 0.8614\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3593 - accuracy: 0.8660\n",
            "Epoch 00980: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3593 - accuracy: 0.8660 - val_loss: 0.3524 - val_accuracy: 0.8614\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3359 - accuracy: 0.8660\n",
            "Epoch 00981: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3359 - accuracy: 0.8660 - val_loss: 0.3511 - val_accuracy: 0.8614\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8362\n",
            "Epoch 00982: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3731 - accuracy: 0.8362 - val_loss: 0.3450 - val_accuracy: 0.8515\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.8883\n",
            "Epoch 00983: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3207 - accuracy: 0.8883 - val_loss: 0.3653 - val_accuracy: 0.8515\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.8660\n",
            "Epoch 00984: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3357 - accuracy: 0.8660 - val_loss: 0.3547 - val_accuracy: 0.8614\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.8834\n",
            "Epoch 00985: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3168 - accuracy: 0.8834 - val_loss: 0.3502 - val_accuracy: 0.8614\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.8784\n",
            "Epoch 00986: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3179 - accuracy: 0.8784 - val_loss: 0.3606 - val_accuracy: 0.8515\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8908\n",
            "Epoch 00987: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3120 - accuracy: 0.8908 - val_loss: 0.3415 - val_accuracy: 0.8614\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.8635\n",
            "Epoch 00988: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3319 - accuracy: 0.8635 - val_loss: 0.3431 - val_accuracy: 0.8614\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.8635\n",
            "Epoch 00989: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3503 - accuracy: 0.8635 - val_loss: 0.3516 - val_accuracy: 0.8515\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.8759\n",
            "Epoch 00990: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3231 - accuracy: 0.8759 - val_loss: 0.3447 - val_accuracy: 0.8614\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.8809\n",
            "Epoch 00991: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3155 - accuracy: 0.8809 - val_loss: 0.3556 - val_accuracy: 0.8515\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.8685\n",
            "Epoch 00992: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3488 - accuracy: 0.8685 - val_loss: 0.3406 - val_accuracy: 0.8515\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3320 - accuracy: 0.8710\n",
            "Epoch 00993: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3320 - accuracy: 0.8710 - val_loss: 0.3410 - val_accuracy: 0.8614\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8933\n",
            "Epoch 00994: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3097 - accuracy: 0.8933 - val_loss: 0.3753 - val_accuracy: 0.8416\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.8536\n",
            "Epoch 00995: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3440 - accuracy: 0.8536 - val_loss: 0.3624 - val_accuracy: 0.8614\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.8462\n",
            "Epoch 00996: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3740 - accuracy: 0.8462 - val_loss: 0.3691 - val_accuracy: 0.8416\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.8635\n",
            "Epoch 00997: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3509 - accuracy: 0.8635 - val_loss: 0.3657 - val_accuracy: 0.8218\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3122 - accuracy: 0.8933\n",
            "Epoch 00998: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3122 - accuracy: 0.8933 - val_loss: 0.3554 - val_accuracy: 0.8317\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3236 - accuracy: 0.8710\n",
            "Epoch 00999: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3236 - accuracy: 0.8710 - val_loss: 0.3527 - val_accuracy: 0.8614\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.8462\n",
            "Epoch 01000: val_accuracy did not improve from 0.88119\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3690 - accuracy: 0.8462 - val_loss: 0.3526 - val_accuracy: 0.8515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfRIyBc5npnj",
        "outputId": "e837660a-7693-4bc3-a8d5-fb2b9285f049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# 모델 학습 과정 표시하기\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXoo6ENYglzQ",
        "outputId": "4b9dc654-07ba-44a1-c555-c6b072b59625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yhat = model.predict(X_test)\n",
        "for idx in range(len(yhat)):\n",
        "    if yhat[idx] >= 0.5:\n",
        "        yhat[idx] = 1\n",
        "    else:\n",
        "        yhat[idx] = 0 \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, yhat)\n",
        "(confusion_matrix(y_test, yhat)[0][0] + confusion_matrix(y_test, yhat)[1][1])/(confusion_matrix(y_test, yhat)[0][0] + confusion_matrix(y_test, yhat)[0][1]+(confusion_matrix(y_test, yhat)[1][0] + confusion_matrix(y_test, yhat)[1][1]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8188976377952756"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    }
  ]
}